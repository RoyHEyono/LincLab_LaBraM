| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(batch_size=128, epochs=100, save_ckpt_freq=20, model='vqnsp_encoder_base_decoder_3x200x12', codebook_n_emd=8192, codebook_emd_dim=64, ema_decay=0.99, quantize_kmeans_init=True, input_size=1600, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.99], clip_grad=None, weight_decay=0.0001, weight_decay_end=None, lr=5e-05, warmup_lr=1e-06, min_lr=1e-05, warmup_epochs=10, warmup_steps=-1, output_dir='./checkpoints/vqnsp/', log_dir='./log/vqnsp/', device='cuda', seed=0, resume='', auto_resume=True, dist_eval=True, disable_eval=False, eval=False, calculate_codebook_usage=False, start_epoch=0, num_workers=10, pin_mem=True, world_size=4, local_rank=-1, dist_on_itp=False, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
{}
Final encoder config {'EEG_size': 1600, 'patch_size': 200, 'in_chans': 1, 'num_classes': 0, 'embed_dim': 200, 'depth': 12, 'num_heads': 10, 'mlp_ratio': 4.0, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'init_values': 0.0, 'use_abs_pos_emb': True, 'use_rel_pos_bias': False, 'use_shared_rel_pos_bias': False, 'use_mean_pooling': True, 'init_scale': 0.001}
Final decoder config {'EEG_size': 8, 'patch_size': 1, 'in_chans': 64, 'num_classes': 0, 'embed_dim': 200, 'depth': 3, 'num_heads': 10, 'mlp_ratio': 4.0, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'init_values': 0.0, 'use_abs_pos_emb': True, 'use_rel_pos_bias': False, 'use_shared_rel_pos_bias': False, 'use_mean_pooling': True, 'init_scale': 0.001}
ddp is enable, so use ddp_reduce to sync the statistic_code_usage for each gpu!
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f867c702590>
Model = VQNSP(
  (encoder): NeuralTransformer(
    (patch_embed): TemporalConv(
      (conv1): Conv2d(1, 8, kernel_size=(1, 15), stride=(1, 8), padding=(0, 7))
      (gelu1): GELU(approximate='none')
      (norm1): GroupNorm(4, 8, eps=1e-05, affine=True)
      (conv2): Conv2d(8, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (gelu2): GELU(approximate='none')
      (norm2): GroupNorm(4, 8, eps=1e-05, affine=True)
      (conv3): Conv2d(8, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (norm3): GroupNorm(4, 8, eps=1e-05, affine=True)
      (gelu3): GELU(approximate='none')
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-11): 12 x Block(
        (norm1): LayerNorm((200,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=200, out_features=600, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=200, out_features=200, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((200,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=200, out_features=800, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=800, out_features=200, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): Identity()
    (fc_norm): LayerNorm((200,), eps=1e-06, elementwise_affine=True)
    (head): Identity()
  )
  (decoder): NeuralTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(64, 200, kernel_size=(1, 1), stride=(1, 1))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-2): 3 x Block(
        (norm1): LayerNorm((200,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=200, out_features=600, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=200, out_features=200, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((200,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=200, out_features=800, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=800, out_features=200, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): Identity()
    (fc_norm): LayerNorm((200,), eps=1e-06, elementwise_affine=True)
    (head): Identity()
  )
  (quantize): NormEMAVectorQuantizer(
    (embedding): EmbeddingEMA()
  )
  (encode_task_layer): Sequential(
    (0): Linear(in_features=200, out_features=200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=200, out_features=64, bias=True)
  )
  (decode_task_layer): Sequential(
    (0): Linear(in_features=200, out_features=200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=200, out_features=200, bias=True)
  )
  (decode_task_layer_angle): Sequential(
    (0): Linear(in_features=200, out_features=200, bias=True)
    (1): Tanh()
    (2): Linear(in_features=200, out_features=200, bias=True)
  )
)
number of learnable params in model.encoder: 5.818976 M
number of fixed params in model.encoder: 0.0 M
number of learnable params in model.decoder: 1.4898 M
number of fixed params in model.decoder: 0.0 M
total number of learnable params: 7.52264 M
total number of fixed params in : 1.056768 M
LR = 0.00020000
Min LR = 0.00001000
Weigth Decay = 0.00010000
Batch size = 512
Number of training steps = 19
Number of training examples per epoch = 9728
Skip weight decay name marked in model: {'encoder.pos_embed', 'decoder.pos_embed', 'encoder.time_embed', 'decoder.time_embed', 'decoder.cls_token', 'encoder.cls_token', 'quantize.embedding.weight'}
Param groups = {
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "encoder.cls_token",
      "encoder.pos_embed",
      "encoder.time_embed",
      "encoder.patch_embed.conv1.bias",
      "encoder.patch_embed.norm1.weight",
      "encoder.patch_embed.norm1.bias",
      "encoder.patch_embed.conv2.bias",
      "encoder.patch_embed.norm2.weight",
      "encoder.patch_embed.norm2.bias",
      "encoder.patch_embed.conv3.bias",
      "encoder.patch_embed.norm3.weight",
      "encoder.patch_embed.norm3.bias",
      "encoder.blocks.0.norm1.weight",
      "encoder.blocks.0.norm1.bias",
      "encoder.blocks.0.attn.q_bias",
      "encoder.blocks.0.attn.v_bias",
      "encoder.blocks.0.attn.proj.bias",
      "encoder.blocks.0.norm2.weight",
      "encoder.blocks.0.norm2.bias",
      "encoder.blocks.0.mlp.fc1.bias",
      "encoder.blocks.0.mlp.fc2.bias",
      "encoder.blocks.1.norm1.weight",
      "encoder.blocks.1.norm1.bias",
      "encoder.blocks.1.attn.q_bias",
      "encoder.blocks.1.attn.v_bias",
      "encoder.blocks.1.attn.proj.bias",
      "encoder.blocks.1.norm2.weight",
      "encoder.blocks.1.norm2.bias",
      "encoder.blocks.1.mlp.fc1.bias",
      "encoder.blocks.1.mlp.fc2.bias",
      "encoder.blocks.2.norm1.weight",
      "encoder.blocks.2.norm1.bias",
      "encoder.blocks.2.attn.q_bias",
      "encoder.blocks.2.attn.v_bias",
      "encoder.blocks.2.attn.proj.bias",
      "encoder.blocks.2.norm2.weight",
      "encoder.blocks.2.norm2.bias",
      "encoder.blocks.2.mlp.fc1.bias",
      "encoder.blocks.2.mlp.fc2.bias",
      "encoder.blocks.3.norm1.weight",
      "encoder.blocks.3.norm1.bias",
      "encoder.blocks.3.attn.q_bias",
      "encoder.blocks.3.attn.v_bias",
      "encoder.blocks.3.attn.proj.bias",
      "encoder.blocks.3.norm2.weight",
      "encoder.blocks.3.norm2.bias",
      "encoder.blocks.3.mlp.fc1.bias",
      "encoder.blocks.3.mlp.fc2.bias",
      "encoder.blocks.4.norm1.weight",
      "encoder.blocks.4.norm1.bias",
      "encoder.blocks.4.attn.q_bias",
      "encoder.blocks.4.attn.v_bias",
      "encoder.blocks.4.attn.proj.bias",
      "encoder.blocks.4.norm2.weight",
      "encoder.blocks.4.norm2.bias",
      "encoder.blocks.4.mlp.fc1.bias",
      "encoder.blocks.4.mlp.fc2.bias",
      "encoder.blocks.5.norm1.weight",
      "encoder.blocks.5.norm1.bias",
      "encoder.blocks.5.attn.q_bias",
      "encoder.blocks.5.attn.v_bias",
      "encoder.blocks.5.attn.proj.bias",
      "encoder.blocks.5.norm2.weight",
      "encoder.blocks.5.norm2.bias",
      "encoder.blocks.5.mlp.fc1.bias",
      "encoder.blocks.5.mlp.fc2.bias",
      "encoder.blocks.6.norm1.weight",
      "encoder.blocks.6.norm1.bias",
      "encoder.blocks.6.attn.q_bias",
      "encoder.blocks.6.attn.v_bias",
      "encoder.blocks.6.attn.proj.bias",
      "encoder.blocks.6.norm2.weight",
      "encoder.blocks.6.norm2.bias",
      "encoder.blocks.6.mlp.fc1.bias",
      "encoder.blocks.6.mlp.fc2.bias",
      "encoder.blocks.7.norm1.weight",
      "encoder.blocks.7.norm1.bias",
      "encoder.blocks.7.attn.q_bias",
      "encoder.blocks.7.attn.v_bias",
      "encoder.blocks.7.attn.proj.bias",
      "encoder.blocks.7.norm2.weight",
      "encoder.blocks.7.norm2.bias",
      "encoder.blocks.7.mlp.fc1.bias",
      "encoder.blocks.7.mlp.fc2.bias",
      "encoder.blocks.8.norm1.weight",
      "encoder.blocks.8.norm1.bias",
      "encoder.blocks.8.attn.q_bias",
      "encoder.blocks.8.attn.v_bias",
      "encoder.blocks.8.attn.proj.bias",
      "encoder.blocks.8.norm2.weight",
      "encoder.blocks.8.norm2.bias",
      "encoder.blocks.8.mlp.fc1.bias",
      "encoder.blocks.8.mlp.fc2.bias",
      "encoder.blocks.9.norm1.weight",
      "encoder.blocks.9.norm1.bias",
      "encoder.blocks.9.attn.q_bias",
      "encoder.blocks.9.attn.v_bias",
      "encoder.blocks.9.attn.proj.bias",
      "encoder.blocks.9.norm2.weight",
      "encoder.blocks.9.norm2.bias",
      "encoder.blocks.9.mlp.fc1.bias",
      "encoder.blocks.9.mlp.fc2.bias",
      "encoder.blocks.10.norm1.weight",
      "encoder.blocks.10.norm1.bias",
      "encoder.blocks.10.attn.q_bias",
      "encoder.blocks.10.attn.v_bias",
      "encoder.blocks.10.attn.proj.bias",
      "encoder.blocks.10.norm2.weight",
      "encoder.blocks.10.norm2.bias",
      "encoder.blocks.10.mlp.fc1.bias",
      "encoder.blocks.10.mlp.fc2.bias",
      "encoder.blocks.11.norm1.weight",
      "encoder.blocks.11.norm1.bias",
      "encoder.blocks.11.attn.q_bias",
      "encoder.blocks.11.attn.v_bias",
      "encoder.blocks.11.attn.proj.bias",
      "encoder.blocks.11.norm2.weight",
      "encoder.blocks.11.norm2.bias",
      "encoder.blocks.11.mlp.fc1.bias",
      "encoder.blocks.11.mlp.fc2.bias",
      "encoder.fc_norm.weight",
      "encoder.fc_norm.bias",
      "decoder.cls_token",
      "decoder.pos_embed",
      "decoder.time_embed",
      "decoder.patch_embed.proj.bias",
      "decoder.blocks.0.norm1.weight",
      "decoder.blocks.0.norm1.bias",
      "decoder.blocks.0.attn.q_bias",
      "decoder.blocks.0.attn.v_bias",
      "decoder.blocks.0.attn.proj.bias",
      "decoder.blocks.0.norm2.weight",
      "decoder.blocks.0.norm2.bias",
      "decoder.blocks.0.mlp.fc1.bias",
      "decoder.blocks.0.mlp.fc2.bias",
      "decoder.blocks.1.norm1.weight",
      "decoder.blocks.1.norm1.bias",
      "decoder.blocks.1.attn.q_bias",
      "decoder.blocks.1.attn.v_bias",
      "decoder.blocks.1.attn.proj.bias",
      "decoder.blocks.1.norm2.weight",
      "decoder.blocks.1.norm2.bias",
      "decoder.blocks.1.mlp.fc1.bias",
      "decoder.blocks.1.mlp.fc2.bias",
      "decoder.blocks.2.norm1.weight",
      "decoder.blocks.2.norm1.bias",
      "decoder.blocks.2.attn.q_bias",
      "decoder.blocks.2.attn.v_bias",
      "decoder.blocks.2.attn.proj.bias",
      "decoder.blocks.2.norm2.weight",
      "decoder.blocks.2.norm2.bias",
      "decoder.blocks.2.mlp.fc1.bias",
      "decoder.blocks.2.mlp.fc2.bias",
      "decoder.fc_norm.weight",
      "decoder.fc_norm.bias",
      "encode_task_layer.0.bias",
      "encode_task_layer.2.bias",
      "decode_task_layer.0.bias",
      "decode_task_layer.2.bias",
      "decode_task_layer_angle.0.bias",
      "decode_task_layer_angle.2.bias"
    ],
    "lr_scale": 1.0
  },
  "decay": {
    "weight_decay": 0.0001,
    "params": [
      "encoder.patch_embed.conv1.weight",
      "encoder.patch_embed.conv2.weight",
      "encoder.patch_embed.conv3.weight",
      "encoder.blocks.0.attn.qkv.weight",
      "encoder.blocks.0.attn.proj.weight",
      "encoder.blocks.0.mlp.fc1.weight",
      "encoder.blocks.0.mlp.fc2.weight",
      "encoder.blocks.1.attn.qkv.weight",
      "encoder.blocks.1.attn.proj.weight",
      "encoder.blocks.1.mlp.fc1.weight",
      "encoder.blocks.1.mlp.fc2.weight",
      "encoder.blocks.2.attn.qkv.weight",
      "encoder.blocks.2.attn.proj.weight",
      "encoder.blocks.2.mlp.fc1.weight",
      "encoder.blocks.2.mlp.fc2.weight",
      "encoder.blocks.3.attn.qkv.weight",
      "encoder.blocks.3.attn.proj.weight",
      "encoder.blocks.3.mlp.fc1.weight",
      "encoder.blocks.3.mlp.fc2.weight",
      "encoder.blocks.4.attn.qkv.weight",
      "encoder.blocks.4.attn.proj.weight",
      "encoder.blocks.4.mlp.fc1.weight",
      "encoder.blocks.4.mlp.fc2.weight",
      "encoder.blocks.5.attn.qkv.weight",
      "encoder.blocks.5.attn.proj.weight",
      "encoder.blocks.5.mlp.fc1.weight",
      "encoder.blocks.5.mlp.fc2.weight",
      "encoder.blocks.6.attn.qkv.weight",
      "encoder.blocks.6.attn.proj.weight",
      "encoder.blocks.6.mlp.fc1.weight",
      "encoder.blocks.6.mlp.fc2.weight",
      "encoder.blocks.7.attn.qkv.weight",
      "encoder.blocks.7.attn.proj.weight",
      "encoder.blocks.7.mlp.fc1.weight",
      "encoder.blocks.7.mlp.fc2.weight",
      "encoder.blocks.8.attn.qkv.weight",
      "encoder.blocks.8.attn.proj.weight",
      "encoder.blocks.8.mlp.fc1.weight",
      "encoder.blocks.8.mlp.fc2.weight",
      "encoder.blocks.9.attn.qkv.weight",
      "encoder.blocks.9.attn.proj.weight",
      "encoder.blocks.9.mlp.fc1.weight",
      "encoder.blocks.9.mlp.fc2.weight",
      "encoder.blocks.10.attn.qkv.weight",
      "encoder.blocks.10.attn.proj.weight",
      "encoder.blocks.10.mlp.fc1.weight",
      "encoder.blocks.10.mlp.fc2.weight",
      "encoder.blocks.11.attn.qkv.weight",
      "encoder.blocks.11.attn.proj.weight",
      "encoder.blocks.11.mlp.fc1.weight",
      "encoder.blocks.11.mlp.fc2.weight",
      "decoder.patch_embed.proj.weight",
      "decoder.blocks.0.attn.qkv.weight",
      "decoder.blocks.0.attn.proj.weight",
      "decoder.blocks.0.mlp.fc1.weight",
      "decoder.blocks.0.mlp.fc2.weight",
      "decoder.blocks.1.attn.qkv.weight",
      "decoder.blocks.1.attn.proj.weight",
      "decoder.blocks.1.mlp.fc1.weight",
      "decoder.blocks.1.mlp.fc2.weight",
      "decoder.blocks.2.attn.qkv.weight",
      "decoder.blocks.2.attn.proj.weight",
      "decoder.blocks.2.mlp.fc1.weight",
      "decoder.blocks.2.mlp.fc2.weight",
      "encode_task_layer.0.weight",
      "encode_task_layer.2.weight",
      "decode_task_layer.0.weight",
      "decode_task_layer.2.weight",
      "decode_task_layer_angle.0.weight",
      "decode_task_layer_angle.2.weight"
    ],
    "lr_scale": 1.0
  }
}
Optimizer config: {'lr': 0.0002, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.99]}
Use step level LR & WD scheduler!
Set warmup steps = 190
Auto resume checkpoint: 
Start training for 100 epochs
Reset the codebook statistic info in quantizer before each epoch
Performing Kemans init for codebook
Epoch: [0]  [ 0/19]  eta: 0:09:00  lr: 0.000000  min_lr: 0.000000  loss: 2.0183 (2.0183)  quant_loss: 0.0027 (0.0027)  rec_loss: 1.0013 (1.0013)  rec_angle_loss: 1.0144 (1.0144)  total_loss: 2.0183 (2.0183)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.9780 (0.9780)  time: 28.4357  data: 20.8221  max mem: 13145
Epoch: [0]  [10/19]  eta: 0:00:26  lr: 0.000011  min_lr: 0.000011  loss: 2.0151 (2.0118)  quant_loss: 0.0064 (0.0061)  rec_loss: 0.9957 (0.9931)  rec_angle_loss: 1.0131 (1.0126)  total_loss: 2.0151 (2.0118)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.9730 (0.9628)  time: 2.9596  data: 1.8931  max mem: 13238
Epoch: [0]  [18/19]  eta: 0:00:01  lr: 0.000019  min_lr: 0.000019  loss: 1.9980 (1.9911)  quant_loss: 0.0066 (0.0065)  rec_loss: 0.9816 (0.9763)  rec_angle_loss: 1.0100 (1.0083)  total_loss: 1.9980 (1.9911)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.9299 (0.8925)  time: 1.8654  data: 1.0961  max mem: 13238
Epoch: [0] Total time: 0:00:36 (1.9095 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000019  loss: 1.9980 (1.9911)  quant_loss: 0.0066 (0.0066)  rec_loss: 0.9816 (0.9762)  rec_angle_loss: 1.0100 (1.0083)  total_loss: 1.9980 (1.9911)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.9299 (0.8925)
Unused code in codebook: 0
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:21  loss: 1.9330 (1.9330)  time: 5.7975  data: 5.2894  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.9320 (1.9313)  time: 0.5824  data: 0.4809  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.9320 (1.9316)  time: 0.4706  data: 0.3779  max mem: 13238
Validation: Total time: 0:00:07 (0.5605 s / it)
Averaged stats: loss: 1.9320 (1.9316)  quant_loss: 0.0079 (0.0080)  rec_loss: 0.9329 (0.9327)  rec_angle_loss: 0.9949 (0.9948)  total_loss: 1.9358 (1.9354)
Unused code in codebook: 12
Validation loss of the network on the 10144 test EEG: 1.9316
Reset the codebook statistic info in quantizer before each epoch
Epoch: [1]  [ 0/19]  eta: 0:03:45  lr: 0.000020  min_lr: 0.000020  loss: 1.9317 (1.9317)  quant_loss: 0.0079 (0.0079)  rec_loss: 0.9292 (0.9292)  rec_angle_loss: 0.9946 (0.9946)  total_loss: 1.9317 (1.9317)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6732 (0.6732)  time: 11.8739  data: 10.9113  max mem: 13238
Epoch: [1]  [10/19]  eta: 0:00:15  lr: 0.000031  min_lr: 0.000031  loss: 1.9026 (1.9011)  quant_loss: 0.0079 (0.0077)  rec_loss: 0.9079 (0.9067)  rec_angle_loss: 0.9867 (0.9866)  total_loss: 1.9026 (1.9011)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6051 (0.6177)  time: 1.7381  data: 1.3214  max mem: 13238
Epoch: [1]  [18/19]  eta: 0:00:01  lr: 0.000039  min_lr: 0.000039  loss: 1.8753 (1.8774)  quant_loss: 0.0071 (0.0068)  rec_loss: 0.8884 (0.8899)  rec_angle_loss: 0.9797 (0.9807)  total_loss: 1.8753 (1.8774)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6029 (0.6119)  time: 1.1578  data: 0.7651  max mem: 13238
Epoch: [1] Total time: 0:00:23 (1.2297 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000039  loss: 1.8753 (1.8776)  quant_loss: 0.0071 (0.0069)  rec_loss: 0.8884 (0.8900)  rec_angle_loss: 0.9797 (0.9807)  total_loss: 1.8753 (1.8776)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6029 (0.6119)
Unused code in codebook: 225
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:19  loss: 1.8154 (1.8154)  time: 5.6752  data: 5.5977  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.8158 (1.8153)  time: 0.5719  data: 0.5090  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.8158 (1.8154)  time: 0.4588  data: 0.3999  max mem: 13238
Validation: Total time: 0:00:07 (0.5521 s / it)
Averaged stats: loss: 1.8158 (1.8153)  quant_loss: 0.0043 (0.0044)  rec_loss: 0.8536 (0.8535)  rec_angle_loss: 0.9618 (0.9617)  total_loss: 1.8197 (1.8196)
Unused code in codebook: 6262
Validation loss of the network on the 10144 test EEG: 1.8153
Reset the codebook statistic info in quantizer before each epoch
Epoch: [2]  [ 0/19]  eta: 0:04:10  lr: 0.000040  min_lr: 0.000040  loss: 1.8165 (1.8165)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.8461 (0.8461)  rec_angle_loss: 0.9661 (0.9661)  total_loss: 1.8165 (1.8165)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6131 (0.6131)  time: 13.1930  data: 11.6648  max mem: 13238
Epoch: [2]  [10/19]  eta: 0:00:16  lr: 0.000051  min_lr: 0.000051  loss: 1.7805 (1.7803)  quant_loss: 0.0033 (0.0034)  rec_loss: 0.8191 (0.8190)  rec_angle_loss: 0.9581 (0.9578)  total_loss: 1.7805 (1.7803)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6245 (0.6245)  time: 1.8514  data: 1.2950  max mem: 13238
Epoch: [2]  [18/19]  eta: 0:00:01  lr: 0.000059  min_lr: 0.000059  loss: 1.7489 (1.7472)  quant_loss: 0.0028 (0.0029)  rec_loss: 0.7951 (0.7930)  rec_angle_loss: 0.9511 (0.9513)  total_loss: 1.7489 (1.7472)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6300 (0.6279)  time: 1.2232  data: 0.7498  max mem: 13238
Epoch: [2] Total time: 0:00:24 (1.2935 s / it)
Averaged stats: lr: 0.000059  min_lr: 0.000059  loss: 1.7489 (1.7479)  quant_loss: 0.0028 (0.0030)  rec_loss: 0.7951 (0.7936)  rec_angle_loss: 0.9511 (0.9514)  total_loss: 1.7489 (1.7479)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6300 (0.6279)
Unused code in codebook: 6625
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 1.6614 (1.6614)  time: 5.4934  data: 5.4159  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.6596 (1.6551)  time: 0.5562  data: 0.4924  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.6596 (1.6568)  time: 0.4466  data: 0.3869  max mem: 13238
Validation: Total time: 0:00:07 (0.5395 s / it)
Averaged stats: loss: 1.6596 (1.6569)  quant_loss: 0.0019 (0.0020)  rec_loss: 0.7480 (0.7480)  rec_angle_loss: 0.9230 (0.9232)  total_loss: 1.6729 (1.6732)
Unused code in codebook: 7749
Validation loss of the network on the 10144 test EEG: 1.6569
Reset the codebook statistic info in quantizer before each epoch
Epoch: [3]  [ 0/19]  eta: 0:03:53  lr: 0.000060  min_lr: 0.000060  loss: 1.6571 (1.6571)  quant_loss: 0.0019 (0.0019)  rec_loss: 0.7223 (0.7223)  rec_angle_loss: 0.9329 (0.9329)  total_loss: 1.6571 (1.6571)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6338 (0.6338)  time: 12.3027  data: 11.2609  max mem: 13238
Epoch: [3]  [10/19]  eta: 0:00:15  lr: 0.000071  min_lr: 0.000071  loss: 1.6106 (1.6116)  quant_loss: 0.0015 (0.0015)  rec_loss: 0.6842 (0.6855)  rec_angle_loss: 0.9243 (0.9245)  total_loss: 1.6106 (1.6116)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6091 (0.6079)  time: 1.7316  data: 1.2871  max mem: 13238
Epoch: [3]  [18/19]  eta: 0:00:01  lr: 0.000079  min_lr: 0.000079  loss: 1.5722 (1.5754)  quant_loss: 0.0012 (0.0013)  rec_loss: 0.6529 (0.6550)  rec_angle_loss: 0.9180 (0.9192)  total_loss: 1.5722 (1.5754)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5848 (0.5809)  time: 1.1536  data: 0.7452  max mem: 13238
Epoch: [3] Total time: 0:00:23 (1.2285 s / it)
Averaged stats: lr: 0.000079  min_lr: 0.000079  loss: 1.5722 (1.5760)  quant_loss: 0.0012 (0.0014)  rec_loss: 0.6529 (0.6557)  rec_angle_loss: 0.9180 (0.9189)  total_loss: 1.5722 (1.5760)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5848 (0.5809)
Unused code in codebook: 7744
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 1.4948 (1.4948)  time: 5.5630  data: 5.4863  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.4907 (1.4817)  time: 0.5722  data: 0.5100  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.4907 (1.4859)  time: 0.4590  data: 0.4007  max mem: 13238
Validation: Total time: 0:00:07 (0.5535 s / it)
Averaged stats: loss: 1.4907 (1.4859)  quant_loss: 0.0008 (0.0008)  rec_loss: 0.6320 (0.6323)  rec_angle_loss: 0.8871 (0.8874)  total_loss: 1.5199 (1.5205)
Unused code in codebook: 8017
Validation loss of the network on the 10144 test EEG: 1.4859
Reset the codebook statistic info in quantizer before each epoch
Epoch: [4]  [ 0/19]  eta: 0:03:37  lr: 0.000080  min_lr: 0.000080  loss: 1.4847 (1.4847)  quant_loss: 0.0008 (0.0008)  rec_loss: 0.5804 (0.5804)  rec_angle_loss: 0.9036 (0.9036)  total_loss: 1.4847 (1.4847)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4970 (0.4970)  time: 11.4343  data: 10.0282  max mem: 13238
Epoch: [4]  [10/19]  eta: 0:00:15  lr: 0.000091  min_lr: 0.000091  loss: 1.4624 (1.4624)  quant_loss: 0.0008 (0.0008)  rec_loss: 0.5572 (0.5592)  rec_angle_loss: 0.9029 (0.9025)  total_loss: 1.4624 (1.4624)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4674 (0.4588)  time: 1.7019  data: 1.1245  max mem: 13238
Epoch: [4]  [18/19]  eta: 0:00:01  lr: 0.000099  min_lr: 0.000099  loss: 1.4440 (1.4445)  quant_loss: 0.0007 (0.0007)  rec_loss: 0.5425 (0.5428)  rec_angle_loss: 0.9008 (0.9010)  total_loss: 1.4440 (1.4445)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4154 (0.4232)  time: 1.1360  data: 0.6511  max mem: 13238
Epoch: [4] Total time: 0:00:22 (1.2091 s / it)
Averaged stats: lr: 0.000099  min_lr: 0.000099  loss: 1.4440 (1.4447)  quant_loss: 0.0007 (0.0006)  rec_loss: 0.5425 (0.5426)  rec_angle_loss: 0.9008 (0.9014)  total_loss: 1.4440 (1.4447)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4154 (0.4232)
Unused code in codebook: 7992
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:18  loss: 1.4000 (1.4000)  time: 5.6000  data: 5.5232  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.3991 (1.3954)  time: 0.5643  data: 0.5022  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.3991 (1.3975)  time: 0.4530  data: 0.3946  max mem: 13238
Validation: Total time: 0:00:07 (0.5469 s / it)
Averaged stats: loss: 1.3991 (1.3972)  quant_loss: 0.0006 (0.0005)  rec_loss: 0.5476 (0.5480)  rec_angle_loss: 0.8776 (0.8778)  total_loss: 1.4258 (1.4263)
Unused code in codebook: 8106
Validation loss of the network on the 10144 test EEG: 1.3972
Reset the codebook statistic info in quantizer before each epoch
Epoch: [5]  [ 0/19]  eta: 0:03:52  lr: 0.000101  min_lr: 0.000101  loss: 1.4158 (1.4158)  quant_loss: 0.0006 (0.0006)  rec_loss: 0.5120 (0.5120)  rec_angle_loss: 0.9031 (0.9031)  total_loss: 1.4158 (1.4158)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3371 (0.3371)  time: 12.2347  data: 10.0226  max mem: 13238
Epoch: [5]  [10/19]  eta: 0:00:15  lr: 0.000111  min_lr: 0.000111  loss: 1.3979 (1.3985)  quant_loss: 0.0006 (0.0006)  rec_loss: 0.4970 (0.4982)  rec_angle_loss: 0.9007 (0.8998)  total_loss: 1.3979 (1.3985)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2974 (0.3014)  time: 1.7232  data: 1.0701  max mem: 13238
Epoch: [5]  [18/19]  eta: 0:00:01  lr: 0.000120  min_lr: 0.000120  loss: 1.3903 (1.3911)  quant_loss: 0.0005 (0.0005)  rec_loss: 0.4901 (0.4907)  rec_angle_loss: 0.8999 (0.8998)  total_loss: 1.3903 (1.3911)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2742 (0.2730)  time: 1.1499  data: 0.6195  max mem: 13238
Epoch: [5] Total time: 0:00:23 (1.2230 s / it)
Averaged stats: lr: 0.000120  min_lr: 0.000120  loss: 1.3903 (1.3875)  quant_loss: 0.0005 (0.0004)  rec_loss: 0.4901 (0.4881)  rec_angle_loss: 0.8999 (0.8990)  total_loss: 1.3903 (1.3875)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2742 (0.2730)
Unused code in codebook: 8062
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:19  loss: 1.3454 (1.3454)  time: 5.6829  data: 5.6058  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.3580 (1.3645)  time: 0.5719  data: 0.5097  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.3531 (1.3599)  time: 0.4589  data: 0.4005  max mem: 13238
Validation: Total time: 0:00:07 (0.5499 s / it)
Averaged stats: loss: 1.3531 (1.3597)  quant_loss: 0.0004 (0.0004)  rec_loss: 0.4788 (0.4791)  rec_angle_loss: 0.8772 (0.8774)  total_loss: 1.3564 (1.3569)
Unused code in codebook: 8124
Validation loss of the network on the 10144 test EEG: 1.3597
Reset the codebook statistic info in quantizer before each epoch
Epoch: [6]  [ 0/19]  eta: 0:03:43  lr: 0.000121  min_lr: 0.000121  loss: 1.3707 (1.3707)  quant_loss: 0.0004 (0.0004)  rec_loss: 0.4748 (0.4748)  rec_angle_loss: 0.8955 (0.8955)  total_loss: 1.3707 (1.3707)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2101 (0.2101)  time: 11.7564  data: 10.8151  max mem: 13238
Epoch: [6]  [10/19]  eta: 0:00:15  lr: 0.000131  min_lr: 0.000131  loss: 1.3668 (1.3667)  quant_loss: 0.0004 (0.0004)  rec_loss: 0.4660 (0.4674)  rec_angle_loss: 0.8988 (0.8989)  total_loss: 1.3668 (1.3667)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.1752 (0.1734)  time: 1.7355  data: 1.3053  max mem: 13238
Epoch: [6]  [18/19]  eta: 0:00:01  lr: 0.000140  min_lr: 0.000140  loss: 1.3668 (1.3666)  quant_loss: 0.0004 (0.0004)  rec_loss: 0.4653 (0.4673)  rec_angle_loss: 0.8988 (0.8989)  total_loss: 1.3668 (1.3666)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.1494 (0.1495)  time: 1.1562  data: 0.7558  max mem: 13238
Epoch: [6] Total time: 0:00:23 (1.2246 s / it)
Averaged stats: lr: 0.000140  min_lr: 0.000140  loss: 1.3668 (1.3643)  quant_loss: 0.0004 (0.0003)  rec_loss: 0.4653 (0.4653)  rec_angle_loss: 0.8988 (0.8988)  total_loss: 1.3668 (1.3643)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.1494 (0.1495)
Unused code in codebook: 8093
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 1.3124 (1.3124)  time: 5.4434  data: 5.3621  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.3483 (1.3547)  time: 0.5655  data: 0.5039  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.3247 (1.3444)  time: 0.4538  data: 0.3960  max mem: 13238
Validation: Total time: 0:00:07 (0.5466 s / it)
Averaged stats: loss: 1.3247 (1.3443)  quant_loss: 0.0003 (0.0003)  rec_loss: 0.4357 (0.4360)  rec_angle_loss: 0.8775 (0.8777)  total_loss: 1.3136 (1.3140)
Unused code in codebook: 8131
Validation loss of the network on the 10144 test EEG: 1.3443
Reset the codebook statistic info in quantizer before each epoch
Epoch: [7]  [ 0/19]  eta: 0:03:37  lr: 0.000141  min_lr: 0.000141  loss: 1.3577 (1.3577)  quant_loss: 0.0003 (0.0003)  rec_loss: 0.4566 (0.4566)  rec_angle_loss: 0.9007 (0.9007)  total_loss: 1.3577 (1.3577)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0968 (0.0968)  time: 11.4588  data: 10.3338  max mem: 13238
Epoch: [7]  [10/19]  eta: 0:00:15  lr: 0.000151  min_lr: 0.000151  loss: 1.3592 (1.3582)  quant_loss: 0.0003 (0.0003)  rec_loss: 0.4594 (0.4580)  rec_angle_loss: 0.8997 (0.8999)  total_loss: 1.3592 (1.3582)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0788 (0.0763)  time: 1.6957  data: 1.1878  max mem: 13238
Epoch: [7]  [18/19]  eta: 0:00:01  lr: 0.000160  min_lr: 0.000160  loss: 1.3577 (1.3570)  quant_loss: 0.0003 (0.0003)  rec_loss: 0.4566 (0.4575)  rec_angle_loss: 0.8988 (0.8992)  total_loss: 1.3577 (1.3570)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0615 (0.0614)  time: 1.1329  data: 0.6877  max mem: 13238
Epoch: [7] Total time: 0:00:22 (1.2058 s / it)
Averaged stats: lr: 0.000160  min_lr: 0.000160  loss: 1.3577 (1.3571)  quant_loss: 0.0003 (0.0003)  rec_loss: 0.4566 (0.4579)  rec_angle_loss: 0.8988 (0.8989)  total_loss: 1.3577 (1.3571)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0615 (0.0614)
Unused code in codebook: 8101
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:23  loss: 1.2979 (1.2979)  time: 5.9756  data: 5.8956  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.3403 (1.3532)  time: 0.5998  data: 0.5360  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.3173 (1.3403)  time: 0.4807  data: 0.4212  max mem: 13238
Validation: Total time: 0:00:08 (0.5759 s / it)
Averaged stats: loss: 1.3173 (1.3401)  quant_loss: 0.0004 (0.0004)  rec_loss: 0.4183 (0.4185)  rec_angle_loss: 0.8773 (0.8775)  total_loss: 1.2960 (1.2963)
Unused code in codebook: 8136
Validation loss of the network on the 10144 test EEG: 1.3401
Reset the codebook statistic info in quantizer before each epoch
Epoch: [8]  [ 0/19]  eta: 0:03:40  lr: 0.000161  min_lr: 0.000161  loss: 1.3585 (1.3585)  quant_loss: 0.0004 (0.0004)  rec_loss: 0.4573 (0.4573)  rec_angle_loss: 0.9009 (0.9009)  total_loss: 1.3585 (1.3585)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0296 (0.0296)  time: 11.6199  data: 10.5473  max mem: 13238
Epoch: [8]  [10/19]  eta: 0:00:15  lr: 0.000171  min_lr: 0.000171  loss: 1.3549 (1.3525)  quant_loss: 0.0005 (0.0005)  rec_loss: 0.4521 (0.4537)  rec_angle_loss: 0.8980 (0.8983)  total_loss: 1.3549 (1.3525)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0244 (0.0274)  time: 1.7004  data: 1.2219  max mem: 13238
Epoch: [8]  [18/19]  eta: 0:00:01  lr: 0.000180  min_lr: 0.000180  loss: 1.3549 (1.3544)  quant_loss: 0.0007 (0.0012)  rec_loss: 0.4539 (0.4550)  rec_angle_loss: 0.8980 (0.8982)  total_loss: 1.3549 (1.3544)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0248 (0.0310)  time: 1.1361  data: 0.7075  max mem: 13238
Epoch: [8] Total time: 0:00:22 (1.2081 s / it)
Averaged stats: lr: 0.000180  min_lr: 0.000180  loss: 1.3549 (1.3558)  quant_loss: 0.0007 (0.0012)  rec_loss: 0.4539 (0.4565)  rec_angle_loss: 0.8980 (0.8981)  total_loss: 1.3549 (1.3558)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0248 (0.0310)
Unused code in codebook: 8044
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 1.2929 (1.2929)  time: 5.4472  data: 5.3685  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.3355 (1.3512)  time: 0.5602  data: 0.4974  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.3129 (1.3372)  time: 0.4498  data: 0.3908  max mem: 13238
Validation: Total time: 0:00:07 (0.5383 s / it)
Averaged stats: loss: 1.3129 (1.3377)  quant_loss: 0.0066 (0.0066)  rec_loss: 0.4121 (0.4126)  rec_angle_loss: 0.8681 (0.8689)  total_loss: 1.2868 (1.2880)
Unused code in codebook: 7996
Validation loss of the network on the 10144 test EEG: 1.3377
Reset the codebook statistic info in quantizer before each epoch
Epoch: [9]  [ 0/19]  eta: 0:03:38  lr: 0.000181  min_lr: 0.000181  loss: 1.3437 (1.3437)  quant_loss: 0.0062 (0.0062)  rec_loss: 0.4444 (0.4444)  rec_angle_loss: 0.8930 (0.8930)  total_loss: 1.3437 (1.3437)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.0826 (0.0826)  time: 11.5075  data: 10.0856  max mem: 13238
Epoch: [9]  [10/19]  eta: 0:00:15  lr: 0.000192  min_lr: 0.000192  loss: 1.3311 (1.3315)  quant_loss: 0.0063 (0.0069)  rec_loss: 0.4538 (0.4540)  rec_angle_loss: 0.8690 (0.8705)  total_loss: 1.3311 (1.3315)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3755 (0.3849)  time: 1.6759  data: 1.1509  max mem: 13238
Epoch: [9]  [18/19]  eta: 0:00:01  lr: 0.000200  min_lr: 0.000200  loss: 1.3150 (1.3175)  quant_loss: 0.0047 (0.0054)  rec_loss: 0.4550 (0.4559)  rec_angle_loss: 0.8556 (0.8562)  total_loss: 1.3150 (1.3175)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3496 (0.3722)  time: 1.1226  data: 0.6664  max mem: 13238
Epoch: [9] Total time: 0:00:22 (1.1964 s / it)
Averaged stats: lr: 0.000200  min_lr: 0.000200  loss: 1.3150 (1.3149)  quant_loss: 0.0047 (0.0054)  rec_loss: 0.4550 (0.4542)  rec_angle_loss: 0.8556 (0.8553)  total_loss: 1.3150 (1.3149)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3496 (0.3722)
Unused code in codebook: 7757
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 1.2165 (1.2165)  time: 5.5550  data: 5.4794  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.2626 (1.2815)  time: 0.5632  data: 0.5017  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.2336 (1.2590)  time: 0.4521  data: 0.3942  max mem: 13238
Validation: Total time: 0:00:07 (0.5443 s / it)
Averaged stats: loss: 1.2336 (1.2590)  quant_loss: 0.0024 (0.0024)  rec_loss: 0.4215 (0.4216)  rec_angle_loss: 0.7442 (0.7460)  total_loss: 1.1681 (1.1700)
Unused code in codebook: 8071
Validation loss of the network on the 10144 test EEG: 1.2590
Reset the codebook statistic info in quantizer before each epoch
Epoch: [10]  [ 0/19]  eta: 0:03:44  lr: 0.000200  min_lr: 0.000200  loss: 1.2744 (1.2744)  quant_loss: 0.0024 (0.0024)  rec_loss: 0.4507 (0.4507)  rec_angle_loss: 0.8213 (0.8213)  total_loss: 1.2744 (1.2744)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3279 (0.3279)  time: 11.8272  data: 10.5139  max mem: 13238
Epoch: [10]  [10/19]  eta: 0:00:15  lr: 0.000200  min_lr: 0.000200  loss: 1.2743 (1.2697)  quant_loss: 0.0021 (0.0020)  rec_loss: 0.4541 (0.4538)  rec_angle_loss: 0.8136 (0.8139)  total_loss: 1.2743 (1.2697)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3186 (0.3182)  time: 1.7148  data: 1.1686  max mem: 13238
Epoch: [10]  [18/19]  eta: 0:00:01  lr: 0.000200  min_lr: 0.000200  loss: 1.2673 (1.2638)  quant_loss: 0.0019 (0.0019)  rec_loss: 0.4538 (0.4539)  rec_angle_loss: 0.8088 (0.8080)  total_loss: 1.2673 (1.2638)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3123 (0.3130)  time: 1.1452  data: 0.6766  max mem: 13238
Epoch: [10] Total time: 0:00:23 (1.2203 s / it)
Averaged stats: lr: 0.000200  min_lr: 0.000200  loss: 1.2673 (1.2631)  quant_loss: 0.0019 (0.0019)  rec_loss: 0.4538 (0.4525)  rec_angle_loss: 0.8088 (0.8086)  total_loss: 1.2673 (1.2631)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3123 (0.3130)
Unused code in codebook: 7950
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 1.1827 (1.1827)  time: 5.4659  data: 5.3889  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.2385 (1.2544)  time: 0.5613  data: 0.4970  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.2034 (1.2292)  time: 0.4506  data: 0.3905  max mem: 13238
Validation: Total time: 0:00:07 (0.5440 s / it)
Averaged stats: loss: 1.2034 (1.2292)  quant_loss: 0.0021 (0.0021)  rec_loss: 0.4107 (0.4106)  rec_angle_loss: 0.7082 (0.7123)  total_loss: 1.1210 (1.1250)
Unused code in codebook: 7987
Validation loss of the network on the 10144 test EEG: 1.2292
Reset the codebook statistic info in quantizer before each epoch
Epoch: [11]  [ 0/19]  eta: 0:04:04  lr: 0.000200  min_lr: 0.000200  loss: 1.2697 (1.2697)  quant_loss: 0.0022 (0.0022)  rec_loss: 0.4636 (0.4636)  rec_angle_loss: 0.8039 (0.8039)  total_loss: 1.2697 (1.2697)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3195 (0.3195)  time: 12.8777  data: 11.5039  max mem: 13238
Epoch: [11]  [10/19]  eta: 0:00:15  lr: 0.000200  min_lr: 0.000200  loss: 1.2418 (1.2420)  quant_loss: 0.0027 (0.0032)  rec_loss: 0.4464 (0.4473)  rec_angle_loss: 0.7926 (0.7914)  total_loss: 1.2418 (1.2420)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3184 (0.3464)  time: 1.7598  data: 1.2745  max mem: 13238
Epoch: [11]  [18/19]  eta: 0:00:01  lr: 0.000200  min_lr: 0.000200  loss: 1.2374 (1.2356)  quant_loss: 0.0045 (0.0041)  rec_loss: 0.4425 (0.4432)  rec_angle_loss: 0.7883 (0.7883)  total_loss: 1.2374 (1.2356)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3338 (0.3757)  time: 1.1705  data: 0.7379  max mem: 13238
Epoch: [11] Total time: 0:00:23 (1.2419 s / it)
Averaged stats: lr: 0.000200  min_lr: 0.000200  loss: 1.2374 (1.2351)  quant_loss: 0.0045 (0.0040)  rec_loss: 0.4425 (0.4427)  rec_angle_loss: 0.7883 (0.7884)  total_loss: 1.2374 (1.2351)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3338 (0.3757)
Unused code in codebook: 7609
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:13  loss: 1.1657 (1.1657)  time: 5.2733  data: 5.1825  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.2078 (1.2213)  time: 0.5673  data: 0.4928  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.1818 (1.1993)  time: 0.4552  data: 0.3872  max mem: 13238
Validation: Total time: 0:00:07 (0.5479 s / it)
Averaged stats: loss: 1.1818 (1.1987)  quant_loss: 0.0049 (0.0048)  rec_loss: 0.4038 (0.4023)  rec_angle_loss: 0.6952 (0.7003)  total_loss: 1.1040 (1.1074)
Unused code in codebook: 7889
Validation loss of the network on the 10144 test EEG: 1.1987
Reset the codebook statistic info in quantizer before each epoch
Epoch: [12]  [ 0/19]  eta: 0:03:42  lr: 0.000200  min_lr: 0.000200  loss: 1.2112 (1.2112)  quant_loss: 0.0048 (0.0048)  rec_loss: 0.4239 (0.4239)  rec_angle_loss: 0.7825 (0.7825)  total_loss: 1.2112 (1.2112)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3150 (0.3150)  time: 11.7279  data: 10.6062  max mem: 13238
Epoch: [12]  [10/19]  eta: 0:00:15  lr: 0.000200  min_lr: 0.000200  loss: 1.2094 (1.2080)  quant_loss: 0.0044 (0.0043)  rec_loss: 0.4242 (0.4244)  rec_angle_loss: 0.7797 (0.7792)  total_loss: 1.2094 (1.2080)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3837 (0.3724)  time: 1.7149  data: 1.2379  max mem: 13238
Epoch: [12]  [18/19]  eta: 0:00:01  lr: 0.000199  min_lr: 0.000199  loss: 1.2025 (1.2003)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.4231 (0.4203)  rec_angle_loss: 0.7757 (0.7758)  total_loss: 1.2025 (1.2003)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3890 (0.3751)  time: 1.1446  data: 0.7167  max mem: 13238
Epoch: [12] Total time: 0:00:23 (1.2187 s / it)
Averaged stats: lr: 0.000199  min_lr: 0.000199  loss: 1.2025 (1.1977)  quant_loss: 0.0043 (0.0041)  rec_loss: 0.4231 (0.4196)  rec_angle_loss: 0.7757 (0.7740)  total_loss: 1.2025 (1.1977)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3890 (0.3751)
Unused code in codebook: 7874
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 1.1438 (1.1438)  time: 5.4159  data: 5.3427  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.1842 (1.1907)  time: 0.5693  data: 0.5067  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.1540 (1.1703)  time: 0.4568  data: 0.3981  max mem: 13238
Validation: Total time: 0:00:07 (0.5479 s / it)
Averaged stats: loss: 1.1540 (1.1699)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.3872 (0.3876)  rec_angle_loss: 0.6875 (0.6908)  total_loss: 1.0782 (1.0819)
Unused code in codebook: 7927
Validation loss of the network on the 10144 test EEG: 1.1699
Reset the codebook statistic info in quantizer before each epoch
Epoch: [13]  [ 0/19]  eta: 0:03:47  lr: 0.000199  min_lr: 0.000199  loss: 1.1779 (1.1779)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.4113 (0.4113)  rec_angle_loss: 0.7631 (0.7631)  total_loss: 1.1779 (1.1779)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6291 (0.6291)  time: 11.9480  data: 10.3260  max mem: 13238
Epoch: [13]  [10/19]  eta: 0:00:15  lr: 0.000199  min_lr: 0.000199  loss: 1.1728 (1.1728)  quant_loss: 0.0035 (0.0036)  rec_loss: 0.4062 (0.4069)  rec_angle_loss: 0.7625 (0.7624)  total_loss: 1.1728 (1.1728)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3437 (0.3787)  time: 1.7049  data: 1.1363  max mem: 13238
Epoch: [13]  [18/19]  eta: 0:00:01  lr: 0.000199  min_lr: 0.000199  loss: 1.1665 (1.1659)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.4031 (0.4033)  rec_angle_loss: 0.7595 (0.7592)  total_loss: 1.1665 (1.1659)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3437 (0.3626)  time: 1.1389  data: 0.6579  max mem: 13238
Epoch: [13] Total time: 0:00:23 (1.2130 s / it)
Averaged stats: lr: 0.000199  min_lr: 0.000199  loss: 1.1665 (1.1678)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.4031 (0.4040)  rec_angle_loss: 0.7595 (0.7603)  total_loss: 1.1665 (1.1678)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3437 (0.3626)
Unused code in codebook: 7926
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:18  loss: 1.1223 (1.1223)  time: 5.6145  data: 5.5378  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.1590 (1.1649)  time: 0.5659  data: 0.5035  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.1310 (1.1449)  time: 0.4541  data: 0.3956  max mem: 13238
Validation: Total time: 0:00:07 (0.5463 s / it)
Averaged stats: loss: 1.1310 (1.1445)  quant_loss: 0.0031 (0.0031)  rec_loss: 0.3725 (0.3728)  rec_angle_loss: 0.6747 (0.6780)  total_loss: 1.0503 (1.0539)
Unused code in codebook: 7936
Validation loss of the network on the 10144 test EEG: 1.1445
Reset the codebook statistic info in quantizer before each epoch
Epoch: [14]  [ 0/19]  eta: 0:03:46  lr: 0.000199  min_lr: 0.000199  loss: 1.1588 (1.1588)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.4014 (0.4014)  rec_angle_loss: 0.7542 (0.7542)  total_loss: 1.1588 (1.1588)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2940 (0.2940)  time: 11.9180  data: 10.6513  max mem: 13238
Epoch: [14]  [10/19]  eta: 0:00:15  lr: 0.000199  min_lr: 0.000199  loss: 1.1469 (1.1472)  quant_loss: 0.0030 (0.0029)  rec_loss: 0.3960 (0.3958)  rec_angle_loss: 0.7469 (0.7485)  total_loss: 1.1469 (1.1472)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2780 (0.2769)  time: 1.7100  data: 1.2087  max mem: 13238
Epoch: [14]  [18/19]  eta: 0:00:01  lr: 0.000199  min_lr: 0.000199  loss: 1.1430 (1.1429)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.3947 (0.3944)  rec_angle_loss: 0.7443 (0.7457)  total_loss: 1.1430 (1.1429)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2940 (0.3493)  time: 1.1412  data: 0.6998  max mem: 13238
Epoch: [14] Total time: 0:00:23 (1.2133 s / it)
Averaged stats: lr: 0.000199  min_lr: 0.000199  loss: 1.1430 (1.1433)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.3947 (0.3941)  rec_angle_loss: 0.7443 (0.7465)  total_loss: 1.1430 (1.1433)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2940 (0.3493)
Unused code in codebook: 7935
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 1.1057 (1.1057)  time: 5.4951  data: 5.4191  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.1425 (1.1458)  time: 0.5730  data: 0.4987  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.1124 (1.1271)  time: 0.4598  data: 0.3919  max mem: 13238
Validation: Total time: 0:00:07 (0.5541 s / it)
Averaged stats: loss: 1.1124 (1.1266)  quant_loss: 0.0022 (0.0022)  rec_loss: 0.3651 (0.3649)  rec_angle_loss: 0.6705 (0.6718)  total_loss: 1.0378 (1.0388)
Unused code in codebook: 7943
Validation loss of the network on the 10144 test EEG: 1.1266
Reset the codebook statistic info in quantizer before each epoch
Epoch: [15]  [ 0/19]  eta: 0:03:48  lr: 0.000199  min_lr: 0.000199  loss: 1.1386 (1.1386)  quant_loss: 0.0025 (0.0025)  rec_loss: 0.3960 (0.3960)  rec_angle_loss: 0.7401 (0.7401)  total_loss: 1.1386 (1.1386)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5191 (0.5191)  time: 12.0333  data: 10.3575  max mem: 13238
Epoch: [15]  [10/19]  eta: 0:00:16  lr: 0.000198  min_lr: 0.000198  loss: 1.1268 (1.1265)  quant_loss: 0.0025 (0.0025)  rec_loss: 0.3860 (0.3872)  rec_angle_loss: 0.7381 (0.7368)  total_loss: 1.1268 (1.1265)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4018 (0.3929)  time: 1.8344  data: 1.2823  max mem: 13238
Epoch: [15]  [18/19]  eta: 0:00:01  lr: 0.000198  min_lr: 0.000198  loss: 1.1210 (1.1206)  quant_loss: 0.0023 (0.0023)  rec_loss: 0.3847 (0.3848)  rec_angle_loss: 0.7347 (0.7334)  total_loss: 1.1210 (1.1206)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3575 (0.3702)  time: 1.2135  data: 0.7424  max mem: 13238
Epoch: [15] Total time: 0:00:24 (1.2857 s / it)
Averaged stats: lr: 0.000198  min_lr: 0.000198  loss: 1.1210 (1.1216)  quant_loss: 0.0023 (0.0023)  rec_loss: 0.3847 (0.3848)  rec_angle_loss: 0.7347 (0.7344)  total_loss: 1.1210 (1.1216)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3575 (0.3702)
Unused code in codebook: 7901
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:18  loss: 1.0906 (1.0906)  time: 5.6123  data: 5.5356  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.1284 (1.1296)  time: 0.5654  data: 0.5033  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0973 (1.1106)  time: 0.4537  data: 0.3955  max mem: 13238
Validation: Total time: 0:00:07 (0.5460 s / it)
Averaged stats: loss: 1.0973 (1.1100)  quant_loss: 0.0018 (0.0018)  rec_loss: 0.3532 (0.3525)  rec_angle_loss: 0.6582 (0.6593)  total_loss: 1.0131 (1.0136)
Unused code in codebook: 7761
Validation loss of the network on the 10144 test EEG: 1.1100
Reset the codebook statistic info in quantizer before each epoch
Epoch: [16]  [ 0/19]  eta: 0:03:51  lr: 0.000198  min_lr: 0.000198  loss: 1.1174 (1.1174)  quant_loss: 0.0023 (0.0023)  rec_loss: 0.3810 (0.3810)  rec_angle_loss: 0.7342 (0.7342)  total_loss: 1.1174 (1.1174)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.8236 (0.8236)  time: 12.1634  data: 11.1241  max mem: 13238
Epoch: [16]  [10/19]  eta: 0:00:15  lr: 0.000198  min_lr: 0.000198  loss: 1.1072 (1.1068)  quant_loss: 0.0023 (0.0023)  rec_loss: 0.3788 (0.3784)  rec_angle_loss: 0.7255 (0.7261)  total_loss: 1.1072 (1.1068)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7102 (0.7826)  time: 1.6980  data: 1.2407  max mem: 13238
Epoch: [16]  [18/19]  eta: 0:00:01  lr: 0.000197  min_lr: 0.000197  loss: 1.1005 (1.1009)  quant_loss: 0.0025 (0.0027)  rec_loss: 0.3726 (0.3737)  rec_angle_loss: 0.7243 (0.7245)  total_loss: 1.1005 (1.1009)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7102 (0.7439)  time: 1.1349  data: 0.7184  max mem: 13238
Epoch: [16] Total time: 0:00:22 (1.2038 s / it)
Averaged stats: lr: 0.000197  min_lr: 0.000197  loss: 1.1005 (1.1016)  quant_loss: 0.0025 (0.0026)  rec_loss: 0.3726 (0.3740)  rec_angle_loss: 0.7243 (0.7250)  total_loss: 1.1005 (1.1016)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7102 (0.7439)
Unused code in codebook: 7543
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:18  loss: 1.0743 (1.0743)  time: 5.6319  data: 5.4064  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.1125 (1.1097)  time: 0.5680  data: 0.4916  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0849 (1.0904)  time: 0.4558  data: 0.3862  max mem: 13238
Validation: Total time: 0:00:07 (0.5486 s / it)
Averaged stats: loss: 1.0849 (1.0906)  quant_loss: 0.0023 (0.0023)  rec_loss: 0.3286 (0.3282)  rec_angle_loss: 0.6574 (0.6578)  total_loss: 0.9883 (0.9883)
Unused code in codebook: 7774
Validation loss of the network on the 10144 test EEG: 1.0906
Reset the codebook statistic info in quantizer before each epoch
Epoch: [17]  [ 0/19]  eta: 0:03:45  lr: 0.000197  min_lr: 0.000197  loss: 1.0731 (1.0731)  quant_loss: 0.0033 (0.0033)  rec_loss: 0.3561 (0.3561)  rec_angle_loss: 0.7136 (0.7136)  total_loss: 1.0731 (1.0731)  weight_decay: 0.0001 (0.0001)  grad_norm: 1.0167 (1.0167)  time: 11.8603  data: 10.2664  max mem: 13238
Epoch: [17]  [10/19]  eta: 0:00:15  lr: 0.000197  min_lr: 0.000197  loss: 1.0776 (1.0769)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.3559 (0.3564)  rec_angle_loss: 0.7180 (0.7170)  total_loss: 1.0776 (1.0769)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4117 (0.4830)  time: 1.6997  data: 1.1163  max mem: 13238
Epoch: [17]  [18/19]  eta: 0:00:01  lr: 0.000196  min_lr: 0.000196  loss: 1.0713 (1.0715)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.3537 (0.3533)  rec_angle_loss: 0.7146 (0.7147)  total_loss: 1.0713 (1.0715)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4163 (0.4482)  time: 1.1361  data: 0.6463  max mem: 13238
Epoch: [17] Total time: 0:00:22 (1.2087 s / it)
Averaged stats: lr: 0.000196  min_lr: 0.000196  loss: 1.0713 (1.0728)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.3537 (0.3538)  rec_angle_loss: 0.7146 (0.7155)  total_loss: 1.0713 (1.0728)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4163 (0.4482)
Unused code in codebook: 7784
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 1.0522 (1.0522)  time: 5.4636  data: 5.3847  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0910 (1.0880)  time: 0.5558  data: 0.4896  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0645 (1.0673)  time: 0.4464  data: 0.3847  max mem: 13238
Validation: Total time: 0:00:07 (0.5374 s / it)
Averaged stats: loss: 1.0645 (1.0671)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.2972 (0.2968)  rec_angle_loss: 0.6551 (0.6542)  total_loss: 0.9556 (0.9541)
Unused code in codebook: 7816
Validation loss of the network on the 10144 test EEG: 1.0671
Reset the codebook statistic info in quantizer before each epoch
Epoch: [18]  [ 0/19]  eta: 0:03:42  lr: 0.000196  min_lr: 0.000196  loss: 1.0607 (1.0607)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.3482 (0.3482)  rec_angle_loss: 0.7090 (0.7090)  total_loss: 1.0607 (1.0607)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2893 (0.2893)  time: 11.7213  data: 10.7646  max mem: 13238
Epoch: [18]  [10/19]  eta: 0:00:15  lr: 0.000196  min_lr: 0.000196  loss: 1.0535 (1.0555)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.3435 (0.3440)  rec_angle_loss: 0.7070 (0.7081)  total_loss: 1.0535 (1.0555)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3630 (0.3889)  time: 1.7136  data: 1.2546  max mem: 13238
Epoch: [18]  [18/19]  eta: 0:00:01  lr: 0.000195  min_lr: 0.000195  loss: 1.0524 (1.0517)  quant_loss: 0.0032 (0.0033)  rec_loss: 0.3423 (0.3425)  rec_angle_loss: 0.7067 (0.7060)  total_loss: 1.0524 (1.0517)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3743 (0.3982)  time: 1.1435  data: 0.7264  max mem: 13238
Epoch: [18] Total time: 0:00:23 (1.2130 s / it)
Averaged stats: lr: 0.000195  min_lr: 0.000195  loss: 1.0524 (1.0521)  quant_loss: 0.0032 (0.0033)  rec_loss: 0.3423 (0.3423)  rec_angle_loss: 0.7067 (0.7065)  total_loss: 1.0524 (1.0521)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3743 (0.3982)
Unused code in codebook: 7817
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 1.0396 (1.0396)  time: 5.5341  data: 5.4540  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0787 (1.0752)  time: 0.5712  data: 0.5073  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0528 (1.0541)  time: 0.4582  data: 0.3986  max mem: 13238
Validation: Total time: 0:00:07 (0.5483 s / it)
Averaged stats: loss: 1.0528 (1.0539)  quant_loss: 0.0025 (0.0025)  rec_loss: 0.2839 (0.2836)  rec_angle_loss: 0.6550 (0.6513)  total_loss: 0.9414 (0.9374)
Unused code in codebook: 7828
Validation loss of the network on the 10144 test EEG: 1.0539
Reset the codebook statistic info in quantizer before each epoch
Epoch: [19]  [ 0/19]  eta: 0:03:50  lr: 0.000195  min_lr: 0.000195  loss: 1.0424 (1.0424)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.3380 (0.3380)  rec_angle_loss: 0.7014 (0.7014)  total_loss: 1.0424 (1.0424)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3476 (0.3476)  time: 12.1499  data: 11.5012  max mem: 13238
Epoch: [19]  [10/19]  eta: 0:00:15  lr: 0.000195  min_lr: 0.000195  loss: 1.0381 (1.0363)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.3340 (0.3350)  rec_angle_loss: 0.6978 (0.6985)  total_loss: 1.0381 (1.0363)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3197 (0.3307)  time: 1.7249  data: 1.2644  max mem: 13238
Epoch: [19]  [18/19]  eta: 0:00:01  lr: 0.000194  min_lr: 0.000194  loss: 1.0362 (1.0347)  quant_loss: 0.0027 (0.0027)  rec_loss: 0.3337 (0.3342)  rec_angle_loss: 0.6973 (0.6978)  total_loss: 1.0362 (1.0347)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3656 (0.5157)  time: 1.1496  data: 0.7321  max mem: 13238
Epoch: [19] Total time: 0:00:23 (1.2243 s / it)
Averaged stats: lr: 0.000194  min_lr: 0.000194  loss: 1.0362 (1.0360)  quant_loss: 0.0027 (0.0027)  rec_loss: 0.3337 (0.3354)  rec_angle_loss: 0.6973 (0.6979)  total_loss: 1.0362 (1.0360)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3656 (0.5157)
Unused code in codebook: 7828
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:29  loss: 1.0351 (1.0351)  time: 6.4255  data: 6.3456  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0719 (1.0674)  time: 0.6393  data: 0.5770  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0460 (1.0463)  time: 0.5119  data: 0.4534  max mem: 13238
Validation: Total time: 0:00:08 (0.6124 s / it)
Averaged stats: loss: 1.0460 (1.0460)  quant_loss: 0.0021 (0.0021)  rec_loss: 0.2795 (0.2794)  rec_angle_loss: 0.6487 (0.6457)  total_loss: 0.9303 (0.9272)
Unused code in codebook: 7831
Validation loss of the network on the 10144 test EEG: 1.0460
Reset the codebook statistic info in quantizer before each epoch
Epoch: [20]  [ 0/19]  eta: 0:03:40  lr: 0.000194  min_lr: 0.000194  loss: 1.0309 (1.0309)  quant_loss: 0.0025 (0.0025)  rec_loss: 0.3348 (0.3348)  rec_angle_loss: 0.6936 (0.6936)  total_loss: 1.0309 (1.0309)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6195 (0.6195)  time: 11.5849  data: 10.6922  max mem: 13238
Epoch: [20]  [10/19]  eta: 0:00:15  lr: 0.000194  min_lr: 0.000194  loss: 1.0301 (1.0282)  quant_loss: 0.0025 (0.0025)  rec_loss: 0.3342 (0.3326)  rec_angle_loss: 0.6936 (0.6932)  total_loss: 1.0301 (1.0282)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5631 (0.6157)  time: 1.7088  data: 1.2593  max mem: 13238
Epoch: [20]  [18/19]  eta: 0:00:01  lr: 0.000193  min_lr: 0.000193  loss: 1.0266 (1.0257)  quant_loss: 0.0023 (0.0024)  rec_loss: 0.3312 (0.3311)  rec_angle_loss: 0.6916 (0.6922)  total_loss: 1.0266 (1.0257)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5045 (0.5567)  time: 1.1409  data: 0.7291  max mem: 13238
Epoch: [20] Total time: 0:00:23 (1.2159 s / it)
Averaged stats: lr: 0.000193  min_lr: 0.000193  loss: 1.0266 (1.0241)  quant_loss: 0.0023 (0.0024)  rec_loss: 0.3312 (0.3305)  rec_angle_loss: 0.6916 (0.6912)  total_loss: 1.0266 (1.0241)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5045 (0.5567)
Unused code in codebook: 7830
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 1.0243 (1.0243)  time: 5.5165  data: 5.0322  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0593 (1.0566)  time: 0.5842  data: 0.4724  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0352 (1.0349)  time: 0.4687  data: 0.3712  max mem: 13238
Validation: Total time: 0:00:07 (0.5600 s / it)
Averaged stats: loss: 1.0352 (1.0347)  quant_loss: 0.0018 (0.0018)  rec_loss: 0.2705 (0.2708)  rec_angle_loss: 0.6410 (0.6408)  total_loss: 0.9133 (0.9135)
Unused code in codebook: 7831
Validation loss of the network on the 10144 test EEG: 1.0347
Reset the codebook statistic info in quantizer before each epoch
Epoch: [21]  [ 0/19]  eta: 0:03:48  lr: 0.000193  min_lr: 0.000193  loss: 1.0135 (1.0135)  quant_loss: 0.0020 (0.0020)  rec_loss: 0.3227 (0.3227)  rec_angle_loss: 0.6888 (0.6888)  total_loss: 1.0135 (1.0135)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3455 (0.3455)  time: 12.0416  data: 11.1378  max mem: 13238
Epoch: [21]  [10/19]  eta: 0:00:15  lr: 0.000192  min_lr: 0.000192  loss: 1.0178 (1.0183)  quant_loss: 0.0020 (0.0020)  rec_loss: 0.3266 (0.3271)  rec_angle_loss: 0.6888 (0.6892)  total_loss: 1.0178 (1.0183)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3836 (0.3812)  time: 1.7461  data: 1.3026  max mem: 13238
Epoch: [21]  [18/19]  eta: 0:00:01  lr: 0.000192  min_lr: 0.000192  loss: 1.0135 (1.0138)  quant_loss: 0.0019 (0.0019)  rec_loss: 0.3257 (0.3263)  rec_angle_loss: 0.6850 (0.6856)  total_loss: 1.0135 (1.0138)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3836 (0.3828)  time: 1.1629  data: 0.7543  max mem: 13238
Epoch: [21] Total time: 0:00:23 (1.2386 s / it)
Averaged stats: lr: 0.000192  min_lr: 0.000192  loss: 1.0135 (1.0122)  quant_loss: 0.0019 (0.0020)  rec_loss: 0.3257 (0.3259)  rec_angle_loss: 0.6850 (0.6843)  total_loss: 1.0135 (1.0122)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3836 (0.3828)
Unused code in codebook: 7831
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 1.0200 (1.0200)  time: 5.5544  data: 5.0729  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0545 (1.0501)  time: 0.5827  data: 0.4750  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0285 (1.0281)  time: 0.4673  data: 0.3732  max mem: 13238
Validation: Total time: 0:00:07 (0.5630 s / it)
Averaged stats: loss: 1.0285 (1.0279)  quant_loss: 0.0015 (0.0016)  rec_loss: 0.2671 (0.2675)  rec_angle_loss: 0.6366 (0.6358)  total_loss: 0.9053 (0.9049)
Unused code in codebook: 7833
Validation loss of the network on the 10144 test EEG: 1.0279
Reset the codebook statistic info in quantizer before each epoch
Epoch: [22]  [ 0/19]  eta: 0:03:47  lr: 0.000192  min_lr: 0.000192  loss: 1.0194 (1.0194)  quant_loss: 0.0018 (0.0018)  rec_loss: 0.3323 (0.3323)  rec_angle_loss: 0.6854 (0.6854)  total_loss: 1.0194 (1.0194)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3084 (0.3084)  time: 11.9834  data: 11.1197  max mem: 13238
Epoch: [22]  [10/19]  eta: 0:00:15  lr: 0.000191  min_lr: 0.000191  loss: 1.0130 (1.0100)  quant_loss: 0.0017 (0.0017)  rec_loss: 0.3267 (0.3265)  rec_angle_loss: 0.6817 (0.6817)  total_loss: 1.0130 (1.0100)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4568 (0.5513)  time: 1.7487  data: 1.3159  max mem: 13238
Epoch: [22]  [18/19]  eta: 0:00:01  lr: 0.000190  min_lr: 0.000190  loss: 1.0059 (1.0074)  quant_loss: 0.0017 (0.0017)  rec_loss: 0.3247 (0.3252)  rec_angle_loss: 0.6809 (0.6806)  total_loss: 1.0059 (1.0074)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4840 (0.5327)  time: 1.1649  data: 0.7618  max mem: 13238
Epoch: [22] Total time: 0:00:23 (1.2377 s / it)
Averaged stats: lr: 0.000190  min_lr: 0.000190  loss: 1.0059 (1.0046)  quant_loss: 0.0017 (0.0017)  rec_loss: 0.3247 (0.3238)  rec_angle_loss: 0.6809 (0.6791)  total_loss: 1.0059 (1.0046)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4840 (0.5327)
Unused code in codebook: 7833
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 1.0157 (1.0157)  time: 5.4071  data: 5.3333  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0502 (1.0451)  time: 0.5659  data: 0.4990  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0237 (1.0233)  time: 0.4542  data: 0.3921  max mem: 13238
Validation: Total time: 0:00:07 (0.5477 s / it)
Averaged stats: loss: 1.0237 (1.0229)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2648 (0.2651)  rec_angle_loss: 0.6366 (0.6354)  total_loss: 0.9028 (0.9020)
Unused code in codebook: 7833
Validation loss of the network on the 10144 test EEG: 1.0229
Reset the codebook statistic info in quantizer before each epoch
Epoch: [23]  [ 0/19]  eta: 0:03:53  lr: 0.000190  min_lr: 0.000190  loss: 0.9986 (0.9986)  quant_loss: 0.0016 (0.0016)  rec_loss: 0.3198 (0.3198)  rec_angle_loss: 0.6772 (0.6772)  total_loss: 0.9986 (0.9986)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4431 (0.4431)  time: 12.3145  data: 11.0346  max mem: 13238
Epoch: [23]  [10/19]  eta: 0:00:15  lr: 0.000190  min_lr: 0.000190  loss: 0.9955 (0.9937)  quant_loss: 0.0015 (0.0015)  rec_loss: 0.3203 (0.3197)  rec_angle_loss: 0.6737 (0.6725)  total_loss: 0.9955 (0.9937)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3792 (0.3861)  time: 1.7603  data: 1.2814  max mem: 13238
Epoch: [23]  [18/19]  eta: 0:00:01  lr: 0.000189  min_lr: 0.000189  loss: 0.9924 (0.9938)  quant_loss: 0.0015 (0.0015)  rec_loss: 0.3202 (0.3200)  rec_angle_loss: 0.6715 (0.6723)  total_loss: 0.9924 (0.9938)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4143 (0.4304)  time: 1.1709  data: 0.7420  max mem: 13238
Epoch: [23] Total time: 0:00:23 (1.2457 s / it)
Averaged stats: lr: 0.000189  min_lr: 0.000189  loss: 0.9924 (0.9951)  quant_loss: 0.0015 (0.0015)  rec_loss: 0.3202 (0.3206)  rec_angle_loss: 0.6715 (0.6730)  total_loss: 0.9924 (0.9951)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4143 (0.4304)
Unused code in codebook: 7830
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:18  loss: 1.0111 (1.0111)  time: 5.5946  data: 5.2754  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0455 (1.0407)  time: 0.5647  data: 0.4797  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0198 (1.0188)  time: 0.4533  data: 0.3769  max mem: 13238
Validation: Total time: 0:00:07 (0.5435 s / it)
Averaged stats: loss: 1.0198 (1.0182)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.2627 (0.2628)  rec_angle_loss: 0.6319 (0.6308)  total_loss: 0.8959 (0.8949)
Unused code in codebook: 7832
Validation loss of the network on the 10144 test EEG: 1.0182
Reset the codebook statistic info in quantizer before each epoch
Epoch: [24]  [ 0/19]  eta: 0:04:14  lr: 0.000189  min_lr: 0.000189  loss: 0.9929 (0.9929)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.3198 (0.3198)  rec_angle_loss: 0.6717 (0.6717)  total_loss: 0.9929 (0.9929)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6310 (0.6310)  time: 13.3829  data: 12.4248  max mem: 13238
Epoch: [24]  [10/19]  eta: 0:00:16  lr: 0.000188  min_lr: 0.000188  loss: 0.9929 (0.9927)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.3196 (0.3206)  rec_angle_loss: 0.6715 (0.6707)  total_loss: 0.9929 (0.9927)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6310 (0.6332)  time: 1.8543  data: 1.4060  max mem: 13238
Epoch: [24]  [18/19]  eta: 0:00:01  lr: 0.000187  min_lr: 0.000187  loss: 0.9895 (0.9896)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.3181 (0.3188)  rec_angle_loss: 0.6688 (0.6695)  total_loss: 0.9895 (0.9896)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5648 (0.5840)  time: 1.2251  data: 0.8140  max mem: 13238
Epoch: [24] Total time: 0:00:24 (1.2977 s / it)
Averaged stats: lr: 0.000187  min_lr: 0.000187  loss: 0.9895 (0.9893)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.3181 (0.3187)  rec_angle_loss: 0.6688 (0.6691)  total_loss: 0.9895 (0.9893)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5648 (0.5840)
Unused code in codebook: 7831
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 1.0117 (1.0117)  time: 5.3499  data: 5.0233  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0396 (1.0370)  time: 0.5735  data: 0.4834  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0171 (1.0153)  time: 0.4602  data: 0.3798  max mem: 13238
Validation: Total time: 0:00:07 (0.5546 s / it)
Averaged stats: loss: 1.0171 (1.0148)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.2606 (0.2608)  rec_angle_loss: 0.6335 (0.6320)  total_loss: 0.8953 (0.8940)
Unused code in codebook: 7830
Validation loss of the network on the 10144 test EEG: 1.0148
Reset the codebook statistic info in quantizer before each epoch
Epoch: [25]  [ 0/19]  eta: 0:03:45  lr: 0.000187  min_lr: 0.000187  loss: 0.9954 (0.9954)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3194 (0.3194)  rec_angle_loss: 0.6747 (0.6747)  total_loss: 0.9954 (0.9954)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5054 (0.5054)  time: 11.8727  data: 10.9272  max mem: 13238
Epoch: [25]  [10/19]  eta: 0:00:15  lr: 0.000186  min_lr: 0.000186  loss: 0.9868 (0.9853)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3178 (0.3179)  rec_angle_loss: 0.6651 (0.6661)  total_loss: 0.9868 (0.9853)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5054 (0.5092)  time: 1.7286  data: 1.2852  max mem: 13238
Epoch: [25]  [18/19]  eta: 0:00:01  lr: 0.000186  min_lr: 0.000186  loss: 0.9843 (0.9841)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3185 (0.3177)  rec_angle_loss: 0.6641 (0.6651)  total_loss: 0.9843 (0.9841)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5054 (0.4951)  time: 1.1580  data: 0.7442  max mem: 13238
Epoch: [25] Total time: 0:00:23 (1.2330 s / it)
Averaged stats: lr: 0.000186  min_lr: 0.000186  loss: 0.9843 (0.9816)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3185 (0.3164)  rec_angle_loss: 0.6641 (0.6639)  total_loss: 0.9843 (0.9816)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5054 (0.4951)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:19  loss: 1.0063 (1.0063)  time: 5.6482  data: 5.5728  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0368 (1.0318)  time: 0.5771  data: 0.5155  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0109 (1.0095)  time: 0.4629  data: 0.4050  max mem: 13238
Validation: Total time: 0:00:07 (0.5595 s / it)
Averaged stats: loss: 1.0109 (1.0087)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.2581 (0.2583)  rec_angle_loss: 0.6261 (0.6259)  total_loss: 0.8854 (0.8854)
Unused code in codebook: 7825
Validation loss of the network on the 10144 test EEG: 1.0087
Reset the codebook statistic info in quantizer before each epoch
Epoch: [26]  [ 0/19]  eta: 0:03:53  lr: 0.000186  min_lr: 0.000186  loss: 0.9743 (0.9743)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3147 (0.3147)  rec_angle_loss: 0.6583 (0.6583)  total_loss: 0.9743 (0.9743)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5567 (0.5567)  time: 12.2926  data: 10.7829  max mem: 13238
Epoch: [26]  [10/19]  eta: 0:00:15  lr: 0.000185  min_lr: 0.000185  loss: 0.9768 (0.9769)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3155 (0.3159)  rec_angle_loss: 0.6583 (0.6597)  total_loss: 0.9768 (0.9769)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6162 (0.6658)  time: 1.7620  data: 1.2414  max mem: 13238
Epoch: [26]  [18/19]  eta: 0:00:01  lr: 0.000184  min_lr: 0.000184  loss: 0.9767 (0.9762)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3147 (0.3155)  rec_angle_loss: 0.6600 (0.6595)  total_loss: 0.9767 (0.9762)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5825 (0.6155)  time: 1.1717  data: 0.7188  max mem: 13238
Epoch: [26] Total time: 0:00:23 (1.2435 s / it)
Averaged stats: lr: 0.000184  min_lr: 0.000184  loss: 0.9767 (0.9766)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3147 (0.3153)  rec_angle_loss: 0.6600 (0.6600)  total_loss: 0.9767 (0.9766)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5825 (0.6155)
Unused code in codebook: 7813
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:21  loss: 1.0041 (1.0041)  time: 5.7988  data: 5.1310  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0341 (1.0294)  time: 0.5884  data: 0.4665  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0083 (1.0071)  time: 0.4719  data: 0.3666  max mem: 13238
Validation: Total time: 0:00:07 (0.5623 s / it)
Averaged stats: loss: 1.0083 (1.0065)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.2569 (0.2571)  rec_angle_loss: 0.6236 (0.6248)  total_loss: 0.8817 (0.8831)
Unused code in codebook: 7817
Validation loss of the network on the 10144 test EEG: 1.0065
Reset the codebook statistic info in quantizer before each epoch
Epoch: [27]  [ 0/19]  eta: 0:03:54  lr: 0.000184  min_lr: 0.000184  loss: 0.9690 (0.9690)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3141 (0.3141)  rec_angle_loss: 0.6537 (0.6537)  total_loss: 0.9690 (0.9690)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5899 (0.5899)  time: 12.3188  data: 10.6558  max mem: 13238
Epoch: [27]  [10/19]  eta: 0:00:15  lr: 0.000183  min_lr: 0.000183  loss: 0.9701 (0.9715)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3126 (0.3134)  rec_angle_loss: 0.6585 (0.6569)  total_loss: 0.9701 (0.9715)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5899 (0.5544)  time: 1.7276  data: 1.2188  max mem: 13238
Epoch: [27]  [18/19]  eta: 0:00:01  lr: 0.000182  min_lr: 0.000182  loss: 0.9721 (0.9723)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3131 (0.3139)  rec_angle_loss: 0.6581 (0.6572)  total_loss: 0.9721 (0.9723)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5162 (0.5235)  time: 1.1522  data: 0.7058  max mem: 13238
Epoch: [27] Total time: 0:00:23 (1.2256 s / it)
Averaged stats: lr: 0.000182  min_lr: 0.000182  loss: 0.9721 (0.9707)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3131 (0.3134)  rec_angle_loss: 0.6581 (0.6561)  total_loss: 0.9721 (0.9707)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5162 (0.5235)
Unused code in codebook: 7807
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 1.0031 (1.0031)  time: 5.3380  data: 5.2639  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0300 (1.0260)  time: 0.5599  data: 0.4949  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0056 (1.0038)  time: 0.4494  data: 0.3888  max mem: 13238
Validation: Total time: 0:00:07 (0.5425 s / it)
Averaged stats: loss: 1.0056 (1.0030)  quant_loss: 0.0012 (0.0013)  rec_loss: 0.2555 (0.2554)  rec_angle_loss: 0.6232 (0.6226)  total_loss: 0.8800 (0.8793)
Unused code in codebook: 7805
Validation loss of the network on the 10144 test EEG: 1.0030
Reset the codebook statistic info in quantizer before each epoch
Epoch: [28]  [ 0/19]  eta: 0:03:52  lr: 0.000182  min_lr: 0.000182  loss: 0.9704 (0.9704)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3127 (0.3127)  rec_angle_loss: 0.6564 (0.6564)  total_loss: 0.9704 (0.9704)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6436 (0.6436)  time: 12.2413  data: 11.0650  max mem: 13238
Epoch: [28]  [10/19]  eta: 0:00:16  lr: 0.000181  min_lr: 0.000181  loss: 0.9650 (0.9656)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3116 (0.3118)  rec_angle_loss: 0.6517 (0.6526)  total_loss: 0.9650 (0.9656)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5955 (0.5705)  time: 1.8376  data: 1.3748  max mem: 13238
Epoch: [28]  [18/19]  eta: 0:00:01  lr: 0.000180  min_lr: 0.000180  loss: 0.9640 (0.9651)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3119 (0.3119)  rec_angle_loss: 0.6510 (0.6520)  total_loss: 0.9640 (0.9651)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5933 (0.5569)  time: 1.2155  data: 0.7960  max mem: 13238
Epoch: [28] Total time: 0:00:24 (1.2926 s / it)
Averaged stats: lr: 0.000180  min_lr: 0.000180  loss: 0.9640 (0.9658)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3119 (0.3125)  rec_angle_loss: 0.6510 (0.6521)  total_loss: 0.9640 (0.9658)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5933 (0.5569)
Unused code in codebook: 7809
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.9984 (0.9984)  time: 5.3945  data: 4.8997  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0252 (1.0215)  time: 0.5780  data: 0.4734  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0010 (0.9991)  time: 0.4638  data: 0.3720  max mem: 13238
Validation: Total time: 0:00:07 (0.5555 s / it)
Averaged stats: loss: 1.0010 (0.9984)  quant_loss: 0.0011 (0.0011)  rec_loss: 0.2543 (0.2545)  rec_angle_loss: 0.6179 (0.6206)  total_loss: 0.8733 (0.8762)
Unused code in codebook: 7814
Validation loss of the network on the 10144 test EEG: 0.9984
Reset the codebook statistic info in quantizer before each epoch
Epoch: [29]  [ 0/19]  eta: 0:03:55  lr: 0.000180  min_lr: 0.000180  loss: 0.9658 (0.9658)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3141 (0.3141)  rec_angle_loss: 0.6505 (0.6505)  total_loss: 0.9658 (0.9658)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3433 (0.3433)  time: 12.4157  data: 10.9749  max mem: 13238
Epoch: [29]  [10/19]  eta: 0:00:15  lr: 0.000179  min_lr: 0.000179  loss: 0.9629 (0.9636)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3136 (0.3122)  rec_angle_loss: 0.6505 (0.6502)  total_loss: 0.9629 (0.9636)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5443 (0.5481)  time: 1.7771  data: 1.2677  max mem: 13238
Epoch: [29]  [18/19]  eta: 0:00:01  lr: 0.000178  min_lr: 0.000178  loss: 0.9615 (0.9626)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3122 (0.3119)  rec_angle_loss: 0.6496 (0.6495)  total_loss: 0.9615 (0.9626)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5443 (0.5519)  time: 1.1803  data: 0.7340  max mem: 13238
Epoch: [29] Total time: 0:00:23 (1.2504 s / it)
Averaged stats: lr: 0.000178  min_lr: 0.000178  loss: 0.9615 (0.9614)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3122 (0.3113)  rec_angle_loss: 0.6496 (0.6489)  total_loss: 0.9615 (0.9614)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5443 (0.5519)
Unused code in codebook: 7811
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:19  loss: 1.0013 (1.0013)  time: 5.6866  data: 5.6110  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0264 (1.0213)  time: 0.5724  data: 0.5101  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 1.0013 (0.9993)  time: 0.4594  data: 0.4008  max mem: 13238
Validation: Total time: 0:00:07 (0.5477 s / it)
Averaged stats: loss: 1.0013 (0.9988)  quant_loss: 0.0011 (0.0012)  rec_loss: 0.2555 (0.2552)  rec_angle_loss: 0.6207 (0.6220)  total_loss: 0.8773 (0.8784)
Unused code in codebook: 7815
Validation loss of the network on the 10144 test EEG: 0.9988
Reset the codebook statistic info in quantizer before each epoch
Epoch: [30]  [ 0/19]  eta: 0:03:49  lr: 0.000178  min_lr: 0.000178  loss: 0.9558 (0.9558)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3106 (0.3106)  rec_angle_loss: 0.6439 (0.6439)  total_loss: 0.9558 (0.9558)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7111 (0.7111)  time: 12.0738  data: 10.9786  max mem: 13238
Epoch: [30]  [10/19]  eta: 0:00:15  lr: 0.000177  min_lr: 0.000177  loss: 0.9614 (0.9608)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.3106 (0.3110)  rec_angle_loss: 0.6500 (0.6485)  total_loss: 0.9614 (0.9608)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6672 (0.6261)  time: 1.7645  data: 1.3068  max mem: 13238
Epoch: [30]  [18/19]  eta: 0:00:01  lr: 0.000176  min_lr: 0.000176  loss: 0.9571 (0.9578)  quant_loss: 0.0013 (0.0012)  rec_loss: 0.3099 (0.3102)  rec_angle_loss: 0.6470 (0.6463)  total_loss: 0.9571 (0.9578)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6494 (0.6244)  time: 1.1729  data: 0.7567  max mem: 13238
Epoch: [30] Total time: 0:00:23 (1.2473 s / it)
Averaged stats: lr: 0.000176  min_lr: 0.000176  loss: 0.9571 (0.9575)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3099 (0.3105)  rec_angle_loss: 0.6470 (0.6457)  total_loss: 0.9571 (0.9575)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6494 (0.6244)
Unused code in codebook: 7816
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.9975 (0.9975)  time: 5.3642  data: 5.2905  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0226 (1.0180)  time: 0.5607  data: 0.4995  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9988 (0.9959)  time: 0.4500  data: 0.3925  max mem: 13238
Validation: Total time: 0:00:07 (0.5384 s / it)
Averaged stats: loss: 0.9988 (0.9950)  quant_loss: 0.0012 (0.0013)  rec_loss: 0.2535 (0.2533)  rec_angle_loss: 0.6188 (0.6211)  total_loss: 0.8736 (0.8757)
Unused code in codebook: 7819
Validation loss of the network on the 10144 test EEG: 0.9950
Reset the codebook statistic info in quantizer before each epoch
Epoch: [31]  [ 0/19]  eta: 0:03:42  lr: 0.000176  min_lr: 0.000176  loss: 0.9599 (0.9599)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3107 (0.3107)  rec_angle_loss: 0.6480 (0.6480)  total_loss: 0.9599 (0.9599)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5232 (0.5232)  time: 11.7250  data: 10.9857  max mem: 13238
Epoch: [31]  [10/19]  eta: 0:00:15  lr: 0.000174  min_lr: 0.000174  loss: 0.9533 (0.9541)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3087 (0.3082)  rec_angle_loss: 0.6458 (0.6445)  total_loss: 0.9533 (0.9541)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5710 (0.6299)  time: 1.7633  data: 1.3094  max mem: 13238
Epoch: [31]  [18/19]  eta: 0:00:01  lr: 0.000173  min_lr: 0.000173  loss: 0.9540 (0.9536)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3091 (0.3088)  rec_angle_loss: 0.6423 (0.6435)  total_loss: 0.9540 (0.9536)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6552 (0.7146)  time: 1.1721  data: 0.7582  max mem: 13238
Epoch: [31] Total time: 0:00:23 (1.2471 s / it)
Averaged stats: lr: 0.000173  min_lr: 0.000173  loss: 0.9540 (0.9534)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3091 (0.3090)  rec_angle_loss: 0.6423 (0.6431)  total_loss: 0.9540 (0.9534)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6552 (0.7146)
Unused code in codebook: 7822
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9971 (0.9971)  time: 5.4807  data: 4.9758  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0227 (1.0172)  time: 0.5769  data: 0.4623  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9977 (0.9949)  time: 0.4628  data: 0.3633  max mem: 13238
Validation: Total time: 0:00:07 (0.5538 s / it)
Averaged stats: loss: 0.9977 (0.9940)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2519 (0.2517)  rec_angle_loss: 0.6188 (0.6191)  total_loss: 0.8720 (0.8722)
Unused code in codebook: 7822
Validation loss of the network on the 10144 test EEG: 0.9940
Reset the codebook statistic info in quantizer before each epoch
Epoch: [32]  [ 0/19]  eta: 0:03:58  lr: 0.000173  min_lr: 0.000173  loss: 0.9449 (0.9449)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3059 (0.3059)  rec_angle_loss: 0.6377 (0.6377)  total_loss: 0.9449 (0.9449)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6531 (0.6531)  time: 12.5369  data: 10.7893  max mem: 13238
Epoch: [32]  [10/19]  eta: 0:00:15  lr: 0.000172  min_lr: 0.000172  loss: 0.9500 (0.9485)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.3077 (0.3082)  rec_angle_loss: 0.6394 (0.6389)  total_loss: 0.9500 (0.9485)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5303 (0.5682)  time: 1.7590  data: 1.2033  max mem: 13238
Epoch: [32]  [18/19]  eta: 0:00:01  lr: 0.000171  min_lr: 0.000171  loss: 0.9500 (0.9490)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.3078 (0.3085)  rec_angle_loss: 0.6394 (0.6392)  total_loss: 0.9500 (0.9490)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5303 (0.5508)  time: 1.1697  data: 0.6967  max mem: 13238
Epoch: [32] Total time: 0:00:23 (1.2447 s / it)
Averaged stats: lr: 0.000171  min_lr: 0.000171  loss: 0.9500 (0.9489)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.3078 (0.3083)  rec_angle_loss: 0.6394 (0.6392)  total_loss: 0.9500 (0.9489)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5303 (0.5508)
Unused code in codebook: 7823
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:27  loss: 0.9912 (0.9912)  time: 6.2825  data: 6.2121  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0175 (1.0128)  time: 0.6399  data: 0.5766  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9934 (0.9908)  time: 0.5123  data: 0.4531  max mem: 13238
Validation: Total time: 0:00:08 (0.6089 s / it)
Averaged stats: loss: 0.9934 (0.9902)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2526 (0.2522)  rec_angle_loss: 0.6163 (0.6167)  total_loss: 0.8703 (0.8703)
Unused code in codebook: 7824
Validation loss of the network on the 10144 test EEG: 0.9902
Reset the codebook statistic info in quantizer before each epoch
Epoch: [33]  [ 0/19]  eta: 0:03:53  lr: 0.000171  min_lr: 0.000171  loss: 0.9499 (0.9499)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3093 (0.3093)  rec_angle_loss: 0.6392 (0.6392)  total_loss: 0.9499 (0.9499)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5227 (0.5227)  time: 12.2958  data: 11.0959  max mem: 13238
Epoch: [33]  [10/19]  eta: 0:00:16  lr: 0.000170  min_lr: 0.000170  loss: 0.9449 (0.9428)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.3066 (0.3063)  rec_angle_loss: 0.6357 (0.6351)  total_loss: 0.9449 (0.9428)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5674 (0.5821)  time: 1.7974  data: 1.3587  max mem: 13238
Epoch: [33]  [18/19]  eta: 0:00:01  lr: 0.000169  min_lr: 0.000169  loss: 0.9429 (0.9426)  quant_loss: 0.0013 (0.0014)  rec_loss: 0.3065 (0.3063)  rec_angle_loss: 0.6350 (0.6349)  total_loss: 0.9429 (0.9426)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5938 (0.5785)  time: 1.1925  data: 0.7868  max mem: 13238
Epoch: [33] Total time: 0:00:24 (1.2671 s / it)
Averaged stats: lr: 0.000169  min_lr: 0.000169  loss: 0.9429 (0.9445)  quant_loss: 0.0013 (0.0014)  rec_loss: 0.3065 (0.3073)  rec_angle_loss: 0.6350 (0.6358)  total_loss: 0.9429 (0.9445)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5938 (0.5785)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:19  loss: 0.9876 (0.9876)  time: 5.6749  data: 4.9743  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0109 (1.0075)  time: 0.5915  data: 0.4604  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9880 (0.9858)  time: 0.4748  data: 0.3618  max mem: 13238
Validation: Total time: 0:00:07 (0.5696 s / it)
Averaged stats: loss: 0.9880 (0.9848)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2503 (0.2501)  rec_angle_loss: 0.6148 (0.6142)  total_loss: 0.8664 (0.8657)
Unused code in codebook: 7824
Validation loss of the network on the 10144 test EEG: 0.9848
Reset the codebook statistic info in quantizer before each epoch
Epoch: [34]  [ 0/19]  eta: 0:03:42  lr: 0.000169  min_lr: 0.000169  loss: 0.9396 (0.9396)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3054 (0.3054)  rec_angle_loss: 0.6329 (0.6329)  total_loss: 0.9396 (0.9396)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4446 (0.4446)  time: 11.7147  data: 11.0150  max mem: 13238
Epoch: [34]  [10/19]  eta: 0:00:16  lr: 0.000167  min_lr: 0.000167  loss: 0.9417 (0.9431)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3070 (0.3077)  rec_angle_loss: 0.6332 (0.6341)  total_loss: 0.9417 (0.9431)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5260 (0.5398)  time: 1.7824  data: 1.3370  max mem: 13238
Epoch: [34]  [18/19]  eta: 0:00:01  lr: 0.000166  min_lr: 0.000166  loss: 0.9398 (0.9411)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3054 (0.3068)  rec_angle_loss: 0.6327 (0.6330)  total_loss: 0.9398 (0.9411)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5260 (0.5343)  time: 1.1830  data: 0.7743  max mem: 13238
Epoch: [34] Total time: 0:00:23 (1.2602 s / it)
Averaged stats: lr: 0.000166  min_lr: 0.000166  loss: 0.9398 (0.9404)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3054 (0.3059)  rec_angle_loss: 0.6327 (0.6332)  total_loss: 0.9398 (0.9404)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5260 (0.5343)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.9853 (0.9853)  time: 5.5645  data: 5.4883  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0093 (1.0059)  time: 0.5774  data: 0.5159  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9867 (0.9838)  time: 0.4632  data: 0.4054  max mem: 13238
Validation: Total time: 0:00:07 (0.5562 s / it)
Averaged stats: loss: 0.9867 (0.9830)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2491 (0.2490)  rec_angle_loss: 0.6113 (0.6136)  total_loss: 0.8618 (0.8640)
Unused code in codebook: 7823
Validation loss of the network on the 10144 test EEG: 0.9830
Reset the codebook statistic info in quantizer before each epoch
Epoch: [35]  [ 0/19]  eta: 0:03:44  lr: 0.000166  min_lr: 0.000166  loss: 0.9359 (0.9359)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3037 (0.3037)  rec_angle_loss: 0.6309 (0.6309)  total_loss: 0.9359 (0.9359)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6137 (0.6137)  time: 11.7977  data: 10.8482  max mem: 13238
Epoch: [35]  [10/19]  eta: 0:00:15  lr: 0.000165  min_lr: 0.000165  loss: 0.9361 (0.9378)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3037 (0.3044)  rec_angle_loss: 0.6313 (0.6320)  total_loss: 0.9361 (0.9378)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6137 (0.5990)  time: 1.7469  data: 1.2915  max mem: 13238
Epoch: [35]  [18/19]  eta: 0:00:01  lr: 0.000164  min_lr: 0.000164  loss: 0.9360 (0.9362)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3035 (0.3034)  rec_angle_loss: 0.6313 (0.6315)  total_loss: 0.9360 (0.9362)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5773 (0.5932)  time: 1.1628  data: 0.7477  max mem: 13238
Epoch: [35] Total time: 0:00:23 (1.2375 s / it)
Averaged stats: lr: 0.000164  min_lr: 0.000164  loss: 0.9360 (0.9368)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3035 (0.3047)  rec_angle_loss: 0.6313 (0.6309)  total_loss: 0.9360 (0.9368)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5773 (0.5932)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.9869 (0.9869)  time: 5.5452  data: 5.4686  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0057 (1.0050)  time: 0.5726  data: 0.5088  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9869 (0.9829)  time: 0.4593  data: 0.3998  max mem: 13238
Validation: Total time: 0:00:07 (0.5536 s / it)
Averaged stats: loss: 0.9869 (0.9818)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2471 (0.2470)  rec_angle_loss: 0.6116 (0.6135)  total_loss: 0.8601 (0.8619)
Unused code in codebook: 7824
Validation loss of the network on the 10144 test EEG: 0.9818
Reset the codebook statistic info in quantizer before each epoch
Epoch: [36]  [ 0/19]  eta: 0:03:37  lr: 0.000163  min_lr: 0.000163  loss: 0.9299 (0.9299)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.2975 (0.2975)  rec_angle_loss: 0.6311 (0.6311)  total_loss: 0.9299 (0.9299)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5157 (0.5157)  time: 11.4533  data: 10.9640  max mem: 13238
Epoch: [36]  [10/19]  eta: 0:00:15  lr: 0.000162  min_lr: 0.000162  loss: 0.9358 (0.9351)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3043 (0.3036)  rec_angle_loss: 0.6308 (0.6302)  total_loss: 0.9358 (0.9351)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7055 (0.6545)  time: 1.7241  data: 1.3516  max mem: 13238
Epoch: [36]  [18/19]  eta: 0:00:01  lr: 0.000161  min_lr: 0.000161  loss: 0.9357 (0.9345)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3043 (0.3038)  rec_angle_loss: 0.6302 (0.6294)  total_loss: 0.9357 (0.9345)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6125 (0.6200)  time: 1.1496  data: 0.7825  max mem: 13238
Epoch: [36] Total time: 0:00:23 (1.2225 s / it)
Averaged stats: lr: 0.000161  min_lr: 0.000161  loss: 0.9357 (0.9336)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3043 (0.3032)  rec_angle_loss: 0.6302 (0.6291)  total_loss: 0.9357 (0.9336)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6125 (0.6200)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:18  loss: 0.9820 (0.9820)  time: 5.6102  data: 4.9675  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 1.0016 (1.0012)  time: 0.5893  data: 0.4603  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9820 (0.9793)  time: 0.4726  data: 0.3617  max mem: 13238
Validation: Total time: 0:00:07 (0.5647 s / it)
Averaged stats: loss: 0.9820 (0.9782)  quant_loss: 0.0013 (0.0014)  rec_loss: 0.2464 (0.2462)  rec_angle_loss: 0.6143 (0.6129)  total_loss: 0.8620 (0.8604)
Unused code in codebook: 7824
Validation loss of the network on the 10144 test EEG: 0.9782
Reset the codebook statistic info in quantizer before each epoch
Epoch: [37]  [ 0/19]  eta: 0:04:15  lr: 0.000161  min_lr: 0.000161  loss: 0.9257 (0.9257)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.2990 (0.2990)  rec_angle_loss: 0.6254 (0.6254)  total_loss: 0.9257 (0.9257)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5640 (0.5640)  time: 13.4330  data: 12.1539  max mem: 13238
Epoch: [37]  [10/19]  eta: 0:00:17  lr: 0.000159  min_lr: 0.000159  loss: 0.9304 (0.9298)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3006 (0.3013)  rec_angle_loss: 0.6272 (0.6272)  total_loss: 0.9304 (0.9298)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6998 (0.7403)  time: 1.9201  data: 1.3741  max mem: 13238
Epoch: [37]  [18/19]  eta: 0:00:01  lr: 0.000158  min_lr: 0.000158  loss: 0.9313 (0.9308)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3010 (0.3018)  rec_angle_loss: 0.6273 (0.6277)  total_loss: 0.9313 (0.9308)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6170 (0.6524)  time: 1.2632  data: 0.7956  max mem: 13238
Epoch: [37] Total time: 0:00:25 (1.3382 s / it)
Averaged stats: lr: 0.000158  min_lr: 0.000158  loss: 0.9313 (0.9306)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3010 (0.3016)  rec_angle_loss: 0.6273 (0.6278)  total_loss: 0.9313 (0.9306)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6170 (0.6524)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.9801 (0.9801)  time: 5.4098  data: 5.3252  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9983 (0.9990)  time: 0.5870  data: 0.5142  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9828 (0.9771)  time: 0.4713  data: 0.4040  max mem: 13238
Validation: Total time: 0:00:07 (0.5638 s / it)
Averaged stats: loss: 0.9828 (0.9764)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2451 (0.2448)  rec_angle_loss: 0.6081 (0.6115)  total_loss: 0.8546 (0.8577)
Unused code in codebook: 7824
Validation loss of the network on the 10144 test EEG: 0.9764
Reset the codebook statistic info in quantizer before each epoch
Epoch: [38]  [ 0/19]  eta: 0:03:45  lr: 0.000158  min_lr: 0.000158  loss: 0.9331 (0.9331)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3030 (0.3030)  rec_angle_loss: 0.6289 (0.6289)  total_loss: 0.9331 (0.9331)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6327 (0.6327)  time: 11.8930  data: 11.1463  max mem: 13238
Epoch: [38]  [10/19]  eta: 0:00:15  lr: 0.000157  min_lr: 0.000157  loss: 0.9249 (0.9265)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3005 (0.3002)  rec_angle_loss: 0.6230 (0.6249)  total_loss: 0.9249 (0.9265)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6327 (0.6204)  time: 1.7416  data: 1.3401  max mem: 13238
Epoch: [38]  [18/19]  eta: 0:00:01  lr: 0.000155  min_lr: 0.000155  loss: 0.9261 (0.9264)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3005 (0.2999)  rec_angle_loss: 0.6244 (0.6252)  total_loss: 0.9261 (0.9264)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5627 (0.5637)  time: 1.1600  data: 0.7759  max mem: 13238
Epoch: [38] Total time: 0:00:23 (1.2357 s / it)
Averaged stats: lr: 0.000155  min_lr: 0.000155  loss: 0.9261 (0.9266)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.3005 (0.2999)  rec_angle_loss: 0.6244 (0.6254)  total_loss: 0.9261 (0.9266)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5627 (0.5637)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9762 (0.9762)  time: 5.4606  data: 5.3858  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9937 (0.9948)  time: 0.5702  data: 0.5068  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9763 (0.9731)  time: 0.4575  data: 0.3982  max mem: 13238
Validation: Total time: 0:00:07 (0.5570 s / it)
Averaged stats: loss: 0.9763 (0.9724)  quant_loss: 0.0013 (0.0014)  rec_loss: 0.2415 (0.2413)  rec_angle_loss: 0.6113 (0.6106)  total_loss: 0.8542 (0.8532)
Unused code in codebook: 7824
Validation loss of the network on the 10144 test EEG: 0.9724
Reset the codebook statistic info in quantizer before each epoch
Epoch: [39]  [ 0/19]  eta: 0:03:55  lr: 0.000155  min_lr: 0.000155  loss: 0.9255 (0.9255)  quant_loss: 0.0012 (0.0012)  rec_loss: 0.2991 (0.2991)  rec_angle_loss: 0.6252 (0.6252)  total_loss: 0.9255 (0.9255)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4166 (0.4166)  time: 12.3959  data: 11.5205  max mem: 13238
Epoch: [39]  [10/19]  eta: 0:00:16  lr: 0.000154  min_lr: 0.000154  loss: 0.9215 (0.9223)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.2977 (0.2970)  rec_angle_loss: 0.6238 (0.6239)  total_loss: 0.9215 (0.9223)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4408 (0.4353)  time: 1.8071  data: 1.3417  max mem: 13238
Epoch: [39]  [18/19]  eta: 0:00:01  lr: 0.000153  min_lr: 0.000153  loss: 0.9215 (0.9222)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.2977 (0.2974)  rec_angle_loss: 0.6238 (0.6234)  total_loss: 0.9215 (0.9222)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4894 (0.5013)  time: 1.1982  data: 0.7768  max mem: 13238
Epoch: [39] Total time: 0:00:24 (1.2679 s / it)
Averaged stats: lr: 0.000153  min_lr: 0.000153  loss: 0.9215 (0.9229)  quant_loss: 0.0013 (0.0013)  rec_loss: 0.2977 (0.2982)  rec_angle_loss: 0.6238 (0.6234)  total_loss: 0.9215 (0.9229)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4894 (0.5013)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.9730 (0.9730)  time: 5.3789  data: 5.3056  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9898 (0.9926)  time: 0.5640  data: 0.5007  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9748 (0.9708)  time: 0.4526  data: 0.3935  max mem: 13238
Validation: Total time: 0:00:07 (0.5473 s / it)
Averaged stats: loss: 0.9748 (0.9701)  quant_loss: 0.0015 (0.0015)  rec_loss: 0.2396 (0.2392)  rec_angle_loss: 0.6099 (0.6090)  total_loss: 0.8511 (0.8498)
Unused code in codebook: 7824
Validation loss of the network on the 10144 test EEG: 0.9701
Reset the codebook statistic info in quantizer before each epoch
Epoch: [40]  [ 0/19]  eta: 0:03:53  lr: 0.000152  min_lr: 0.000152  loss: 0.9133 (0.9133)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2944 (0.2944)  rec_angle_loss: 0.6176 (0.6176)  total_loss: 0.9133 (0.9133)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5407 (0.5407)  time: 12.2702  data: 10.9878  max mem: 13238
Epoch: [40]  [10/19]  eta: 0:00:16  lr: 0.000151  min_lr: 0.000151  loss: 0.9210 (0.9202)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2981 (0.2979)  rec_angle_loss: 0.6206 (0.6210)  total_loss: 0.9210 (0.9202)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5944 (0.6402)  time: 1.7928  data: 1.2727  max mem: 13238
Epoch: [40]  [18/19]  eta: 0:00:01  lr: 0.000150  min_lr: 0.000150  loss: 0.9197 (0.9201)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2964 (0.2971)  rec_angle_loss: 0.6214 (0.6216)  total_loss: 0.9197 (0.9201)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5926 (0.6196)  time: 1.1904  data: 0.7369  max mem: 13238
Epoch: [40] Total time: 0:00:24 (1.2657 s / it)
Averaged stats: lr: 0.000150  min_lr: 0.000150  loss: 0.9197 (0.9202)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2964 (0.2969)  rec_angle_loss: 0.6214 (0.6220)  total_loss: 0.9197 (0.9202)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5926 (0.6196)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:21  loss: 0.9687 (0.9687)  time: 5.8315  data: 4.9677  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9866 (0.9901)  time: 0.5879  data: 0.4517  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9744 (0.9680)  time: 0.4716  data: 0.3549  max mem: 13238
Validation: Total time: 0:00:07 (0.5656 s / it)
Averaged stats: loss: 0.9744 (0.9673)  quant_loss: 0.0015 (0.0015)  rec_loss: 0.2378 (0.2374)  rec_angle_loss: 0.6085 (0.6102)  total_loss: 0.8478 (0.8491)
Unused code in codebook: 7823
Validation loss of the network on the 10144 test EEG: 0.9673
Reset the codebook statistic info in quantizer before each epoch
Epoch: [41]  [ 0/19]  eta: 0:03:49  lr: 0.000150  min_lr: 0.000150  loss: 0.9193 (0.9193)  quant_loss: 0.0014 (0.0014)  rec_loss: 0.2980 (0.2980)  rec_angle_loss: 0.6198 (0.6198)  total_loss: 0.9193 (0.9193)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6319 (0.6319)  time: 12.0721  data: 11.2507  max mem: 13238
Epoch: [41]  [10/19]  eta: 0:00:17  lr: 0.000148  min_lr: 0.000148  loss: 0.9178 (0.9177)  quant_loss: 0.0016 (0.0015)  rec_loss: 0.2956 (0.2951)  rec_angle_loss: 0.6207 (0.6211)  total_loss: 0.9178 (0.9177)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5996 (0.6254)  time: 1.8962  data: 1.4938  max mem: 13238
Epoch: [41]  [18/19]  eta: 0:00:01  lr: 0.000147  min_lr: 0.000147  loss: 0.9157 (0.9148)  quant_loss: 0.0016 (0.0016)  rec_loss: 0.2941 (0.2932)  rec_angle_loss: 0.6205 (0.6199)  total_loss: 0.9157 (0.9148)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5979 (0.6148)  time: 1.2498  data: 0.8649  max mem: 13238
Epoch: [41] Total time: 0:00:25 (1.3271 s / it)
Averaged stats: lr: 0.000147  min_lr: 0.000147  loss: 0.9157 (0.9172)  quant_loss: 0.0016 (0.0016)  rec_loss: 0.2941 (0.2948)  rec_angle_loss: 0.6205 (0.6208)  total_loss: 0.9157 (0.9172)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5979 (0.6148)
Unused code in codebook: 7824
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.9680 (0.9680)  time: 5.4109  data: 5.3348  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9846 (0.9890)  time: 0.5619  data: 0.4987  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9734 (0.9674)  time: 0.4512  data: 0.3919  max mem: 13238
Validation: Total time: 0:00:07 (0.5460 s / it)
Averaged stats: loss: 0.9734 (0.9665)  quant_loss: 0.0020 (0.0020)  rec_loss: 0.2353 (0.2352)  rec_angle_loss: 0.6131 (0.6110)  total_loss: 0.8505 (0.8482)
Unused code in codebook: 7818
Validation loss of the network on the 10144 test EEG: 0.9665
Reset the codebook statistic info in quantizer before each epoch
Epoch: [42]  [ 0/19]  eta: 0:03:49  lr: 0.000147  min_lr: 0.000147  loss: 0.9245 (0.9245)  quant_loss: 0.0018 (0.0018)  rec_loss: 0.2977 (0.2977)  rec_angle_loss: 0.6251 (0.6251)  total_loss: 0.9245 (0.9245)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4003 (0.4003)  time: 12.0560  data: 11.5922  max mem: 13238
Epoch: [42]  [10/19]  eta: 0:00:15  lr: 0.000145  min_lr: 0.000145  loss: 0.9147 (0.9153)  quant_loss: 0.0020 (0.0020)  rec_loss: 0.2924 (0.2927)  rec_angle_loss: 0.6209 (0.6206)  total_loss: 0.9147 (0.9153)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4406 (0.6975)  time: 1.7596  data: 1.3627  max mem: 13238
Epoch: [42]  [18/19]  eta: 0:00:01  lr: 0.000144  min_lr: 0.000144  loss: 0.9153 (0.9161)  quant_loss: 0.0021 (0.0021)  rec_loss: 0.2922 (0.2932)  rec_angle_loss: 0.6210 (0.6208)  total_loss: 0.9153 (0.9161)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7518 (0.8294)  time: 1.1716  data: 0.7889  max mem: 13238
Epoch: [42] Total time: 0:00:23 (1.2463 s / it)
Averaged stats: lr: 0.000144  min_lr: 0.000144  loss: 0.9153 (0.9179)  quant_loss: 0.0021 (0.0021)  rec_loss: 0.2922 (0.2942)  rec_angle_loss: 0.6210 (0.6216)  total_loss: 0.9153 (0.9179)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7518 (0.8294)
Unused code in codebook: 7795
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9636 (0.9636)  time: 5.4514  data: 5.3761  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9856 (0.9880)  time: 0.5676  data: 0.5042  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9747 (0.9665)  time: 0.4555  data: 0.3962  max mem: 13238
Validation: Total time: 0:00:07 (0.5538 s / it)
Averaged stats: loss: 0.9747 (0.9653)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2319 (0.2319)  rec_angle_loss: 0.6149 (0.6130)  total_loss: 0.8496 (0.8477)
Unused code in codebook: 7791
Validation loss of the network on the 10144 test EEG: 0.9653
Reset the codebook statistic info in quantizer before each epoch
Epoch: [43]  [ 0/19]  eta: 0:03:43  lr: 0.000144  min_lr: 0.000144  loss: 0.9204 (0.9204)  quant_loss: 0.0025 (0.0025)  rec_loss: 0.2930 (0.2930)  rec_angle_loss: 0.6249 (0.6249)  total_loss: 0.9204 (0.9204)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6051 (0.6051)  time: 11.7385  data: 10.9579  max mem: 13238
Epoch: [43]  [10/19]  eta: 0:00:15  lr: 0.000142  min_lr: 0.000142  loss: 0.9124 (0.9151)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2909 (0.2907)  rec_angle_loss: 0.6204 (0.6216)  total_loss: 0.9124 (0.9151)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5361 (0.5587)  time: 1.7553  data: 1.2959  max mem: 13238
Epoch: [43]  [18/19]  eta: 0:00:01  lr: 0.000141  min_lr: 0.000141  loss: 0.9116 (0.9123)  quant_loss: 0.0031 (0.0030)  rec_loss: 0.2889 (0.2881)  rec_angle_loss: 0.6204 (0.6212)  total_loss: 0.9116 (0.9123)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5008 (0.5099)  time: 1.1681  data: 0.7503  max mem: 13238
Epoch: [43] Total time: 0:00:23 (1.2413 s / it)
Averaged stats: lr: 0.000141  min_lr: 0.000141  loss: 0.9116 (0.9142)  quant_loss: 0.0031 (0.0030)  rec_loss: 0.2889 (0.2894)  rec_angle_loss: 0.6204 (0.6218)  total_loss: 0.9116 (0.9142)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5008 (0.5099)
Unused code in codebook: 7781
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.9545 (0.9545)  time: 5.5433  data: 5.4672  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9831 (0.9852)  time: 0.5598  data: 0.4971  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9760 (0.9635)  time: 0.4495  data: 0.3906  max mem: 13238
Validation: Total time: 0:00:07 (0.5444 s / it)
Averaged stats: loss: 0.9760 (0.9621)  quant_loss: 0.0039 (0.0039)  rec_loss: 0.2239 (0.2235)  rec_angle_loss: 0.6189 (0.6170)  total_loss: 0.8466 (0.8444)
Unused code in codebook: 7797
Validation loss of the network on the 10144 test EEG: 0.9621
Reset the codebook statistic info in quantizer before each epoch
Epoch: [44]  [ 0/19]  eta: 0:03:45  lr: 0.000141  min_lr: 0.000141  loss: 0.9115 (0.9115)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.2840 (0.2840)  rec_angle_loss: 0.6241 (0.6241)  total_loss: 0.9115 (0.9115)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4389 (0.4389)  time: 11.8830  data: 11.2610  max mem: 13238
Epoch: [44]  [10/19]  eta: 0:00:16  lr: 0.000139  min_lr: 0.000139  loss: 0.9112 (0.9094)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2861 (0.2850)  rec_angle_loss: 0.6208 (0.6209)  total_loss: 0.9112 (0.9094)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4200 (0.4162)  time: 1.7795  data: 1.3568  max mem: 13238
Epoch: [44]  [18/19]  eta: 0:00:01  lr: 0.000138  min_lr: 0.000138  loss: 0.9093 (0.9081)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2852 (0.2840)  rec_angle_loss: 0.6205 (0.6205)  total_loss: 0.9093 (0.9081)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3966 (0.4006)  time: 1.1819  data: 0.7856  max mem: 13238
Epoch: [44] Total time: 0:00:23 (1.2593 s / it)
Averaged stats: lr: 0.000138  min_lr: 0.000138  loss: 0.9093 (0.9087)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2852 (0.2840)  rec_angle_loss: 0.6205 (0.6212)  total_loss: 0.9093 (0.9087)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3966 (0.4006)
Unused code in codebook: 7791
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9478 (0.9478)  time: 5.4401  data: 5.2955  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9794 (0.9805)  time: 0.5682  data: 0.5002  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9714 (0.9583)  time: 0.4560  data: 0.3930  max mem: 13238
Validation: Total time: 0:00:07 (0.5507 s / it)
Averaged stats: loss: 0.9714 (0.9574)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2180 (0.2181)  rec_angle_loss: 0.6162 (0.6158)  total_loss: 0.8381 (0.8379)
Unused code in codebook: 7791
Validation loss of the network on the 10144 test EEG: 0.9574
Reset the codebook statistic info in quantizer before each epoch
Epoch: [45]  [ 0/19]  eta: 0:03:48  lr: 0.000137  min_lr: 0.000137  loss: 0.9108 (0.9108)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2835 (0.2835)  rec_angle_loss: 0.6238 (0.6238)  total_loss: 0.9108 (0.9108)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5809 (0.5809)  time: 12.0218  data: 10.8497  max mem: 13238
Epoch: [45]  [10/19]  eta: 0:00:15  lr: 0.000136  min_lr: 0.000136  loss: 0.9060 (0.9056)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2823 (0.2808)  rec_angle_loss: 0.6205 (0.6213)  total_loss: 0.9060 (0.9056)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7077 (0.7081)  time: 1.7647  data: 1.2601  max mem: 13238
Epoch: [45]  [18/19]  eta: 0:00:01  lr: 0.000135  min_lr: 0.000135  loss: 0.9054 (0.9055)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2803 (0.2804)  rec_angle_loss: 0.6216 (0.6216)  total_loss: 0.9054 (0.9055)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5949 (0.6395)  time: 1.1762  data: 0.7296  max mem: 13238
Epoch: [45] Total time: 0:00:23 (1.2504 s / it)
Averaged stats: lr: 0.000135  min_lr: 0.000135  loss: 0.9054 (0.9042)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2803 (0.2799)  rec_angle_loss: 0.6216 (0.6208)  total_loss: 0.9054 (0.9042)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5949 (0.6395)
Unused code in codebook: 7781
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:20  loss: 0.9402 (0.9402)  time: 5.7259  data: 5.6491  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9726 (0.9745)  time: 0.6048  data: 0.5432  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9645 (0.9514)  time: 0.4848  data: 0.4268  max mem: 13238
Validation: Total time: 0:00:08 (0.5811 s / it)
Averaged stats: loss: 0.9645 (0.9505)  quant_loss: 0.0040 (0.0041)  rec_loss: 0.2118 (0.2114)  rec_angle_loss: 0.6091 (0.6099)  total_loss: 0.8250 (0.8254)
Unused code in codebook: 7765
Validation loss of the network on the 10144 test EEG: 0.9505
Reset the codebook statistic info in quantizer before each epoch
Epoch: [46]  [ 0/19]  eta: 0:03:55  lr: 0.000134  min_lr: 0.000134  loss: 0.9115 (0.9115)  quant_loss: 0.0036 (0.0036)  rec_loss: 0.2833 (0.2833)  rec_angle_loss: 0.6245 (0.6245)  total_loss: 0.9115 (0.9115)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3364 (0.3364)  time: 12.3786  data: 10.9726  max mem: 13238
Epoch: [46]  [10/19]  eta: 0:00:15  lr: 0.000133  min_lr: 0.000133  loss: 0.9028 (0.9021)  quant_loss: 0.0037 (0.0037)  rec_loss: 0.2772 (0.2775)  rec_angle_loss: 0.6223 (0.6210)  total_loss: 0.9028 (0.9021)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5530 (0.5344)  time: 1.7529  data: 1.1992  max mem: 13238
Epoch: [46]  [18/19]  eta: 0:00:01  lr: 0.000131  min_lr: 0.000131  loss: 0.9014 (0.9006)  quant_loss: 0.0036 (0.0037)  rec_loss: 0.2766 (0.2765)  rec_angle_loss: 0.6194 (0.6204)  total_loss: 0.9014 (0.9006)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5647 (0.5630)  time: 1.1667  data: 0.6943  max mem: 13238
Epoch: [46] Total time: 0:00:23 (1.2425 s / it)
Averaged stats: lr: 0.000131  min_lr: 0.000131  loss: 0.9014 (0.8998)  quant_loss: 0.0036 (0.0037)  rec_loss: 0.2766 (0.2766)  rec_angle_loss: 0.6194 (0.6196)  total_loss: 0.9014 (0.8998)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5647 (0.5630)
Unused code in codebook: 7768
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.9357 (0.9357)  time: 5.5695  data: 5.2550  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9742 (0.9729)  time: 0.5819  data: 0.4891  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9632 (0.9498)  time: 0.4671  data: 0.3843  max mem: 13238
Validation: Total time: 0:00:07 (0.5648 s / it)
Averaged stats: loss: 0.9632 (0.9486)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2085 (0.2087)  rec_angle_loss: 0.6132 (0.6123)  total_loss: 0.8257 (0.8251)
Unused code in codebook: 7771
Validation loss of the network on the 10144 test EEG: 0.9486
Reset the codebook statistic info in quantizer before each epoch
Epoch: [47]  [ 0/19]  eta: 0:03:52  lr: 0.000131  min_lr: 0.000131  loss: 0.9005 (0.9005)  quant_loss: 0.0037 (0.0037)  rec_loss: 0.2766 (0.2766)  rec_angle_loss: 0.6202 (0.6202)  total_loss: 0.9005 (0.9005)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4899 (0.4899)  time: 12.2617  data: 10.8962  max mem: 13238
Epoch: [47]  [10/19]  eta: 0:00:15  lr: 0.000130  min_lr: 0.000130  loss: 0.8999 (0.8988)  quant_loss: 0.0036 (0.0036)  rec_loss: 0.2759 (0.2757)  rec_angle_loss: 0.6196 (0.6194)  total_loss: 0.8999 (0.8988)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5465 (0.5505)  time: 1.7574  data: 1.2506  max mem: 13238
Epoch: [47]  [18/19]  eta: 0:00:01  lr: 0.000128  min_lr: 0.000128  loss: 0.8980 (0.8972)  quant_loss: 0.0036 (0.0036)  rec_loss: 0.2746 (0.2748)  rec_angle_loss: 0.6189 (0.6188)  total_loss: 0.8980 (0.8972)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5499 (0.5516)  time: 1.1698  data: 0.7241  max mem: 13238
Epoch: [47] Total time: 0:00:23 (1.2454 s / it)
Averaged stats: lr: 0.000128  min_lr: 0.000128  loss: 0.8980 (0.8963)  quant_loss: 0.0036 (0.0036)  rec_loss: 0.2746 (0.2739)  rec_angle_loss: 0.6189 (0.6189)  total_loss: 0.8980 (0.8963)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5499 (0.5516)
Unused code in codebook: 7782
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9345 (0.9345)  time: 5.4967  data: 5.3912  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9712 (0.9712)  time: 0.5829  data: 0.5190  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9597 (0.9482)  time: 0.4677  data: 0.4078  max mem: 13238
Validation: Total time: 0:00:07 (0.5664 s / it)
Averaged stats: loss: 0.9597 (0.9468)  quant_loss: 0.0039 (0.0039)  rec_loss: 0.2064 (0.2061)  rec_angle_loss: 0.6200 (0.6136)  total_loss: 0.8303 (0.8236)
Unused code in codebook: 7783
Validation loss of the network on the 10144 test EEG: 0.9468
Reset the codebook statistic info in quantizer before each epoch
Epoch: [48]  [ 0/19]  eta: 0:03:47  lr: 0.000128  min_lr: 0.000128  loss: 0.8866 (0.8866)  quant_loss: 0.0036 (0.0036)  rec_loss: 0.2664 (0.2664)  rec_angle_loss: 0.6165 (0.6165)  total_loss: 0.8866 (0.8866)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7408 (0.7408)  time: 11.9951  data: 11.2275  max mem: 13238
Epoch: [48]  [10/19]  eta: 0:00:15  lr: 0.000126  min_lr: 0.000126  loss: 0.8949 (0.8941)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.2720 (0.2719)  rec_angle_loss: 0.6178 (0.6188)  total_loss: 0.8949 (0.8941)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6515 (0.6551)  time: 1.7683  data: 1.3323  max mem: 13238
Epoch: [48]  [18/19]  eta: 0:00:01  lr: 0.000125  min_lr: 0.000125  loss: 0.8930 (0.8933)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.2714 (0.2713)  rec_angle_loss: 0.6181 (0.6186)  total_loss: 0.8930 (0.8933)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6353 (0.6405)  time: 1.1750  data: 0.7714  max mem: 13238
Epoch: [48] Total time: 0:00:23 (1.2484 s / it)
Averaged stats: lr: 0.000125  min_lr: 0.000125  loss: 0.8930 (0.8935)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.2714 (0.2718)  rec_angle_loss: 0.6181 (0.6184)  total_loss: 0.8930 (0.8935)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6353 (0.6405)
Unused code in codebook: 7787
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9301 (0.9301)  time: 5.4878  data: 5.4172  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9682 (0.9671)  time: 0.5719  data: 0.5061  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9582 (0.9437)  time: 0.4589  data: 0.3977  max mem: 13238
Validation: Total time: 0:00:07 (0.5533 s / it)
Averaged stats: loss: 0.9582 (0.9426)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2038 (0.2034)  rec_angle_loss: 0.6153 (0.6123)  total_loss: 0.8226 (0.8192)
Unused code in codebook: 7788
Validation loss of the network on the 10144 test EEG: 0.9426
Reset the codebook statistic info in quantizer before each epoch
Epoch: [49]  [ 0/19]  eta: 0:03:52  lr: 0.000125  min_lr: 0.000125  loss: 0.8840 (0.8840)  quant_loss: 0.0033 (0.0033)  rec_loss: 0.2691 (0.2691)  rec_angle_loss: 0.6116 (0.6116)  total_loss: 0.8840 (0.8840)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6436 (0.6436)  time: 12.2346  data: 10.8335  max mem: 13238
Epoch: [49]  [10/19]  eta: 0:00:15  lr: 0.000123  min_lr: 0.000123  loss: 0.8902 (0.8903)  quant_loss: 0.0033 (0.0033)  rec_loss: 0.2692 (0.2698)  rec_angle_loss: 0.6174 (0.6172)  total_loss: 0.8902 (0.8903)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5519 (0.5255)  time: 1.7722  data: 1.2551  max mem: 13238
Epoch: [49]  [18/19]  eta: 0:00:01  lr: 0.000122  min_lr: 0.000122  loss: 0.8864 (0.8882)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.2674 (0.2685)  rec_angle_loss: 0.6167 (0.6164)  total_loss: 0.8864 (0.8882)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4396 (0.4749)  time: 1.1777  data: 0.7267  max mem: 13238
Epoch: [49] Total time: 0:00:23 (1.2531 s / it)
Averaged stats: lr: 0.000122  min_lr: 0.000122  loss: 0.8864 (0.8894)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.2674 (0.2693)  rec_angle_loss: 0.6167 (0.6168)  total_loss: 0.8864 (0.8894)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4396 (0.4749)
Unused code in codebook: 7791
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9244 (0.9244)  time: 5.4328  data: 5.3293  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9637 (0.9628)  time: 0.5728  data: 0.4977  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9524 (0.9394)  time: 0.4596  data: 0.3911  max mem: 13238
Validation: Total time: 0:00:07 (0.5539 s / it)
Averaged stats: loss: 0.9524 (0.9384)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2007 (0.2005)  rec_angle_loss: 0.6143 (0.6090)  total_loss: 0.8184 (0.8129)
Unused code in codebook: 7792
Validation loss of the network on the 10144 test EEG: 0.9384
Reset the codebook statistic info in quantizer before each epoch
Epoch: [50]  [ 0/19]  eta: 0:04:27  lr: 0.000121  min_lr: 0.000121  loss: 0.8892 (0.8892)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.2704 (0.2704)  rec_angle_loss: 0.6156 (0.6156)  total_loss: 0.8892 (0.8892)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3983 (0.3983)  time: 14.0860  data: 13.0311  max mem: 13238
Epoch: [50]  [10/19]  eta: 0:00:17  lr: 0.000120  min_lr: 0.000120  loss: 0.8899 (0.8885)  quant_loss: 0.0031 (0.0032)  rec_loss: 0.2699 (0.2692)  rec_angle_loss: 0.6168 (0.6161)  total_loss: 0.8899 (0.8885)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4310 (0.4734)  time: 1.9211  data: 1.4820  max mem: 13238
Epoch: [50]  [18/19]  eta: 0:00:01  lr: 0.000118  min_lr: 0.000118  loss: 0.8882 (0.8868)  quant_loss: 0.0031 (0.0031)  rec_loss: 0.2681 (0.2680)  rec_angle_loss: 0.6156 (0.6157)  total_loss: 0.8882 (0.8868)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5086 (0.5411)  time: 1.2637  data: 0.8582  max mem: 13238
Epoch: [50] Total time: 0:00:25 (1.3414 s / it)
Averaged stats: lr: 0.000118  min_lr: 0.000118  loss: 0.8882 (0.8864)  quant_loss: 0.0031 (0.0031)  rec_loss: 0.2681 (0.2677)  rec_angle_loss: 0.6156 (0.6156)  total_loss: 0.8882 (0.8864)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5086 (0.5411)
Unused code in codebook: 7793
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9220 (0.9220)  time: 5.4627  data: 5.3893  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9601 (0.9609)  time: 0.5792  data: 0.5135  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9513 (0.9371)  time: 0.4646  data: 0.4035  max mem: 13238
Validation: Total time: 0:00:07 (0.5623 s / it)
Averaged stats: loss: 0.9513 (0.9366)  quant_loss: 0.0033 (0.0033)  rec_loss: 0.1987 (0.1985)  rec_angle_loss: 0.6107 (0.6104)  total_loss: 0.8127 (0.8122)
Unused code in codebook: 7795
Validation loss of the network on the 10144 test EEG: 0.9366
Reset the codebook statistic info in quantizer before each epoch
Epoch: [51]  [ 0/19]  eta: 0:03:54  lr: 0.000118  min_lr: 0.000118  loss: 0.8864 (0.8864)  quant_loss: 0.0031 (0.0031)  rec_loss: 0.2700 (0.2700)  rec_angle_loss: 0.6133 (0.6133)  total_loss: 0.8864 (0.8864)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6477 (0.6477)  time: 12.3297  data: 10.9295  max mem: 13238
Epoch: [51]  [10/19]  eta: 0:00:16  lr: 0.000116  min_lr: 0.000116  loss: 0.8864 (0.8833)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.2674 (0.2660)  rec_angle_loss: 0.6152 (0.6143)  total_loss: 0.8864 (0.8833)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4687 (0.4866)  time: 1.7778  data: 1.2710  max mem: 13238
Epoch: [51]  [18/19]  eta: 0:00:01  lr: 0.000115  min_lr: 0.000115  loss: 0.8859 (0.8829)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.2666 (0.2656)  rec_angle_loss: 0.6153 (0.6142)  total_loss: 0.8859 (0.8829)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4850 (0.5527)  time: 1.1808  data: 0.7360  max mem: 13238
Epoch: [51] Total time: 0:00:23 (1.2554 s / it)
Averaged stats: lr: 0.000115  min_lr: 0.000115  loss: 0.8859 (0.8837)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.2666 (0.2658)  rec_angle_loss: 0.6153 (0.6149)  total_loss: 0.8859 (0.8837)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4850 (0.5527)
Unused code in codebook: 7794
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.9168 (0.9168)  time: 5.5536  data: 5.4397  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9591 (0.9578)  time: 0.5818  data: 0.5119  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9475 (0.9341)  time: 0.4668  data: 0.4022  max mem: 13238
Validation: Total time: 0:00:07 (0.5571 s / it)
Averaged stats: loss: 0.9475 (0.9333)  quant_loss: 0.0031 (0.0031)  rec_loss: 0.1988 (0.1983)  rec_angle_loss: 0.6083 (0.6072)  total_loss: 0.8102 (0.8086)
Unused code in codebook: 7794
Validation loss of the network on the 10144 test EEG: 0.9333
Reset the codebook statistic info in quantizer before each epoch
Epoch: [52]  [ 0/19]  eta: 0:03:42  lr: 0.000115  min_lr: 0.000115  loss: 0.8813 (0.8813)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2650 (0.2650)  rec_angle_loss: 0.6134 (0.6134)  total_loss: 0.8813 (0.8813)  weight_decay: 0.0001 (0.0001)  grad_norm: 1.0728 (1.0728)  time: 11.6877  data: 10.6135  max mem: 13238
Epoch: [52]  [10/19]  eta: 0:00:15  lr: 0.000113  min_lr: 0.000113  loss: 0.8872 (0.8861)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2677 (0.2667)  rec_angle_loss: 0.6158 (0.6164)  total_loss: 0.8872 (0.8861)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.8311 (0.8441)  time: 1.7563  data: 1.2460  max mem: 13238
Epoch: [52]  [18/19]  eta: 0:00:01  lr: 0.000112  min_lr: 0.000112  loss: 0.8872 (0.8852)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2664 (0.2662)  rec_angle_loss: 0.6158 (0.6161)  total_loss: 0.8872 (0.8852)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5834 (0.6646)  time: 1.1692  data: 0.7214  max mem: 13238
Epoch: [52] Total time: 0:00:23 (1.2451 s / it)
Averaged stats: lr: 0.000112  min_lr: 0.000112  loss: 0.8872 (0.8825)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2664 (0.2651)  rec_angle_loss: 0.6158 (0.6145)  total_loss: 0.8872 (0.8825)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5834 (0.6646)
Unused code in codebook: 7795
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.9163 (0.9163)  time: 5.5226  data: 5.4476  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9553 (0.9553)  time: 0.5752  data: 0.5137  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9454 (0.9313)  time: 0.4616  data: 0.4036  max mem: 13238
Validation: Total time: 0:00:07 (0.5599 s / it)
Averaged stats: loss: 0.9454 (0.9304)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.1952 (0.1948)  rec_angle_loss: 0.6065 (0.6048)  total_loss: 0.8048 (0.8028)
Unused code in codebook: 7794
Validation loss of the network on the 10144 test EEG: 0.9304
Reset the codebook statistic info in quantizer before each epoch
Epoch: [53]  [ 0/19]  eta: 0:03:41  lr: 0.000112  min_lr: 0.000112  loss: 0.8794 (0.8794)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2618 (0.2618)  rec_angle_loss: 0.6147 (0.6147)  total_loss: 0.8794 (0.8794)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4201 (0.4201)  time: 11.6639  data: 11.2903  max mem: 13238
Epoch: [53]  [10/19]  eta: 0:00:16  lr: 0.000110  min_lr: 0.000110  loss: 0.8747 (0.8786)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2622 (0.2634)  rec_angle_loss: 0.6116 (0.6122)  total_loss: 0.8747 (0.8786)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3252 (0.3255)  time: 1.7823  data: 1.3979  max mem: 13238
Epoch: [53]  [18/19]  eta: 0:00:01  lr: 0.000108  min_lr: 0.000108  loss: 0.8747 (0.8775)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2618 (0.2625)  rec_angle_loss: 0.6116 (0.6121)  total_loss: 0.8747 (0.8775)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3252 (0.3255)  time: 1.1833  data: 0.8094  max mem: 13238
Epoch: [53] Total time: 0:00:23 (1.2582 s / it)
Averaged stats: lr: 0.000108  min_lr: 0.000108  loss: 0.8747 (0.8788)  quant_loss: 0.0029 (0.0028)  rec_loss: 0.2618 (0.2628)  rec_angle_loss: 0.6116 (0.6132)  total_loss: 0.8747 (0.8788)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3252 (0.3255)
Unused code in codebook: 7795
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:21  loss: 0.9143 (0.9143)  time: 5.8545  data: 4.9585  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9512 (0.9531)  time: 0.5891  data: 0.4508  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9426 (0.9290)  time: 0.4724  data: 0.3542  max mem: 13238
Validation: Total time: 0:00:07 (0.5683 s / it)
Averaged stats: loss: 0.9426 (0.9287)  quant_loss: 0.0031 (0.0031)  rec_loss: 0.1929 (0.1927)  rec_angle_loss: 0.6064 (0.6047)  total_loss: 0.8024 (0.8006)
Unused code in codebook: 7794
Validation loss of the network on the 10144 test EEG: 0.9287
Reset the codebook statistic info in quantizer before each epoch
Epoch: [54]  [ 0/19]  eta: 0:03:55  lr: 0.000108  min_lr: 0.000108  loss: 0.8742 (0.8742)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2602 (0.2602)  rec_angle_loss: 0.6112 (0.6112)  total_loss: 0.8742 (0.8742)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3599 (0.3599)  time: 12.3924  data: 11.0218  max mem: 13238
Epoch: [54]  [10/19]  eta: 0:00:16  lr: 0.000107  min_lr: 0.000107  loss: 0.8761 (0.8780)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2619 (0.2625)  rec_angle_loss: 0.6126 (0.6127)  total_loss: 0.8761 (0.8780)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5423 (0.5725)  time: 1.8801  data: 1.3922  max mem: 13238
Epoch: [54]  [18/19]  eta: 0:00:01  lr: 0.000105  min_lr: 0.000105  loss: 0.8786 (0.8791)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2641 (0.2631)  rec_angle_loss: 0.6128 (0.6131)  total_loss: 0.8786 (0.8791)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5936 (0.5981)  time: 1.2407  data: 0.8062  max mem: 13238
Epoch: [54] Total time: 0:00:25 (1.3164 s / it)
Averaged stats: lr: 0.000105  min_lr: 0.000105  loss: 0.8786 (0.8777)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2641 (0.2620)  rec_angle_loss: 0.6128 (0.6128)  total_loss: 0.8786 (0.8777)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5936 (0.5981)
Unused code in codebook: 7795
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:20  loss: 0.9118 (0.9118)  time: 5.7342  data: 5.3132  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9502 (0.9510)  time: 0.5829  data: 0.4833  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9387 (0.9273)  time: 0.4675  data: 0.3798  max mem: 13238
Validation: Total time: 0:00:07 (0.5610 s / it)
Averaged stats: loss: 0.9387 (0.9268)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1921 (0.1916)  rec_angle_loss: 0.6080 (0.6053)  total_loss: 0.8031 (0.7999)
Unused code in codebook: 7793
Validation loss of the network on the 10144 test EEG: 0.9268
Reset the codebook statistic info in quantizer before each epoch
Epoch: [55]  [ 0/19]  eta: 0:03:52  lr: 0.000105  min_lr: 0.000105  loss: 0.8714 (0.8714)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2604 (0.2604)  rec_angle_loss: 0.6082 (0.6082)  total_loss: 0.8714 (0.8714)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4764 (0.4764)  time: 12.2359  data: 11.0072  max mem: 13238
Epoch: [55]  [10/19]  eta: 0:00:15  lr: 0.000103  min_lr: 0.000103  loss: 0.8755 (0.8740)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2599 (0.2596)  rec_angle_loss: 0.6132 (0.6117)  total_loss: 0.8755 (0.8740)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6002 (0.5935)  time: 1.7625  data: 1.2318  max mem: 13238
Epoch: [55]  [18/19]  eta: 0:00:01  lr: 0.000102  min_lr: 0.000102  loss: 0.8757 (0.8756)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2603 (0.2607)  rec_angle_loss: 0.6133 (0.6121)  total_loss: 0.8757 (0.8756)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4948 (0.5492)  time: 1.1717  data: 0.7132  max mem: 13238
Epoch: [55] Total time: 0:00:23 (1.2478 s / it)
Averaged stats: lr: 0.000102  min_lr: 0.000102  loss: 0.8757 (0.8757)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2603 (0.2605)  rec_angle_loss: 0.6133 (0.6124)  total_loss: 0.8757 (0.8757)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4948 (0.5492)
Unused code in codebook: 7794
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.9090 (0.9090)  time: 5.4515  data: 4.8598  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9463 (0.9492)  time: 0.5778  data: 0.4670  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9375 (0.9249)  time: 0.4636  data: 0.3669  max mem: 13238
Validation: Total time: 0:00:07 (0.5596 s / it)
Averaged stats: loss: 0.9375 (0.9242)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1905 (0.1903)  rec_angle_loss: 0.6041 (0.6021)  total_loss: 0.7976 (0.7954)
Unused code in codebook: 7792
Validation loss of the network on the 10144 test EEG: 0.9242
Reset the codebook statistic info in quantizer before each epoch
Epoch: [56]  [ 0/19]  eta: 0:03:43  lr: 0.000102  min_lr: 0.000102  loss: 0.8688 (0.8688)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2530 (0.2530)  rec_angle_loss: 0.6130 (0.6130)  total_loss: 0.8688 (0.8688)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4959 (0.4959)  time: 11.7710  data: 11.2877  max mem: 13238
Epoch: [56]  [10/19]  eta: 0:00:15  lr: 0.000100  min_lr: 0.000100  loss: 0.8737 (0.8738)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2603 (0.2591)  rec_angle_loss: 0.6113 (0.6119)  total_loss: 0.8737 (0.8738)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4959 (0.5494)  time: 1.7678  data: 1.3219  max mem: 13238
Epoch: [56]  [18/19]  eta: 0:00:01  lr: 0.000099  min_lr: 0.000099  loss: 0.8734 (0.8730)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2594 (0.2588)  rec_angle_loss: 0.6116 (0.6115)  total_loss: 0.8734 (0.8730)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5780 (0.5580)  time: 1.1763  data: 0.7653  max mem: 13238
Epoch: [56] Total time: 0:00:23 (1.2514 s / it)
Averaged stats: lr: 0.000099  min_lr: 0.000099  loss: 0.8734 (0.8737)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2594 (0.2595)  rec_angle_loss: 0.6116 (0.6115)  total_loss: 0.8734 (0.8737)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5780 (0.5580)
Unused code in codebook: 7794
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 0.9080 (0.9080)  time: 5.3137  data: 5.2446  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9448 (0.9465)  time: 0.5704  data: 0.5044  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9350 (0.9225)  time: 0.4577  data: 0.3963  max mem: 13238
Validation: Total time: 0:00:07 (0.5533 s / it)
Averaged stats: loss: 0.9350 (0.9218)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1893 (0.1890)  rec_angle_loss: 0.6052 (0.6027)  total_loss: 0.7974 (0.7948)
Unused code in codebook: 7794
Validation loss of the network on the 10144 test EEG: 0.9218
Reset the codebook statistic info in quantizer before each epoch
Epoch: [57]  [ 0/19]  eta: 0:03:45  lr: 0.000098  min_lr: 0.000098  loss: 0.8685 (0.8685)  quant_loss: 0.0027 (0.0027)  rec_loss: 0.2535 (0.2535)  rec_angle_loss: 0.6122 (0.6122)  total_loss: 0.8685 (0.8685)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3158 (0.3158)  time: 11.8769  data: 10.8307  max mem: 13238
Epoch: [57]  [10/19]  eta: 0:00:16  lr: 0.000097  min_lr: 0.000097  loss: 0.8711 (0.8725)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2589 (0.2585)  rec_angle_loss: 0.6112 (0.6112)  total_loss: 0.8711 (0.8725)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4243 (0.4320)  time: 1.7870  data: 1.2808  max mem: 13238
Epoch: [57]  [18/19]  eta: 0:00:01  lr: 0.000095  min_lr: 0.000095  loss: 0.8730 (0.8732)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2589 (0.2590)  rec_angle_loss: 0.6112 (0.6115)  total_loss: 0.8730 (0.8732)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4243 (0.4980)  time: 1.1861  data: 0.7416  max mem: 13238
Epoch: [57] Total time: 0:00:23 (1.2610 s / it)
Averaged stats: lr: 0.000095  min_lr: 0.000095  loss: 0.8730 (0.8721)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2589 (0.2584)  rec_angle_loss: 0.6112 (0.6110)  total_loss: 0.8730 (0.8721)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4243 (0.4980)
Unused code in codebook: 7794
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.9092 (0.9092)  time: 5.3683  data: 5.2954  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9454 (0.9477)  time: 0.5620  data: 0.4985  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9355 (0.9231)  time: 0.4513  data: 0.3917  max mem: 13238
Validation: Total time: 0:00:07 (0.5567 s / it)
Averaged stats: loss: 0.9355 (0.9227)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1883 (0.1886)  rec_angle_loss: 0.6032 (0.6041)  total_loss: 0.7945 (0.7956)
Unused code in codebook: 7793
Validation loss of the network on the 10144 test EEG: 0.9227
Reset the codebook statistic info in quantizer before each epoch
Epoch: [58]  [ 0/19]  eta: 0:03:53  lr: 0.000095  min_lr: 0.000095  loss: 0.8709 (0.8709)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2603 (0.2603)  rec_angle_loss: 0.6078 (0.6078)  total_loss: 0.8709 (0.8709)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7105 (0.7105)  time: 12.3098  data: 10.8762  max mem: 13238
Epoch: [58]  [10/19]  eta: 0:00:15  lr: 0.000093  min_lr: 0.000093  loss: 0.8709 (0.8702)  quant_loss: 0.0027 (0.0027)  rec_loss: 0.2569 (0.2572)  rec_angle_loss: 0.6081 (0.6103)  total_loss: 0.8709 (0.8702)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4969 (0.5342)  time: 1.7772  data: 1.2071  max mem: 13238
Epoch: [58]  [18/19]  eta: 0:00:01  lr: 0.000092  min_lr: 0.000092  loss: 0.8690 (0.8693)  quant_loss: 0.0027 (0.0028)  rec_loss: 0.2567 (0.2565)  rec_angle_loss: 0.6097 (0.6101)  total_loss: 0.8690 (0.8693)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4933 (0.5232)  time: 1.1810  data: 0.6989  max mem: 13238
Epoch: [58] Total time: 0:00:23 (1.2588 s / it)
Averaged stats: lr: 0.000092  min_lr: 0.000092  loss: 0.8690 (0.8704)  quant_loss: 0.0027 (0.0028)  rec_loss: 0.2567 (0.2572)  rec_angle_loss: 0.6097 (0.6105)  total_loss: 0.8690 (0.8704)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4933 (0.5232)
Unused code in codebook: 7794
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 0.9036 (0.9036)  time: 5.3141  data: 5.2408  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9425 (0.9432)  time: 0.5614  data: 0.4978  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9316 (0.9191)  time: 0.4508  data: 0.3912  max mem: 13238
Validation: Total time: 0:00:07 (0.5426 s / it)
Averaged stats: loss: 0.9316 (0.9185)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1880 (0.1880)  rec_angle_loss: 0.6043 (0.6007)  total_loss: 0.7953 (0.7917)
Unused code in codebook: 7793
Validation loss of the network on the 10144 test EEG: 0.9185
Reset the codebook statistic info in quantizer before each epoch
Epoch: [59]  [ 0/19]  eta: 0:03:44  lr: 0.000092  min_lr: 0.000092  loss: 0.8667 (0.8667)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2558 (0.2558)  rec_angle_loss: 0.6081 (0.6081)  total_loss: 0.8667 (0.8667)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4387 (0.4387)  time: 11.8316  data: 11.3606  max mem: 13238
Epoch: [59]  [10/19]  eta: 0:00:15  lr: 0.000090  min_lr: 0.000090  loss: 0.8680 (0.8683)  quant_loss: 0.0027 (0.0028)  rec_loss: 0.2559 (0.2568)  rec_angle_loss: 0.6088 (0.6087)  total_loss: 0.8680 (0.8683)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5128 (0.5282)  time: 1.7736  data: 1.3941  max mem: 13238
Epoch: [59]  [18/19]  eta: 0:00:01  lr: 0.000089  min_lr: 0.000089  loss: 0.8697 (0.8689)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2559 (0.2565)  rec_angle_loss: 0.6100 (0.6096)  total_loss: 0.8697 (0.8689)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5158 (0.5719)  time: 1.1787  data: 0.8072  max mem: 13238
Epoch: [59] Total time: 0:00:23 (1.2543 s / it)
Averaged stats: lr: 0.000089  min_lr: 0.000089  loss: 0.8697 (0.8693)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2559 (0.2563)  rec_angle_loss: 0.6100 (0.6103)  total_loss: 0.8697 (0.8693)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5158 (0.5719)
Unused code in codebook: 7794
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:11  loss: 0.9023 (0.9023)  time: 5.1311  data: 5.0600  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9427 (0.9427)  time: 0.5323  data: 0.4707  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9294 (0.9189)  time: 0.4279  data: 0.3699  max mem: 13238
Validation: Total time: 0:00:07 (0.5239 s / it)
Averaged stats: loss: 0.9294 (0.9180)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1865 (0.1866)  rec_angle_loss: 0.6081 (0.6034)  total_loss: 0.7976 (0.7931)
Unused code in codebook: 7791
Validation loss of the network on the 10144 test EEG: 0.9180
Reset the codebook statistic info in quantizer before each epoch
Epoch: [60]  [ 0/19]  eta: 0:03:50  lr: 0.000089  min_lr: 0.000089  loss: 0.8578 (0.8578)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2475 (0.2475)  rec_angle_loss: 0.6076 (0.6076)  total_loss: 0.8578 (0.8578)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5482 (0.5482)  time: 12.1347  data: 11.0040  max mem: 13238
Epoch: [60]  [10/19]  eta: 0:00:16  lr: 0.000087  min_lr: 0.000087  loss: 0.8663 (0.8672)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2541 (0.2554)  rec_angle_loss: 0.6095 (0.6091)  total_loss: 0.8663 (0.8672)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5422 (0.5421)  time: 1.7918  data: 1.2849  max mem: 13238
Epoch: [60]  [18/19]  eta: 0:00:01  lr: 0.000085  min_lr: 0.000085  loss: 0.8664 (0.8678)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2550 (0.2558)  rec_angle_loss: 0.6095 (0.6092)  total_loss: 0.8664 (0.8678)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5612 (0.6224)  time: 1.1889  data: 0.7441  max mem: 13238
Epoch: [60] Total time: 0:00:24 (1.2650 s / it)
Averaged stats: lr: 0.000085  min_lr: 0.000085  loss: 0.8664 (0.8679)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2550 (0.2554)  rec_angle_loss: 0.6095 (0.6098)  total_loss: 0.8664 (0.8679)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5612 (0.6224)
Unused code in codebook: 7795
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.9026 (0.9026)  time: 5.5672  data: 5.4913  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9407 (0.9420)  time: 0.5764  data: 0.5144  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9284 (0.9178)  time: 0.4624  data: 0.4042  max mem: 13238
Validation: Total time: 0:00:07 (0.5599 s / it)
Averaged stats: loss: 0.9284 (0.9171)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1844 (0.1844)  rec_angle_loss: 0.6077 (0.6027)  total_loss: 0.7950 (0.7901)
Unused code in codebook: 7794
Validation loss of the network on the 10144 test EEG: 0.9171
Reset the codebook statistic info in quantizer before each epoch
Epoch: [61]  [ 0/19]  eta: 0:03:42  lr: 0.000085  min_lr: 0.000085  loss: 0.8832 (0.8832)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2660 (0.2660)  rec_angle_loss: 0.6143 (0.6143)  total_loss: 0.8832 (0.8832)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5883 (0.5883)  time: 11.6958  data: 11.1931  max mem: 13238
Epoch: [61]  [10/19]  eta: 0:00:16  lr: 0.000084  min_lr: 0.000084  loss: 0.8637 (0.8662)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2527 (0.2550)  rec_angle_loss: 0.6081 (0.6083)  total_loss: 0.8637 (0.8662)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4589 (0.4587)  time: 1.7821  data: 1.3972  max mem: 13238
Epoch: [61]  [18/19]  eta: 0:00:01  lr: 0.000082  min_lr: 0.000082  loss: 0.8658 (0.8669)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2556 (0.2555)  rec_angle_loss: 0.6084 (0.6086)  total_loss: 0.8658 (0.8669)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4576 (0.4514)  time: 1.1834  data: 0.8089  max mem: 13238
Epoch: [61] Total time: 0:00:23 (1.2572 s / it)
Averaged stats: lr: 0.000082  min_lr: 0.000082  loss: 0.8658 (0.8660)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2556 (0.2543)  rec_angle_loss: 0.6084 (0.6089)  total_loss: 0.8658 (0.8660)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4576 (0.4514)
Unused code in codebook: 7795
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:18  loss: 0.8963 (0.8963)  time: 5.6386  data: 4.9173  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9385 (0.9383)  time: 0.5730  data: 0.4471  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9248 (0.9136)  time: 0.4607  data: 0.3513  max mem: 13238
Validation: Total time: 0:00:07 (0.5567 s / it)
Averaged stats: loss: 0.9248 (0.9135)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1840 (0.1837)  rec_angle_loss: 0.5999 (0.5993)  total_loss: 0.7870 (0.7860)
Unused code in codebook: 7791
Validation loss of the network on the 10144 test EEG: 0.9135
Reset the codebook statistic info in quantizer before each epoch
Epoch: [62]  [ 0/19]  eta: 0:03:48  lr: 0.000082  min_lr: 0.000082  loss: 0.8727 (0.8727)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2571 (0.2571)  rec_angle_loss: 0.6128 (0.6128)  total_loss: 0.8727 (0.8727)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5255 (0.5255)  time: 12.0304  data: 11.1257  max mem: 13238
Epoch: [62]  [10/19]  eta: 0:00:16  lr: 0.000080  min_lr: 0.000080  loss: 0.8662 (0.8660)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2534 (0.2529)  rec_angle_loss: 0.6106 (0.6103)  total_loss: 0.8662 (0.8660)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3961 (0.4053)  time: 1.7795  data: 1.3432  max mem: 13238
Epoch: [62]  [18/19]  eta: 0:00:01  lr: 0.000079  min_lr: 0.000079  loss: 0.8651 (0.8661)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2534 (0.2537)  rec_angle_loss: 0.6096 (0.6096)  total_loss: 0.8651 (0.8661)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4053 (0.4406)  time: 1.1813  data: 0.7777  max mem: 13238
Epoch: [62] Total time: 0:00:23 (1.2555 s / it)
Averaged stats: lr: 0.000079  min_lr: 0.000079  loss: 0.8651 (0.8648)  quant_loss: 0.0028 (0.0028)  rec_loss: 0.2534 (0.2532)  rec_angle_loss: 0.6096 (0.6088)  total_loss: 0.8651 (0.8648)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4053 (0.4406)
Unused code in codebook: 7794
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:20  loss: 0.8961 (0.8961)  time: 5.7380  data: 4.8820  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9367 (0.9377)  time: 0.5875  data: 0.4439  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9240 (0.9129)  time: 0.4712  data: 0.3488  max mem: 13238
Validation: Total time: 0:00:07 (0.5653 s / it)
Averaged stats: loss: 0.9240 (0.9126)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.1825 (0.1826)  rec_angle_loss: 0.6018 (0.5992)  total_loss: 0.7873 (0.7849)
Unused code in codebook: 7790
Validation loss of the network on the 10144 test EEG: 0.9126
Reset the codebook statistic info in quantizer before each epoch
Epoch: [63]  [ 0/19]  eta: 0:04:18  lr: 0.000079  min_lr: 0.000079  loss: 0.8588 (0.8588)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2501 (0.2501)  rec_angle_loss: 0.6058 (0.6058)  total_loss: 0.8588 (0.8588)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2199 (0.2199)  time: 13.5869  data: 12.5050  max mem: 13238
Epoch: [63]  [10/19]  eta: 0:00:17  lr: 0.000077  min_lr: 0.000077  loss: 0.8626 (0.8635)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2537 (0.2529)  rec_angle_loss: 0.6070 (0.6077)  total_loss: 0.8626 (0.8635)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5463 (0.5210)  time: 1.9080  data: 1.4448  max mem: 13238
Epoch: [63]  [18/19]  eta: 0:00:01  lr: 0.000076  min_lr: 0.000076  loss: 0.8626 (0.8633)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2508 (0.2521)  rec_angle_loss: 0.6070 (0.6083)  total_loss: 0.8626 (0.8633)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5992 (0.5649)  time: 1.2560  data: 0.8365  max mem: 13238
Epoch: [63] Total time: 0:00:25 (1.3316 s / it)
Averaged stats: lr: 0.000076  min_lr: 0.000076  loss: 0.8626 (0.8639)  quant_loss: 0.0029 (0.0029)  rec_loss: 0.2508 (0.2522)  rec_angle_loss: 0.6070 (0.6087)  total_loss: 0.8626 (0.8639)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5992 (0.5649)
Unused code in codebook: 7793
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.8962 (0.8962)  time: 5.3785  data: 5.0178  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9378 (0.9375)  time: 0.5805  data: 0.4896  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9241 (0.9127)  time: 0.4656  data: 0.3847  max mem: 13238
Validation: Total time: 0:00:07 (0.5640 s / it)
Averaged stats: loss: 0.9241 (0.9124)  quant_loss: 0.0031 (0.0031)  rec_loss: 0.1816 (0.1811)  rec_angle_loss: 0.6019 (0.6015)  total_loss: 0.7866 (0.7857)
Unused code in codebook: 7790
Validation loss of the network on the 10144 test EEG: 0.9124
Reset the codebook statistic info in quantizer before each epoch
Epoch: [64]  [ 0/19]  eta: 0:03:48  lr: 0.000076  min_lr: 0.000076  loss: 0.8683 (0.8683)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.2541 (0.2541)  rec_angle_loss: 0.6112 (0.6112)  total_loss: 0.8683 (0.8683)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4737 (0.4737)  time: 12.0453  data: 11.0858  max mem: 13238
Epoch: [64]  [10/19]  eta: 0:00:15  lr: 0.000074  min_lr: 0.000074  loss: 0.8599 (0.8619)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.2493 (0.2504)  rec_angle_loss: 0.6076 (0.6085)  total_loss: 0.8599 (0.8619)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5487 (0.5741)  time: 1.7397  data: 1.2961  max mem: 13238
Epoch: [64]  [18/19]  eta: 0:00:01  lr: 0.000073  min_lr: 0.000073  loss: 0.8608 (0.8620)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.2495 (0.2502)  rec_angle_loss: 0.6093 (0.6088)  total_loss: 0.8608 (0.8620)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5487 (0.5535)  time: 1.1615  data: 0.7505  max mem: 13238
Epoch: [64] Total time: 0:00:23 (1.2356 s / it)
Averaged stats: lr: 0.000073  min_lr: 0.000073  loss: 0.8608 (0.8628)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.2495 (0.2512)  rec_angle_loss: 0.6093 (0.6086)  total_loss: 0.8608 (0.8628)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5487 (0.5535)
Unused code in codebook: 7794
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8954 (0.8954)  time: 5.4401  data: 5.3687  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9373 (0.9364)  time: 0.5677  data: 0.5021  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9226 (0.9112)  time: 0.4557  data: 0.3945  max mem: 13238
Validation: Total time: 0:00:07 (0.5490 s / it)
Averaged stats: loss: 0.9226 (0.9108)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.1805 (0.1796)  rec_angle_loss: 0.5999 (0.5995)  total_loss: 0.7836 (0.7824)
Unused code in codebook: 7791
Validation loss of the network on the 10144 test EEG: 0.9108
Reset the codebook statistic info in quantizer before each epoch
Epoch: [65]  [ 0/19]  eta: 0:03:49  lr: 0.000073  min_lr: 0.000073  loss: 0.8586 (0.8586)  quant_loss: 0.0030 (0.0030)  rec_loss: 0.2505 (0.2505)  rec_angle_loss: 0.6050 (0.6050)  total_loss: 0.8586 (0.8586)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.8058 (0.8058)  time: 12.0915  data: 10.9091  max mem: 13238
Epoch: [65]  [10/19]  eta: 0:00:15  lr: 0.000071  min_lr: 0.000071  loss: 0.8622 (0.8619)  quant_loss: 0.0031 (0.0031)  rec_loss: 0.2505 (0.2500)  rec_angle_loss: 0.6076 (0.6088)  total_loss: 0.8622 (0.8619)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5616 (0.6308)  time: 1.7470  data: 1.2485  max mem: 13238
Epoch: [65]  [18/19]  eta: 0:00:01  lr: 0.000070  min_lr: 0.000070  loss: 0.8614 (0.8619)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.2495 (0.2500)  rec_angle_loss: 0.6082 (0.6087)  total_loss: 0.8614 (0.8619)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4799 (0.5434)  time: 1.1637  data: 0.7229  max mem: 13238
Epoch: [65] Total time: 0:00:23 (1.2371 s / it)
Averaged stats: lr: 0.000070  min_lr: 0.000070  loss: 0.8614 (0.8620)  quant_loss: 0.0032 (0.0032)  rec_loss: 0.2495 (0.2500)  rec_angle_loss: 0.6082 (0.6088)  total_loss: 0.8614 (0.8620)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4799 (0.5434)
Unused code in codebook: 7793
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8912 (0.8912)  time: 5.4401  data: 4.9626  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9335 (0.9341)  time: 0.5796  data: 0.4750  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9222 (0.9089)  time: 0.4665  data: 0.3737  max mem: 13238
Validation: Total time: 0:00:07 (0.5629 s / it)
Averaged stats: loss: 0.9222 (0.9084)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.1787 (0.1785)  rec_angle_loss: 0.5992 (0.5975)  total_loss: 0.7812 (0.7794)
Unused code in codebook: 7791
Validation loss of the network on the 10144 test EEG: 0.9084
Reset the codebook statistic info in quantizer before each epoch
Epoch: [66]  [ 0/19]  eta: 0:03:48  lr: 0.000069  min_lr: 0.000069  loss: 0.8567 (0.8567)  quant_loss: 0.0033 (0.0033)  rec_loss: 0.2476 (0.2476)  rec_angle_loss: 0.6058 (0.6058)  total_loss: 0.8567 (0.8567)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5157 (0.5157)  time: 12.0070  data: 10.8374  max mem: 13238
Epoch: [66]  [10/19]  eta: 0:00:16  lr: 0.000068  min_lr: 0.000068  loss: 0.8569 (0.8593)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.2476 (0.2485)  rec_angle_loss: 0.6058 (0.6075)  total_loss: 0.8569 (0.8593)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4077 (0.3870)  time: 1.7801  data: 1.2519  max mem: 13238
Epoch: [66]  [18/19]  eta: 0:00:01  lr: 0.000067  min_lr: 0.000067  loss: 0.8589 (0.8608)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.2476 (0.2485)  rec_angle_loss: 0.6090 (0.6089)  total_loss: 0.8589 (0.8608)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4312 (0.4607)  time: 1.1823  data: 0.7248  max mem: 13238
Epoch: [66] Total time: 0:00:23 (1.2584 s / it)
Averaged stats: lr: 0.000067  min_lr: 0.000067  loss: 0.8589 (0.8609)  quant_loss: 0.0034 (0.0034)  rec_loss: 0.2476 (0.2487)  rec_angle_loss: 0.6090 (0.6087)  total_loss: 0.8589 (0.8609)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4312 (0.4607)
Unused code in codebook: 7785
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8867 (0.8867)  time: 5.4531  data: 4.9181  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9335 (0.9330)  time: 0.5804  data: 0.4612  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9185 (0.9079)  time: 0.4656  data: 0.3624  max mem: 13238
Validation: Total time: 0:00:07 (0.5637 s / it)
Averaged stats: loss: 0.9185 (0.9073)  quant_loss: 0.0036 (0.0036)  rec_loss: 0.1782 (0.1776)  rec_angle_loss: 0.6007 (0.5986)  total_loss: 0.7825 (0.7799)
Unused code in codebook: 7788
Validation loss of the network on the 10144 test EEG: 0.9073
Reset the codebook statistic info in quantizer before each epoch
Epoch: [67]  [ 0/19]  eta: 0:03:58  lr: 0.000066  min_lr: 0.000066  loss: 0.8616 (0.8616)  quant_loss: 0.0035 (0.0035)  rec_loss: 0.2494 (0.2494)  rec_angle_loss: 0.6087 (0.6087)  total_loss: 0.8616 (0.8616)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5912 (0.5912)  time: 12.5474  data: 11.1704  max mem: 13238
Epoch: [67]  [10/19]  eta: 0:00:17  lr: 0.000065  min_lr: 0.000065  loss: 0.8610 (0.8601)  quant_loss: 0.0036 (0.0036)  rec_loss: 0.2478 (0.2474)  rec_angle_loss: 0.6087 (0.6091)  total_loss: 0.8610 (0.8601)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4525 (0.4522)  time: 1.9289  data: 1.4356  max mem: 13238
Epoch: [67]  [18/19]  eta: 0:00:01  lr: 0.000064  min_lr: 0.000064  loss: 0.8612 (0.8607)  quant_loss: 0.0037 (0.0037)  rec_loss: 0.2472 (0.2472)  rec_angle_loss: 0.6095 (0.6097)  total_loss: 0.8612 (0.8607)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5323 (0.5298)  time: 1.2692  data: 0.8313  max mem: 13238
Epoch: [67] Total time: 0:00:25 (1.3445 s / it)
Averaged stats: lr: 0.000064  min_lr: 0.000064  loss: 0.8612 (0.8601)  quant_loss: 0.0037 (0.0037)  rec_loss: 0.2472 (0.2475)  rec_angle_loss: 0.6095 (0.6089)  total_loss: 0.8612 (0.8601)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5323 (0.5298)
Unused code in codebook: 7788
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8892 (0.8892)  time: 5.4407  data: 4.9781  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9339 (0.9331)  time: 0.5729  data: 0.4709  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9213 (0.9078)  time: 0.4604  data: 0.3700  max mem: 13238
Validation: Total time: 0:00:07 (0.5567 s / it)
Averaged stats: loss: 0.9213 (0.9073)  quant_loss: 0.0039 (0.0039)  rec_loss: 0.1758 (0.1755)  rec_angle_loss: 0.6003 (0.5983)  total_loss: 0.7800 (0.7778)
Unused code in codebook: 7783
Validation loss of the network on the 10144 test EEG: 0.9073
Reset the codebook statistic info in quantizer before each epoch
Epoch: [68]  [ 0/19]  eta: 0:03:46  lr: 0.000063  min_lr: 0.000063  loss: 0.8548 (0.8548)  quant_loss: 0.0038 (0.0038)  rec_loss: 0.2448 (0.2448)  rec_angle_loss: 0.6062 (0.6062)  total_loss: 0.8548 (0.8548)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6666 (0.6666)  time: 11.9179  data: 11.1231  max mem: 13238
Epoch: [68]  [10/19]  eta: 0:00:16  lr: 0.000062  min_lr: 0.000062  loss: 0.8574 (0.8586)  quant_loss: 0.0038 (0.0039)  rec_loss: 0.2455 (0.2461)  rec_angle_loss: 0.6090 (0.6087)  total_loss: 0.8574 (0.8586)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5846 (0.6168)  time: 1.7811  data: 1.3321  max mem: 13238
Epoch: [68]  [18/19]  eta: 0:00:01  lr: 0.000061  min_lr: 0.000061  loss: 0.8583 (0.8591)  quant_loss: 0.0039 (0.0039)  rec_loss: 0.2455 (0.2462)  rec_angle_loss: 0.6090 (0.6090)  total_loss: 0.8583 (0.8591)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6507 (0.6322)  time: 1.1835  data: 0.7712  max mem: 13238
Epoch: [68] Total time: 0:00:23 (1.2597 s / it)
Averaged stats: lr: 0.000061  min_lr: 0.000061  loss: 0.8583 (0.8590)  quant_loss: 0.0039 (0.0039)  rec_loss: 0.2455 (0.2461)  rec_angle_loss: 0.6090 (0.6090)  total_loss: 0.8583 (0.8590)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6507 (0.6322)
Unused code in codebook: 7781
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 0.8865 (0.8865)  time: 5.3250  data: 5.2534  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9316 (0.9312)  time: 0.5640  data: 0.4991  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9181 (0.9059)  time: 0.4527  data: 0.3922  max mem: 13238
Validation: Total time: 0:00:07 (0.5552 s / it)
Averaged stats: loss: 0.9181 (0.9053)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.1740 (0.1740)  rec_angle_loss: 0.6004 (0.5978)  total_loss: 0.7787 (0.7759)
Unused code in codebook: 7788
Validation loss of the network on the 10144 test EEG: 0.9053
Reset the codebook statistic info in quantizer before each epoch
Epoch: [69]  [ 0/19]  eta: 0:04:00  lr: 0.000060  min_lr: 0.000060  loss: 0.8550 (0.8550)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2420 (0.2420)  rec_angle_loss: 0.6090 (0.6090)  total_loss: 0.8550 (0.8550)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3722 (0.3722)  time: 12.6439  data: 11.0439  max mem: 13238
Epoch: [69]  [10/19]  eta: 0:00:16  lr: 0.000059  min_lr: 0.000059  loss: 0.8570 (0.8577)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2434 (0.2447)  rec_angle_loss: 0.6096 (0.6090)  total_loss: 0.8570 (0.8577)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4511 (0.4825)  time: 1.7798  data: 1.2200  max mem: 13238
Epoch: [69]  [18/19]  eta: 0:00:01  lr: 0.000058  min_lr: 0.000058  loss: 0.8570 (0.8568)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2433 (0.2440)  rec_angle_loss: 0.6091 (0.6088)  total_loss: 0.8570 (0.8568)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5719 (0.5806)  time: 1.1826  data: 0.7065  max mem: 13238
Epoch: [69] Total time: 0:00:23 (1.2556 s / it)
Averaged stats: lr: 0.000058  min_lr: 0.000058  loss: 0.8570 (0.8579)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2433 (0.2450)  rec_angle_loss: 0.6091 (0.6089)  total_loss: 0.8570 (0.8579)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5719 (0.5806)
Unused code in codebook: 7788
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 0.8836 (0.8836)  time: 5.2890  data: 5.2176  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9306 (0.9313)  time: 0.5593  data: 0.4976  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9169 (0.9060)  time: 0.4491  data: 0.3910  max mem: 13238
Validation: Total time: 0:00:07 (0.5405 s / it)
Averaged stats: loss: 0.9169 (0.9060)  quant_loss: 0.0041 (0.0042)  rec_loss: 0.1735 (0.1735)  rec_angle_loss: 0.5995 (0.5996)  total_loss: 0.7771 (0.7772)
Unused code in codebook: 7784
Validation loss of the network on the 10144 test EEG: 0.9060
Reset the codebook statistic info in quantizer before each epoch
Epoch: [70]  [ 0/19]  eta: 0:03:42  lr: 0.000057  min_lr: 0.000057  loss: 0.8665 (0.8665)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2507 (0.2507)  rec_angle_loss: 0.6117 (0.6117)  total_loss: 0.8665 (0.8665)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.9120 (0.9120)  time: 11.6881  data: 10.9219  max mem: 13238
Epoch: [70]  [10/19]  eta: 0:00:15  lr: 0.000056  min_lr: 0.000056  loss: 0.8558 (0.8567)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2446 (0.2441)  rec_angle_loss: 0.6091 (0.6086)  total_loss: 0.8558 (0.8567)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7031 (0.6944)  time: 1.7610  data: 1.3445  max mem: 13238
Epoch: [70]  [18/19]  eta: 0:00:01  lr: 0.000055  min_lr: 0.000055  loss: 0.8558 (0.8564)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2446 (0.2438)  rec_angle_loss: 0.6087 (0.6086)  total_loss: 0.8558 (0.8564)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5335 (0.6027)  time: 1.1713  data: 0.7784  max mem: 13238
Epoch: [70] Total time: 0:00:23 (1.2469 s / it)
Averaged stats: lr: 0.000055  min_lr: 0.000055  loss: 0.8558 (0.8568)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2446 (0.2440)  rec_angle_loss: 0.6087 (0.6087)  total_loss: 0.8558 (0.8568)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5335 (0.6027)
Unused code in codebook: 7785
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 0.8808 (0.8808)  time: 5.3222  data: 5.2484  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9291 (0.9281)  time: 0.5609  data: 0.4910  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9148 (0.9027)  time: 0.4502  data: 0.3858  max mem: 13238
Validation: Total time: 0:00:07 (0.5442 s / it)
Averaged stats: loss: 0.9148 (0.9022)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.1705 (0.1711)  rec_angle_loss: 0.5978 (0.5968)  total_loss: 0.7724 (0.7720)
Unused code in codebook: 7783
Validation loss of the network on the 10144 test EEG: 0.9022
Reset the codebook statistic info in quantizer before each epoch
Epoch: [71]  [ 0/19]  eta: 0:03:50  lr: 0.000055  min_lr: 0.000055  loss: 0.8643 (0.8643)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2486 (0.2486)  rec_angle_loss: 0.6117 (0.6117)  total_loss: 0.8643 (0.8643)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6561 (0.6561)  time: 12.1480  data: 11.0137  max mem: 13238
Epoch: [71]  [10/19]  eta: 0:00:16  lr: 0.000053  min_lr: 0.000053  loss: 0.8540 (0.8552)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2426 (0.2421)  rec_angle_loss: 0.6088 (0.6091)  total_loss: 0.8540 (0.8552)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6561 (0.6554)  time: 1.7979  data: 1.3057  max mem: 13238
Epoch: [71]  [18/19]  eta: 0:00:01  lr: 0.000052  min_lr: 0.000052  loss: 0.8557 (0.8558)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2430 (0.2431)  rec_angle_loss: 0.6088 (0.6088)  total_loss: 0.8557 (0.8558)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5602 (0.6083)  time: 1.1928  data: 0.7561  max mem: 13238
Epoch: [71] Total time: 0:00:24 (1.2686 s / it)
Averaged stats: lr: 0.000052  min_lr: 0.000052  loss: 0.8557 (0.8557)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2430 (0.2431)  rec_angle_loss: 0.6088 (0.6087)  total_loss: 0.8557 (0.8557)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5602 (0.6083)
Unused code in codebook: 7783
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:29  loss: 0.8803 (0.8803)  time: 6.3984  data: 5.8295  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9273 (0.9272)  time: 0.6633  data: 0.5456  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9157 (0.9017)  time: 0.5309  data: 0.4287  max mem: 13238
Validation: Total time: 0:00:08 (0.6222 s / it)
Averaged stats: loss: 0.9157 (0.9016)  quant_loss: 0.0041 (0.0042)  rec_loss: 0.1698 (0.1700)  rec_angle_loss: 0.5988 (0.5983)  total_loss: 0.7727 (0.7724)
Unused code in codebook: 7779
Validation loss of the network on the 10144 test EEG: 0.9016
Reset the codebook statistic info in quantizer before each epoch
Epoch: [72]  [ 0/19]  eta: 0:03:52  lr: 0.000052  min_lr: 0.000052  loss: 0.8544 (0.8544)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2426 (0.2426)  rec_angle_loss: 0.6079 (0.6079)  total_loss: 0.8544 (0.8544)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5007 (0.5007)  time: 12.2331  data: 11.2144  max mem: 13238
Epoch: [72]  [10/19]  eta: 0:00:16  lr: 0.000050  min_lr: 0.000050  loss: 0.8544 (0.8557)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2439 (0.2432)  rec_angle_loss: 0.6079 (0.6085)  total_loss: 0.8544 (0.8557)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3958 (0.4167)  time: 1.7804  data: 1.2985  max mem: 13238
Epoch: [72]  [18/19]  eta: 0:00:01  lr: 0.000049  min_lr: 0.000049  loss: 0.8534 (0.8547)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2421 (0.2422)  rec_angle_loss: 0.6079 (0.6086)  total_loss: 0.8534 (0.8547)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4183 (0.4173)  time: 1.1830  data: 0.7518  max mem: 13238
Epoch: [72] Total time: 0:00:23 (1.2583 s / it)
Averaged stats: lr: 0.000049  min_lr: 0.000049  loss: 0.8534 (0.8543)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2421 (0.2420)  rec_angle_loss: 0.6079 (0.6083)  total_loss: 0.8534 (0.8543)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4183 (0.4173)
Unused code in codebook: 7780
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8787 (0.8787)  time: 5.4521  data: 5.3832  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9258 (0.9268)  time: 0.5796  data: 0.5180  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9135 (0.9018)  time: 0.4651  data: 0.4070  max mem: 13238
Validation: Total time: 0:00:07 (0.5608 s / it)
Averaged stats: loss: 0.9135 (0.9014)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.1689 (0.1693)  rec_angle_loss: 0.6017 (0.5999)  total_loss: 0.7748 (0.7734)
Unused code in codebook: 7784
Validation loss of the network on the 10144 test EEG: 0.9014
Reset the codebook statistic info in quantizer before each epoch
Epoch: [73]  [ 0/19]  eta: 0:03:51  lr: 0.000049  min_lr: 0.000049  loss: 0.8609 (0.8609)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2463 (0.2463)  rec_angle_loss: 0.6106 (0.6106)  total_loss: 0.8609 (0.8609)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6353 (0.6353)  time: 12.1962  data: 10.9337  max mem: 13238
Epoch: [73]  [10/19]  eta: 0:00:15  lr: 0.000048  min_lr: 0.000048  loss: 0.8532 (0.8536)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2436 (0.2417)  rec_angle_loss: 0.6067 (0.6079)  total_loss: 0.8532 (0.8536)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6618 (0.6476)  time: 1.7755  data: 1.2720  max mem: 13238
Epoch: [73]  [18/19]  eta: 0:00:01  lr: 0.000047  min_lr: 0.000047  loss: 0.8538 (0.8545)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2436 (0.2419)  rec_angle_loss: 0.6091 (0.6086)  total_loss: 0.8538 (0.8545)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7152 (0.7606)  time: 1.1804  data: 0.7365  max mem: 13238
Epoch: [73] Total time: 0:00:23 (1.2554 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000047  loss: 0.8538 (0.8537)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2436 (0.2413)  rec_angle_loss: 0.6091 (0.6083)  total_loss: 0.8538 (0.8537)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7152 (0.7606)
Unused code in codebook: 7785
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8770 (0.8770)  time: 5.4560  data: 5.3797  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9258 (0.9259)  time: 0.5620  data: 0.4999  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9145 (0.9004)  time: 0.4519  data: 0.3928  max mem: 13238
Validation: Total time: 0:00:07 (0.5468 s / it)
Averaged stats: loss: 0.9145 (0.8999)  quant_loss: 0.0041 (0.0042)  rec_loss: 0.1688 (0.1691)  rec_angle_loss: 0.5979 (0.5955)  total_loss: 0.7709 (0.7688)
Unused code in codebook: 7780
Validation loss of the network on the 10144 test EEG: 0.8999
Reset the codebook statistic info in quantizer before each epoch
Epoch: [74]  [ 0/19]  eta: 0:03:49  lr: 0.000047  min_lr: 0.000047  loss: 0.8545 (0.8545)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2421 (0.2421)  rec_angle_loss: 0.6084 (0.6084)  total_loss: 0.8545 (0.8545)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7244 (0.7244)  time: 12.0579  data: 11.6749  max mem: 13238
Epoch: [74]  [10/19]  eta: 0:00:15  lr: 0.000045  min_lr: 0.000045  loss: 0.8544 (0.8527)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2421 (0.2412)  rec_angle_loss: 0.6074 (0.6075)  total_loss: 0.8544 (0.8527)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6148 (0.6101)  time: 1.7581  data: 1.3870  max mem: 13238
Epoch: [74]  [18/19]  eta: 0:00:01  lr: 0.000044  min_lr: 0.000044  loss: 0.8517 (0.8518)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2418 (0.2407)  rec_angle_loss: 0.6073 (0.6072)  total_loss: 0.8517 (0.8518)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4501 (0.5304)  time: 1.1689  data: 0.8030  max mem: 13238
Epoch: [74] Total time: 0:00:23 (1.2452 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000044  loss: 0.8517 (0.8530)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2418 (0.2407)  rec_angle_loss: 0.6073 (0.6083)  total_loss: 0.8517 (0.8530)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4501 (0.5304)
Unused code in codebook: 7781
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.8747 (0.8747)  time: 5.5044  data: 5.4313  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9253 (0.9240)  time: 0.5825  data: 0.5176  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9108 (0.8983)  time: 0.4673  data: 0.4067  max mem: 13238
Validation: Total time: 0:00:07 (0.5605 s / it)
Averaged stats: loss: 0.9108 (0.8979)  quant_loss: 0.0042 (0.0043)  rec_loss: 0.1671 (0.1674)  rec_angle_loss: 0.5966 (0.5962)  total_loss: 0.7680 (0.7678)
Unused code in codebook: 7776
Validation loss of the network on the 10144 test EEG: 0.8979
Reset the codebook statistic info in quantizer before each epoch
Epoch: [75]  [ 0/19]  eta: 0:03:50  lr: 0.000044  min_lr: 0.000044  loss: 0.8554 (0.8554)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2453 (0.2453)  rec_angle_loss: 0.6061 (0.6061)  total_loss: 0.8554 (0.8554)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3452 (0.3452)  time: 12.1227  data: 11.1476  max mem: 13238
Epoch: [75]  [10/19]  eta: 0:00:15  lr: 0.000043  min_lr: 0.000043  loss: 0.8530 (0.8524)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2401 (0.2399)  rec_angle_loss: 0.6085 (0.6085)  total_loss: 0.8530 (0.8524)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3710 (0.3850)  time: 1.7694  data: 1.2892  max mem: 13238
Epoch: [75]  [18/19]  eta: 0:00:01  lr: 0.000042  min_lr: 0.000042  loss: 0.8530 (0.8524)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2403 (0.2403)  rec_angle_loss: 0.6076 (0.6081)  total_loss: 0.8530 (0.8524)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3551 (0.3683)  time: 1.1765  data: 0.7464  max mem: 13238
Epoch: [75] Total time: 0:00:23 (1.2530 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000042  loss: 0.8530 (0.8517)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2403 (0.2399)  rec_angle_loss: 0.6076 (0.6078)  total_loss: 0.8530 (0.8517)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3551 (0.3683)
Unused code in codebook: 7785
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8752 (0.8752)  time: 5.4787  data: 4.8655  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9248 (0.9235)  time: 0.5794  data: 0.4671  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9093 (0.8980)  time: 0.4653  data: 0.3670  max mem: 13238
Validation: Total time: 0:00:07 (0.5561 s / it)
Averaged stats: loss: 0.9093 (0.8977)  quant_loss: 0.0043 (0.0044)  rec_loss: 0.1664 (0.1666)  rec_angle_loss: 0.5980 (0.5963)  total_loss: 0.7687 (0.7673)
Unused code in codebook: 7773
Validation loss of the network on the 10144 test EEG: 0.8977
Reset the codebook statistic info in quantizer before each epoch
Epoch: [76]  [ 0/19]  eta: 0:04:15  lr: 0.000041  min_lr: 0.000041  loss: 0.8525 (0.8525)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2420 (0.2420)  rec_angle_loss: 0.6065 (0.6065)  total_loss: 0.8525 (0.8525)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3528 (0.3528)  time: 13.4700  data: 13.0891  max mem: 13238
Epoch: [76]  [10/19]  eta: 0:00:17  lr: 0.000040  min_lr: 0.000040  loss: 0.8525 (0.8531)  quant_loss: 0.0040 (0.0041)  rec_loss: 0.2407 (0.2404)  rec_angle_loss: 0.6090 (0.6086)  total_loss: 0.8525 (0.8531)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4041 (0.4263)  time: 1.8984  data: 1.5235  max mem: 13238
Epoch: [76]  [18/19]  eta: 0:00:01  lr: 0.000039  min_lr: 0.000039  loss: 0.8525 (0.8529)  quant_loss: 0.0041 (0.0041)  rec_loss: 0.2406 (0.2404)  rec_angle_loss: 0.6089 (0.6084)  total_loss: 0.8525 (0.8529)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4035 (0.4083)  time: 1.2509  data: 0.8821  max mem: 13238
Epoch: [76] Total time: 0:00:25 (1.3254 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000039  loss: 0.8525 (0.8512)  quant_loss: 0.0041 (0.0041)  rec_loss: 0.2406 (0.2394)  rec_angle_loss: 0.6089 (0.6077)  total_loss: 0.8525 (0.8512)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4035 (0.4083)
Unused code in codebook: 7783
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8741 (0.8741)  time: 5.4534  data: 5.3767  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9232 (0.9225)  time: 0.5727  data: 0.5109  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9086 (0.8972)  time: 0.4595  data: 0.4014  max mem: 13238
Validation: Total time: 0:00:07 (0.5538 s / it)
Averaged stats: loss: 0.9086 (0.8968)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.1662 (0.1663)  rec_angle_loss: 0.5982 (0.5959)  total_loss: 0.7687 (0.7665)
Unused code in codebook: 7770
Validation loss of the network on the 10144 test EEG: 0.8968
Reset the codebook statistic info in quantizer before each epoch
Epoch: [77]  [ 0/19]  eta: 0:03:46  lr: 0.000039  min_lr: 0.000039  loss: 0.8559 (0.8559)  quant_loss: 0.0040 (0.0040)  rec_loss: 0.2419 (0.2419)  rec_angle_loss: 0.6100 (0.6100)  total_loss: 0.8559 (0.8559)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3720 (0.3720)  time: 11.9439  data: 10.5897  max mem: 13238
Epoch: [77]  [10/19]  eta: 0:00:16  lr: 0.000038  min_lr: 0.000038  loss: 0.8492 (0.8500)  quant_loss: 0.0041 (0.0041)  rec_loss: 0.2374 (0.2376)  rec_angle_loss: 0.6100 (0.6082)  total_loss: 0.8492 (0.8500)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4530 (0.4967)  time: 1.7870  data: 1.2199  max mem: 13238
Epoch: [77]  [18/19]  eta: 0:00:01  lr: 0.000037  min_lr: 0.000037  loss: 0.8491 (0.8503)  quant_loss: 0.0041 (0.0041)  rec_loss: 0.2372 (0.2377)  rec_angle_loss: 0.6085 (0.6085)  total_loss: 0.8491 (0.8503)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5453 (0.6324)  time: 1.1864  data: 0.7064  max mem: 13238
Epoch: [77] Total time: 0:00:23 (1.2625 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000037  loss: 0.8491 (0.8512)  quant_loss: 0.0041 (0.0041)  rec_loss: 0.2372 (0.2390)  rec_angle_loss: 0.6085 (0.6080)  total_loss: 0.8491 (0.8512)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5453 (0.6324)
Unused code in codebook: 7773
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 0.8730 (0.8730)  time: 5.2931  data: 4.8934  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9243 (0.9232)  time: 0.5806  data: 0.4815  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9096 (0.8976)  time: 0.4659  data: 0.3783  max mem: 13238
Validation: Total time: 0:00:07 (0.5648 s / it)
Averaged stats: loss: 0.9096 (0.8974)  quant_loss: 0.0043 (0.0044)  rec_loss: 0.1655 (0.1654)  rec_angle_loss: 0.5950 (0.5965)  total_loss: 0.7649 (0.7663)
Unused code in codebook: 7771
Validation loss of the network on the 10144 test EEG: 0.8974
Reset the codebook statistic info in quantizer before each epoch
Epoch: [78]  [ 0/19]  eta: 0:03:43  lr: 0.000037  min_lr: 0.000037  loss: 0.8477 (0.8477)  quant_loss: 0.0041 (0.0041)  rec_loss: 0.2399 (0.2399)  rec_angle_loss: 0.6037 (0.6037)  total_loss: 0.8477 (0.8477)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5630 (0.5630)  time: 11.7376  data: 10.9097  max mem: 13238
Epoch: [78]  [10/19]  eta: 0:00:15  lr: 0.000035  min_lr: 0.000035  loss: 0.8468 (0.8468)  quant_loss: 0.0041 (0.0041)  rec_loss: 0.2361 (0.2367)  rec_angle_loss: 0.6059 (0.6059)  total_loss: 0.8468 (0.8468)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7811 (0.7201)  time: 1.7731  data: 1.3061  max mem: 13238
Epoch: [78]  [18/19]  eta: 0:00:01  lr: 0.000035  min_lr: 0.000035  loss: 0.8477 (0.8492)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2369 (0.2375)  rec_angle_loss: 0.6068 (0.6075)  total_loss: 0.8477 (0.8492)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7811 (0.7119)  time: 1.1788  data: 0.7563  max mem: 13238
Epoch: [78] Total time: 0:00:23 (1.2530 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000035  loss: 0.8477 (0.8510)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2369 (0.2388)  rec_angle_loss: 0.6068 (0.6081)  total_loss: 0.8477 (0.8510)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7811 (0.7119)
Unused code in codebook: 7776
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.8759 (0.8759)  time: 5.5047  data: 4.8967  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9243 (0.9240)  time: 0.5832  data: 0.4729  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9085 (0.8986)  time: 0.4684  data: 0.3716  max mem: 13238
Validation: Total time: 0:00:07 (0.5601 s / it)
Averaged stats: loss: 0.9085 (0.8987)  quant_loss: 0.0044 (0.0044)  rec_loss: 0.1665 (0.1661)  rec_angle_loss: 0.5966 (0.5976)  total_loss: 0.7675 (0.7681)
Unused code in codebook: 7769
Validation loss of the network on the 10144 test EEG: 0.8987
Reset the codebook statistic info in quantizer before each epoch
Epoch: [79]  [ 0/19]  eta: 0:03:45  lr: 0.000034  min_lr: 0.000034  loss: 0.8574 (0.8574)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2445 (0.2445)  rec_angle_loss: 0.6087 (0.6087)  total_loss: 0.8574 (0.8574)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.9220 (0.9220)  time: 11.8858  data: 11.2700  max mem: 13238
Epoch: [79]  [10/19]  eta: 0:00:16  lr: 0.000033  min_lr: 0.000033  loss: 0.8529 (0.8516)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2387 (0.2392)  rec_angle_loss: 0.6087 (0.6082)  total_loss: 0.8529 (0.8516)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5695 (0.6238)  time: 1.7952  data: 1.3855  max mem: 13238
Epoch: [79]  [18/19]  eta: 0:00:01  lr: 0.000032  min_lr: 0.000032  loss: 0.8505 (0.8500)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2374 (0.2377)  rec_angle_loss: 0.6087 (0.6081)  total_loss: 0.8505 (0.8500)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5123 (0.5505)  time: 1.1914  data: 0.8023  max mem: 13238
Epoch: [79] Total time: 0:00:24 (1.2691 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000032  loss: 0.8505 (0.8498)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2374 (0.2381)  rec_angle_loss: 0.6087 (0.6075)  total_loss: 0.8505 (0.8498)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5123 (0.5505)
Unused code in codebook: 7773
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.8717 (0.8717)  time: 5.3788  data: 5.3068  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9230 (0.9216)  time: 0.5689  data: 0.5056  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9067 (0.8958)  time: 0.4566  data: 0.3972  max mem: 13238
Validation: Total time: 0:00:07 (0.5541 s / it)
Averaged stats: loss: 0.9067 (0.8956)  quant_loss: 0.0044 (0.0045)  rec_loss: 0.1650 (0.1643)  rec_angle_loss: 0.5942 (0.5947)  total_loss: 0.7637 (0.7634)
Unused code in codebook: 7774
Validation loss of the network on the 10144 test EEG: 0.8956
Reset the codebook statistic info in quantizer before each epoch
Epoch: [80]  [ 0/19]  eta: 0:03:58  lr: 0.000032  min_lr: 0.000032  loss: 0.8448 (0.8448)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2356 (0.2356)  rec_angle_loss: 0.6049 (0.6049)  total_loss: 0.8448 (0.8448)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3873 (0.3873)  time: 12.5418  data: 10.8105  max mem: 13238
Epoch: [80]  [10/19]  eta: 0:00:17  lr: 0.000031  min_lr: 0.000031  loss: 0.8448 (0.8461)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2356 (0.2355)  rec_angle_loss: 0.6071 (0.6063)  total_loss: 0.8448 (0.8461)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3853 (0.3786)  time: 1.9525  data: 1.4108  max mem: 13238
Epoch: [80]  [18/19]  eta: 0:00:01  lr: 0.000030  min_lr: 0.000030  loss: 0.8471 (0.8476)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2363 (0.2364)  rec_angle_loss: 0.6075 (0.6070)  total_loss: 0.8471 (0.8476)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3394 (0.3506)  time: 1.2829  data: 0.8168  max mem: 13238
Epoch: [80] Total time: 0:00:25 (1.3586 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000030  loss: 0.8471 (0.8494)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2363 (0.2379)  rec_angle_loss: 0.6075 (0.6073)  total_loss: 0.8471 (0.8494)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3394 (0.3506)
Unused code in codebook: 7773
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 0.8710 (0.8710)  time: 5.3113  data: 5.2408  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9223 (0.9209)  time: 0.5597  data: 0.4946  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9071 (0.8954)  time: 0.4492  data: 0.3886  max mem: 13238
Validation: Total time: 0:00:07 (0.5431 s / it)
Averaged stats: loss: 0.9071 (0.8952)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1642 (0.1636)  rec_angle_loss: 0.5940 (0.5936)  total_loss: 0.7627 (0.7617)
Unused code in codebook: 7778
Validation loss of the network on the 10144 test EEG: 0.8952
Reset the codebook statistic info in quantizer before each epoch
Epoch: [81]  [ 0/19]  eta: 0:03:47  lr: 0.000030  min_lr: 0.000030  loss: 0.8472 (0.8472)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2354 (0.2354)  rec_angle_loss: 0.6076 (0.6076)  total_loss: 0.8472 (0.8472)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3827 (0.3827)  time: 11.9768  data: 11.0808  max mem: 13238
Epoch: [81]  [10/19]  eta: 0:00:16  lr: 0.000029  min_lr: 0.000029  loss: 0.8506 (0.8503)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2393 (0.2385)  rec_angle_loss: 0.6076 (0.6075)  total_loss: 0.8506 (0.8503)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3038 (0.3106)  time: 1.7966  data: 1.3437  max mem: 13238
Epoch: [81]  [18/19]  eta: 0:00:01  lr: 0.000028  min_lr: 0.000028  loss: 0.8495 (0.8490)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2376 (0.2377)  rec_angle_loss: 0.6074 (0.6070)  total_loss: 0.8495 (0.8490)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3038 (0.3180)  time: 1.1923  data: 0.7780  max mem: 13238
Epoch: [81] Total time: 0:00:24 (1.2684 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000028  loss: 0.8495 (0.8495)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2376 (0.2377)  rec_angle_loss: 0.6074 (0.6075)  total_loss: 0.8495 (0.8495)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3038 (0.3180)
Unused code in codebook: 7777
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:17  loss: 0.8711 (0.8711)  time: 5.5464  data: 5.4721  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9225 (0.9204)  time: 0.5612  data: 0.4988  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9055 (0.8945)  time: 0.4506  data: 0.3920  max mem: 13238
Validation: Total time: 0:00:07 (0.5487 s / it)
Averaged stats: loss: 0.9055 (0.8945)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1644 (0.1637)  rec_angle_loss: 0.5919 (0.5940)  total_loss: 0.7608 (0.7622)
Unused code in codebook: 7778
Validation loss of the network on the 10144 test EEG: 0.8945
Reset the codebook statistic info in quantizer before each epoch
Epoch: [82]  [ 0/19]  eta: 0:03:49  lr: 0.000028  min_lr: 0.000028  loss: 0.8413 (0.8413)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2335 (0.2335)  rec_angle_loss: 0.6035 (0.6035)  total_loss: 0.8413 (0.8413)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3303 (0.3303)  time: 12.0795  data: 10.7479  max mem: 13238
Epoch: [82]  [10/19]  eta: 0:00:16  lr: 0.000027  min_lr: 0.000027  loss: 0.8448 (0.8477)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2335 (0.2366)  rec_angle_loss: 0.6070 (0.6068)  total_loss: 0.8448 (0.8477)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3624 (0.3559)  time: 1.7912  data: 1.2249  max mem: 13238
Epoch: [82]  [18/19]  eta: 0:00:01  lr: 0.000026  min_lr: 0.000026  loss: 0.8458 (0.8470)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2341 (0.2366)  rec_angle_loss: 0.6058 (0.6062)  total_loss: 0.8458 (0.8470)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3539 (0.3414)  time: 1.1892  data: 0.7093  max mem: 13238
Epoch: [82] Total time: 0:00:23 (1.2629 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000026  loss: 0.8458 (0.8490)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2341 (0.2374)  rec_angle_loss: 0.6058 (0.6074)  total_loss: 0.8458 (0.8490)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3539 (0.3414)
Unused code in codebook: 7778
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.8711 (0.8711)  time: 5.3742  data: 5.3005  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9223 (0.9208)  time: 0.5614  data: 0.4993  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9056 (0.8952)  time: 0.4506  data: 0.3924  max mem: 13238
Validation: Total time: 0:00:07 (0.5438 s / it)
Averaged stats: loss: 0.9056 (0.8951)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1644 (0.1636)  rec_angle_loss: 0.5934 (0.5953)  total_loss: 0.7623 (0.7634)
Unused code in codebook: 7782
Validation loss of the network on the 10144 test EEG: 0.8951
Reset the codebook statistic info in quantizer before each epoch
Epoch: [83]  [ 0/19]  eta: 0:03:54  lr: 0.000026  min_lr: 0.000026  loss: 0.8539 (0.8539)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2404 (0.2404)  rec_angle_loss: 0.6092 (0.6092)  total_loss: 0.8539 (0.8539)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2815 (0.2815)  time: 12.3428  data: 11.9603  max mem: 13238
Epoch: [83]  [10/19]  eta: 0:00:15  lr: 0.000025  min_lr: 0.000025  loss: 0.8476 (0.8491)  quant_loss: 0.0042 (0.0043)  rec_loss: 0.2362 (0.2375)  rec_angle_loss: 0.6077 (0.6073)  total_loss: 0.8476 (0.8491)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4152 (0.4215)  time: 1.7549  data: 1.3902  max mem: 13238
Epoch: [83]  [18/19]  eta: 0:00:01  lr: 0.000025  min_lr: 0.000025  loss: 0.8476 (0.8480)  quant_loss: 0.0042 (0.0043)  rec_loss: 0.2362 (0.2367)  rec_angle_loss: 0.6076 (0.6070)  total_loss: 0.8476 (0.8480)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3986 (0.4201)  time: 1.1678  data: 0.8050  max mem: 13238
Epoch: [83] Total time: 0:00:23 (1.2435 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000025  loss: 0.8476 (0.8486)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2362 (0.2371)  rec_angle_loss: 0.6076 (0.6072)  total_loss: 0.8476 (0.8486)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3986 (0.4201)
Unused code in codebook: 7778
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:22  loss: 0.8709 (0.8709)  time: 5.8762  data: 5.8010  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9226 (0.9211)  time: 0.6062  data: 0.5439  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9061 (0.8956)  time: 0.4858  data: 0.4274  max mem: 13238
Validation: Total time: 0:00:08 (0.5821 s / it)
Averaged stats: loss: 0.9061 (0.8953)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1640 (0.1635)  rec_angle_loss: 0.5949 (0.5962)  total_loss: 0.7634 (0.7642)
Unused code in codebook: 7781
Validation loss of the network on the 10144 test EEG: 0.8953
Reset the codebook statistic info in quantizer before each epoch
Epoch: [84]  [ 0/19]  eta: 0:03:48  lr: 0.000024  min_lr: 0.000024  loss: 0.8459 (0.8459)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2357 (0.2357)  rec_angle_loss: 0.6060 (0.6060)  total_loss: 0.8459 (0.8459)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7540 (0.7540)  time: 12.0225  data: 11.4963  max mem: 13238
Epoch: [84]  [10/19]  eta: 0:00:15  lr: 0.000024  min_lr: 0.000024  loss: 0.8459 (0.8464)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2356 (0.2355)  rec_angle_loss: 0.6060 (0.6067)  total_loss: 0.8459 (0.8464)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5254 (0.5394)  time: 1.7778  data: 1.3296  max mem: 13238
Epoch: [84]  [18/19]  eta: 0:00:01  lr: 0.000023  min_lr: 0.000023  loss: 0.8491 (0.8480)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2358 (0.2364)  rec_angle_loss: 0.6064 (0.6074)  total_loss: 0.8491 (0.8480)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5254 (0.5086)  time: 1.1806  data: 0.7698  max mem: 13238
Epoch: [84] Total time: 0:00:23 (1.2543 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000023  loss: 0.8491 (0.8484)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2358 (0.2369)  rec_angle_loss: 0.6064 (0.6073)  total_loss: 0.8491 (0.8484)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5254 (0.5086)
Unused code in codebook: 7779
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:28  loss: 0.8713 (0.8713)  time: 6.2884  data: 6.2119  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9223 (0.9204)  time: 0.6524  data: 0.5900  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9059 (0.8949)  time: 0.5222  data: 0.4636  max mem: 13238
Validation: Total time: 0:00:08 (0.6198 s / it)
Averaged stats: loss: 0.9059 (0.8948)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1638 (0.1634)  rec_angle_loss: 0.5947 (0.5976)  total_loss: 0.7630 (0.7654)
Unused code in codebook: 7782
Validation loss of the network on the 10144 test EEG: 0.8948
Reset the codebook statistic info in quantizer before each epoch
Epoch: [85]  [ 0/19]  eta: 0:03:50  lr: 0.000023  min_lr: 0.000023  loss: 0.8508 (0.8508)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2381 (0.2381)  rec_angle_loss: 0.6084 (0.6084)  total_loss: 0.8508 (0.8508)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.7557 (0.7557)  time: 12.1057  data: 11.4234  max mem: 13238
Epoch: [85]  [10/19]  eta: 0:00:16  lr: 0.000022  min_lr: 0.000022  loss: 0.8504 (0.8481)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2365 (0.2370)  rec_angle_loss: 0.6071 (0.6068)  total_loss: 0.8504 (0.8481)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.6127 (0.5306)  time: 1.7821  data: 1.3780  max mem: 13238
Epoch: [85]  [18/19]  eta: 0:00:01  lr: 0.000021  min_lr: 0.000021  loss: 0.8481 (0.8478)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2379 (0.2366)  rec_angle_loss: 0.6057 (0.6069)  total_loss: 0.8481 (0.8478)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5202 (0.4754)  time: 1.1828  data: 0.7979  max mem: 13238
Epoch: [85] Total time: 0:00:23 (1.2575 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000021  loss: 0.8481 (0.8480)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2379 (0.2365)  rec_angle_loss: 0.6057 (0.6073)  total_loss: 0.8481 (0.8480)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.5202 (0.4754)
Unused code in codebook: 7781
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:18  loss: 0.8681 (0.8681)  time: 5.5738  data: 5.4975  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9216 (0.9197)  time: 0.5751  data: 0.5105  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9059 (0.8939)  time: 0.4614  data: 0.4011  max mem: 13238
Validation: Total time: 0:00:07 (0.5571 s / it)
Averaged stats: loss: 0.9059 (0.8938)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1636 (0.1629)  rec_angle_loss: 0.5908 (0.5938)  total_loss: 0.7589 (0.7612)
Unused code in codebook: 7783
Validation loss of the network on the 10144 test EEG: 0.8938
Reset the codebook statistic info in quantizer before each epoch
Epoch: [86]  [ 0/19]  eta: 0:03:51  lr: 0.000021  min_lr: 0.000021  loss: 0.8470 (0.8470)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2374 (0.2374)  rec_angle_loss: 0.6053 (0.6053)  total_loss: 0.8470 (0.8470)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2260 (0.2260)  time: 12.1644  data: 11.2836  max mem: 13238
Epoch: [86]  [10/19]  eta: 0:00:16  lr: 0.000020  min_lr: 0.000020  loss: 0.8443 (0.8461)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2349 (0.2358)  rec_angle_loss: 0.6059 (0.6061)  total_loss: 0.8443 (0.8461)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4614 (0.4581)  time: 1.8007  data: 1.3501  max mem: 13238
Epoch: [86]  [18/19]  eta: 0:00:01  lr: 0.000020  min_lr: 0.000020  loss: 0.8454 (0.8469)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2367 (0.2360)  rec_angle_loss: 0.6061 (0.6066)  total_loss: 0.8454 (0.8469)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4044 (0.3866)  time: 1.1948  data: 0.7817  max mem: 13238
Epoch: [86] Total time: 0:00:24 (1.2713 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000020  loss: 0.8454 (0.8475)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2367 (0.2363)  rec_angle_loss: 0.6061 (0.6070)  total_loss: 0.8454 (0.8475)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4044 (0.3866)
Unused code in codebook: 7778
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:19  loss: 0.8688 (0.8688)  time: 5.6527  data: 5.5761  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9214 (0.9198)  time: 0.5960  data: 0.5319  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9046 (0.8941)  time: 0.4778  data: 0.4179  max mem: 13238
Validation: Total time: 0:00:08 (0.5750 s / it)
Averaged stats: loss: 0.9046 (0.8941)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1636 (0.1630)  rec_angle_loss: 0.5940 (0.5963)  total_loss: 0.7620 (0.7637)
Unused code in codebook: 7782
Validation loss of the network on the 10144 test EEG: 0.8941
Reset the codebook statistic info in quantizer before each epoch
Epoch: [87]  [ 0/19]  eta: 0:03:47  lr: 0.000020  min_lr: 0.000020  loss: 0.8521 (0.8521)  quant_loss: 0.0043 (0.0043)  rec_loss: 0.2440 (0.2440)  rec_angle_loss: 0.6039 (0.6039)  total_loss: 0.8521 (0.8521)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3529 (0.3529)  time: 11.9726  data: 10.9504  max mem: 13238
Epoch: [87]  [10/19]  eta: 0:00:16  lr: 0.000019  min_lr: 0.000019  loss: 0.8471 (0.8475)  quant_loss: 0.0043 (0.0042)  rec_loss: 0.2363 (0.2368)  rec_angle_loss: 0.6069 (0.6065)  total_loss: 0.8471 (0.8475)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3529 (0.3454)  time: 1.8379  data: 1.3941  max mem: 13238
Epoch: [87]  [18/19]  eta: 0:00:01  lr: 0.000018  min_lr: 0.000018  loss: 0.8469 (0.8470)  quant_loss: 0.0043 (0.0042)  rec_loss: 0.2360 (0.2361)  rec_angle_loss: 0.6073 (0.6066)  total_loss: 0.8469 (0.8470)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3529 (0.3589)  time: 1.2160  data: 0.8071  max mem: 13238
Epoch: [87] Total time: 0:00:24 (1.2916 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000018  loss: 0.8469 (0.8475)  quant_loss: 0.0043 (0.0042)  rec_loss: 0.2360 (0.2362)  rec_angle_loss: 0.6073 (0.6071)  total_loss: 0.8469 (0.8475)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3529 (0.3589)
Unused code in codebook: 7780
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8668 (0.8668)  time: 5.4417  data: 5.3715  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9203 (0.9183)  time: 0.5723  data: 0.5090  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9037 (0.8922)  time: 0.4595  data: 0.3999  max mem: 13238
Validation: Total time: 0:00:07 (0.5519 s / it)
Averaged stats: loss: 0.9037 (0.8922)  quant_loss: 0.0044 (0.0045)  rec_loss: 0.1632 (0.1626)  rec_angle_loss: 0.5909 (0.5928)  total_loss: 0.7585 (0.7599)
Unused code in codebook: 7781
Validation loss of the network on the 10144 test EEG: 0.8922
Reset the codebook statistic info in quantizer before each epoch
Epoch: [88]  [ 0/19]  eta: 0:03:43  lr: 0.000018  min_lr: 0.000018  loss: 0.8471 (0.8471)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2341 (0.2341)  rec_angle_loss: 0.6087 (0.6087)  total_loss: 0.8471 (0.8471)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2594 (0.2594)  time: 11.7880  data: 10.7990  max mem: 13238
Epoch: [88]  [10/19]  eta: 0:00:16  lr: 0.000018  min_lr: 0.000018  loss: 0.8475 (0.8478)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2357 (0.2363)  rec_angle_loss: 0.6083 (0.6073)  total_loss: 0.8475 (0.8478)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3251 (0.3235)  time: 1.7885  data: 1.2927  max mem: 13238
Epoch: [88]  [18/19]  eta: 0:00:01  lr: 0.000017  min_lr: 0.000017  loss: 0.8471 (0.8469)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2351 (0.2354)  rec_angle_loss: 0.6071 (0.6072)  total_loss: 0.8471 (0.8469)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3660 (0.3596)  time: 1.1871  data: 0.7485  max mem: 13238
Epoch: [88] Total time: 0:00:23 (1.2595 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000017  loss: 0.8471 (0.8472)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2351 (0.2360)  rec_angle_loss: 0.6071 (0.6070)  total_loss: 0.8471 (0.8472)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3660 (0.3596)
Unused code in codebook: 7779
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.8670 (0.8670)  time: 5.4152  data: 5.1987  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9208 (0.9189)  time: 0.5848  data: 0.5099  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9050 (0.8931)  time: 0.4691  data: 0.4007  max mem: 13238
Validation: Total time: 0:00:07 (0.5630 s / it)
Averaged stats: loss: 0.9050 (0.8929)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1632 (0.1626)  rec_angle_loss: 0.5921 (0.5951)  total_loss: 0.7597 (0.7621)
Unused code in codebook: 7781
Validation loss of the network on the 10144 test EEG: 0.8929
Reset the codebook statistic info in quantizer before each epoch
Epoch: [89]  [ 0/19]  eta: 0:04:34  lr: 0.000017  min_lr: 0.000017  loss: 0.8382 (0.8382)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2313 (0.2313)  rec_angle_loss: 0.6027 (0.6027)  total_loss: 0.8382 (0.8382)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2901 (0.2901)  time: 14.4658  data: 13.1802  max mem: 13238
Epoch: [89]  [10/19]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000016  loss: 0.8432 (0.8438)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2332 (0.2340)  rec_angle_loss: 0.6055 (0.6056)  total_loss: 0.8432 (0.8438)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2901 (0.2892)  time: 1.9323  data: 1.4753  max mem: 13238
Epoch: [89]  [18/19]  eta: 0:00:01  lr: 0.000016  min_lr: 0.000016  loss: 0.8459 (0.8453)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2354 (0.2349)  rec_angle_loss: 0.6062 (0.6062)  total_loss: 0.8459 (0.8453)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2520 (0.2669)  time: 1.2702  data: 0.8542  max mem: 13238
Epoch: [89] Total time: 0:00:25 (1.3440 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000016  loss: 0.8459 (0.8470)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2354 (0.2358)  rec_angle_loss: 0.6062 (0.6070)  total_loss: 0.8459 (0.8470)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2520 (0.2669)
Unused code in codebook: 7780
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:20  loss: 0.8663 (0.8663)  time: 5.7686  data: 4.9573  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9200 (0.9179)  time: 0.5796  data: 0.4507  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9040 (0.8920)  time: 0.4652  data: 0.3542  max mem: 13238
Validation: Total time: 0:00:07 (0.5654 s / it)
Averaged stats: loss: 0.9040 (0.8918)  quant_loss: 0.0045 (0.0045)  rec_loss: 0.1628 (0.1624)  rec_angle_loss: 0.5910 (0.5930)  total_loss: 0.7583 (0.7599)
Unused code in codebook: 7780
Validation loss of the network on the 10144 test EEG: 0.8918
Reset the codebook statistic info in quantizer before each epoch
Epoch: [90]  [ 0/19]  eta: 0:03:47  lr: 0.000016  min_lr: 0.000016  loss: 0.8444 (0.8444)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2358 (0.2358)  rec_angle_loss: 0.6043 (0.6043)  total_loss: 0.8444 (0.8444)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3447 (0.3447)  time: 11.9671  data: 10.7949  max mem: 13238
Epoch: [90]  [10/19]  eta: 0:00:15  lr: 0.000015  min_lr: 0.000015  loss: 0.8468 (0.8476)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2361 (0.2362)  rec_angle_loss: 0.6069 (0.6072)  total_loss: 0.8468 (0.8476)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3078 (0.2798)  time: 1.7618  data: 1.2618  max mem: 13238
Epoch: [90]  [18/19]  eta: 0:00:01  lr: 0.000015  min_lr: 0.000015  loss: 0.8464 (0.8465)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2358 (0.2356)  rec_angle_loss: 0.6065 (0.6066)  total_loss: 0.8464 (0.8465)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2701 (0.2671)  time: 1.1713  data: 0.7306  max mem: 13238
Epoch: [90] Total time: 0:00:23 (1.2463 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000015  loss: 0.8464 (0.8463)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2358 (0.2354)  rec_angle_loss: 0.6065 (0.6067)  total_loss: 0.8464 (0.8463)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2701 (0.2671)
Unused code in codebook: 7778
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:15  loss: 0.8665 (0.8665)  time: 5.4235  data: 4.9038  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9199 (0.9176)  time: 0.5778  data: 0.4713  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9036 (0.8919)  time: 0.4636  data: 0.3704  max mem: 13238
Validation: Total time: 0:00:07 (0.5592 s / it)
Averaged stats: loss: 0.9036 (0.8918)  quant_loss: 0.0044 (0.0045)  rec_loss: 0.1629 (0.1623)  rec_angle_loss: 0.5948 (0.5949)  total_loss: 0.7622 (0.7617)
Unused code in codebook: 7779
Validation loss of the network on the 10144 test EEG: 0.8918
Reset the codebook statistic info in quantizer before each epoch
Epoch: [91]  [ 0/19]  eta: 0:03:48  lr: 0.000015  min_lr: 0.000015  loss: 0.8634 (0.8634)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2471 (0.2471)  rec_angle_loss: 0.6121 (0.6121)  total_loss: 0.8634 (0.8634)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2341 (0.2341)  time: 12.0234  data: 10.8543  max mem: 13238
Epoch: [91]  [10/19]  eta: 0:00:16  lr: 0.000014  min_lr: 0.000014  loss: 0.8484 (0.8473)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2349 (0.2355)  rec_angle_loss: 0.6079 (0.6076)  total_loss: 0.8484 (0.8473)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.2553 (0.2963)  time: 1.7941  data: 1.2690  max mem: 13238
Epoch: [91]  [18/19]  eta: 0:00:01  lr: 0.000014  min_lr: 0.000014  loss: 0.8445 (0.8453)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2344 (0.2343)  rec_angle_loss: 0.6065 (0.6067)  total_loss: 0.8445 (0.8453)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3297 (0.3275)  time: 1.1901  data: 0.7348  max mem: 13238
Epoch: [91] Total time: 0:00:24 (1.2654 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000014  loss: 0.8445 (0.8464)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2344 (0.2354)  rec_angle_loss: 0.6065 (0.6068)  total_loss: 0.8445 (0.8464)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3297 (0.3275)
Unused code in codebook: 7780
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:16  loss: 0.8664 (0.8664)  time: 5.4863  data: 5.3578  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9204 (0.9180)  time: 0.5616  data: 0.4946  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9036 (0.8921)  time: 0.4510  data: 0.3886  max mem: 13238
Validation: Total time: 0:00:07 (0.5440 s / it)
Averaged stats: loss: 0.9036 (0.8921)  quant_loss: 0.0044 (0.0045)  rec_loss: 0.1624 (0.1621)  rec_angle_loss: 0.5932 (0.5945)  total_loss: 0.7601 (0.7611)
Unused code in codebook: 7780
Validation loss of the network on the 10144 test EEG: 0.8921
Reset the codebook statistic info in quantizer before each epoch
Epoch: [92]  [ 0/19]  eta: 0:03:45  lr: 0.000014  min_lr: 0.000014  loss: 0.8394 (0.8394)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2300 (0.2300)  rec_angle_loss: 0.6052 (0.6052)  total_loss: 0.8394 (0.8394)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.3377 (0.3377)  time: 11.8496  data: 10.7270  max mem: 13238
Epoch: [92]  [10/19]  eta: 0:00:17  lr: 0.000013  min_lr: 0.000013  loss: 0.8460 (0.8456)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2345 (0.2346)  rec_angle_loss: 0.6061 (0.6068)  total_loss: 0.8460 (0.8456)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4074 (0.4153)  time: 1.9550  data: 1.4805  max mem: 13238
Epoch: [92]  [18/19]  eta: 0:00:01  lr: 0.000013  min_lr: 0.000013  loss: 0.8466 (0.8458)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2356 (0.2348)  rec_angle_loss: 0.6061 (0.6068)  total_loss: 0.8466 (0.8458)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4090 (0.4333)  time: 1.2838  data: 0.8573  max mem: 13238
Epoch: [92] Total time: 0:00:25 (1.3592 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000013  loss: 0.8466 (0.8459)  quant_loss: 0.0042 (0.0042)  rec_loss: 0.2356 (0.2350)  rec_angle_loss: 0.6061 (0.6067)  total_loss: 0.8466 (0.8459)  weight_decay: 0.0001 (0.0001)  grad_norm: 0.4090 (0.4333)
Unused code in codebook: 7779
Reset the codebook statistic info in quantizer before testing
Validation:  [ 0/14]  eta: 0:01:14  loss: 0.8664 (0.8664)  time: 5.3348  data: 5.2662  max mem: 13238
Validation:  [10/14]  eta: 0:00:02  loss: 0.9190 (0.9176)  time: 0.5712  data: 0.5067  max mem: 13238
Validation:  [13/14]  eta: 0:00:00  loss: 0.9038 (0.8917)  time: 0.4583  data: 0.3982  max mem: 13238
Validation: Total time: 0:00:07 (0.5545 s / it)
Averaged stats: loss: 0.9038 (0.8916)  quant_loss: 0.0044 (0.0045)  rec_loss: 0.1632 (0.1627)  rec_angle_loss: 0.5937 (0.5943)  total_loss: 0.7614 (0.7614)
Unused code in codebook: 7778
Validation loss of the network on the 10144 test EEG: 0.8916
Reset the codebook statistic info in quantizer before each epoch

======== GPU REPORT ========

==============NVSMI LOG==============

Timestamp                                 : Thu Aug 29 19:46:02 2024
Driver Version                            : 535.183.06
CUDA Version                              : 12.2

Attached GPUs                             : 4
GPU 00000000:44:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 1148568
            GPU Utilization               : 27 %
            Memory Utilization            : 21 %
            Max memory usage              : 16940 MiB
            Time                          : 0 ms
            Is Running                    : 1

GPU 00000000:A4:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 1148569
            GPU Utilization               : 26 %
            Memory Utilization            : 21 %
            Max memory usage              : 16944 MiB
            Time                          : 0 ms
            Is Running                    : 1

GPU 00000000:C3:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 1148570
            GPU Utilization               : 23 %
            Memory Utilization            : 21 %
            Max memory usage              : 16944 MiB
            Time                          : 0 ms
            Is Running                    : 1

GPU 00000000:C4:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 1148571
            GPU Utilization               : 26 %
            Memory Utilization            : 21 %
            Max memory usage              : 16924 MiB
            Time                          : 0 ms
            Is Running                    : 1

Thu Aug 29 19:46:02 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 8000                On  | 00000000:44:00.0 Off |                    0 |
| 33%   54C    P2              74W / 260W |  14903MiB / 46080MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Quadro RTX 8000                On  | 00000000:A4:00.0 Off |                    0 |
| 33%   39C    P2              68W / 260W |  14907MiB / 46080MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Quadro RTX 8000                On  | 00000000:C3:00.0 Off |                    0 |
| 34%   48C    P2              76W / 260W |  14907MiB / 46080MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  Quadro RTX 8000                On  | 00000000:C4:00.0 Off |                    0 |
| 34%   48C    P2              68W / 260W |  14887MiB / 46080MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   1148568      C   ...eyono/.conda/envs/labram/bin/python    14898MiB |
|    1   N/A  N/A   1148569      C   ...eyono/.conda/envs/labram/bin/python    14902MiB |
|    2   N/A  N/A   1148570      C   ...eyono/.conda/envs/labram/bin/python    14902MiB |
|    3   N/A  N/A   1148571      C   ...eyono/.conda/envs/labram/bin/python    14882MiB |
+---------------------------------------------------------------------------------------+
